| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=4, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/data/i5O/kinetics-dataset/annotations', data_root='/data/i5O/kinetics400/train/', data_set='Kinetics-400', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=150, eval=False, eval_data_path=None, eval_log_name='log_eval', fc_drop_rate=0.0, finetune='/data/i5O/pretrained/mvd_s_from_b_ckpt_399.pth', gpu=0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', lr=0.001, merge_test=False, min_lr=0.001, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_small_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=400, no_save_best_ckpt=True, num_frames=16, num_sample=2, num_segments=1, num_workers=6, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', pin_mem=True, rank=0, recount=1, remode='pixel', remove_pos_emb=False, reprob=0.25, resplit=False, resume='', resume_best=False, sampling_rate=4, save_ckpt=True, save_ckpt_freq=50, seed=0, short_side_size=224, smoothing=0.1, start_epoch=0, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=2, use_checkpoint=False, use_cls_token=False, use_mean_pooling=True, warmup_epochs=0, warmup_lr=0.001, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=1)
Patch size = (16, 16)
                                                        0    1
0       /data/i5O/kinetics400/train/-3B32lodo2M_000059...    0
1       /data/i5O/kinetics400/train/-7kbO0v4hag_000107...    0
2       /data/i5O/kinetics400/train/-bwYZwnwb8E_000013...    0
3       /data/i5O/kinetics400/train/-Cv3NwxG_8g_000087...    0
4       /data/i5O/kinetics400/train/-hLv_HL6UhY_000151...    0
...                                                   ...  ...
241202  /data/i5O/kinetics400/train/_GRX1r0JV30_000039...  399
241203  /data/i5O/kinetics400/train/_JuIL8GGX0A_000150...  399
241204  /data/i5O/kinetics400/train/_Jun6C7ICps_000037...  399
241205  /data/i5O/kinetics400/train/_SyAhrLns4k_000200...  399
241206  /data/i5O/kinetics400/train/_URmIF4eHg4_000014...  399

[241207 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f221c482e20>
Mixup is activated!
Load ckpt from /data/i5O/pretrained/mvd_s_from_b_ckpt_399.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['pos_embed', 'mask_token_img', 'pos_embed_img', 'mask_token_vid', 'pos_embed_vid', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=400, bias=True)
)
number of params: 22033936
LR = 0.00003125
Batch size = 8
Update frequent = 2
Number of training examples = 241207
Number of training training per epoch = 30150
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-07-14 00:19:15,156] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.8, git-hash=unknown, git-branch=unknown
[2023-07-14 00:19:15,165] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups
[2023-07-14 00:19:15,165] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1
[2023-07-14 00:19:15,166] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1
[2023-07-14 00:19:15,166] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]
[2023-07-14 00:19:15,167] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]
[2023-07-14 00:19:15,443] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.4 does not match the version torch was compiled with 11.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py38_cu111/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.7455251216888428 seconds
[2023-07-14 00:19:16,995] [INFO] [engine.py:1108:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-07-14 00:19:17,001] [INFO] [engine.py:1116:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2023-07-14 00:19:17,001] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-07-14 00:19:17,012] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-07-14 00:19:17,012] [INFO] [engine.py:808:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-07-14 00:19:17,012] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-07-14 00:19:17,012] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 00:19:17,013] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2023-07-14 00:19:17,013] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-14 00:19:17,013] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-14 00:19:17,013] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2023-07-14 00:19:17,013] [INFO] [config.py:1063:print]   amp_params ................... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   dump_state ................... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'min_scale': 1}
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   global_rank .................. 0
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 2
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 128
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   optimizer_name ............... adam
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-14 00:19:17,014] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   pld_params ................... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   steps_per_print .............. 1000
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   train_batch_size ............. 8
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  4
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   world_size ................... 1
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 0, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   zero_enabled ................. False
[2023-07-14 00:19:17,015] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0
[2023-07-14 00:19:17,015] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cu111/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.7302324771881104 seconds
model.gradient_accumulation_steps() = 2
Use step level LR scheduler!
Set warmup steps = 0
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 150 epochs
Epoch: [0]  [    0/60301]  eta: 2 days, 18:32:34  lr: 0.000031  min_lr: 0.000001  loss: 5.9922 (5.9922)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 3.9727  data: 3.5302  max mem: 8117
Epoch: [0]  [   10/60301]  eta: 13:40:48  lr: 0.000031  min_lr: 0.000001  loss: 5.9922 (5.9921)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8168  data: 0.3389  max mem: 8161
Epoch: [0]  [   20/60301]  eta: 11:01:07  lr: 0.000031  min_lr: 0.000001  loss: 5.9921 (5.9918)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0101  max mem: 8161
Epoch: [0]  [   30/60301]  eta: 10:07:57  lr: 0.000031  min_lr: 0.000001  loss: 5.9923 (5.9918)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0018  max mem: 8161
Epoch: [0]  [   40/60301]  eta: 11:06:01  lr: 0.000031  min_lr: 0.000001  loss: 5.9919 (5.9909)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.1833  max mem: 8161
Epoch: [0]  [   50/60301]  eta: 10:30:07  lr: 0.000031  min_lr: 0.000001  loss: 5.9912 (5.9913)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.1820  max mem: 8161
Epoch: [0]  [   60/60301]  eta: 10:06:19  lr: 0.000031  min_lr: 0.000001  loss: 5.9953 (5.9921)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0015  max mem: 8161
Epoch: [0]  [   70/60301]  eta: 10:44:04  lr: 0.000031  min_lr: 0.000001  loss: 5.9981 (5.9922)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.1961  max mem: 8161
Epoch: [0]  [   80/60301]  eta: 10:24:49  lr: 0.000031  min_lr: 0.000001  loss: 5.9858 (5.9906)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.1951  max mem: 8161
Epoch: [0]  [   90/60301]  eta: 10:09:59  lr: 0.000031  min_lr: 0.000001  loss: 5.9771 (5.9885)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0005  max mem: 8161
Epoch: [0]  [  100/60301]  eta: 9:57:12  lr: 0.000031  min_lr: 0.000001  loss: 5.9834 (5.9890)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0005  max mem: 8161
Epoch: [0]  [  110/60301]  eta: 10:03:03  lr: 0.000031  min_lr: 0.000001  loss: 5.9875 (5.9872)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0893  max mem: 8161
Epoch: [0]  [  120/60301]  eta: 9:53:20  lr: 0.000031  min_lr: 0.000001  loss: 5.9579 (5.9868)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0903  max mem: 8161
Epoch: [0]  [  130/60301]  eta: 9:45:03  lr: 0.000031  min_lr: 0.000001  loss: 5.9928 (5.9870)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0024  max mem: 8161
Epoch: [0]  [  140/60301]  eta: 9:37:58  lr: 0.000031  min_lr: 0.000001  loss: 6.0030 (5.9881)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0014  max mem: 8161
Epoch: [0]  [  150/60301]  eta: 9:31:34  lr: 0.000031  min_lr: 0.000001  loss: 5.9887 (5.9882)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0005  max mem: 8161
Epoch: [0]  [  160/60301]  eta: 9:29:38  lr: 0.000031  min_lr: 0.000001  loss: 5.9631 (5.9857)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0279  max mem: 8161
Epoch: [0]  [  170/60301]  eta: 9:24:36  lr: 0.000031  min_lr: 0.000001  loss: 5.9410 (5.9842)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0287  max mem: 8161
Epoch: [0]  [  180/60301]  eta: 9:19:57  lr: 0.000031  min_lr: 0.000001  loss: 5.9779 (5.9862)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0012  max mem: 8161
Epoch: [0]  [  190/60301]  eta: 9:15:48  lr: 0.000031  min_lr: 0.000001  loss: 5.9860 (5.9859)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0004  max mem: 8161
Epoch: [0]  [  200/60301]  eta: 9:12:02  lr: 0.000031  min_lr: 0.000001  loss: 5.9746 (5.9858)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0004  max mem: 8161
Epoch: [0]  [  210/60301]  eta: 8:55:35  lr: 0.000031  min_lr: 0.000001  loss: 5.9745 (5.9846)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3438  data: 0.0013  max mem: 8161
Epoch: [0]  [  220/60301]  eta: 8:56:38  lr: 0.000031  min_lr: 0.000001  loss: 5.9603 (5.9837)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3831  data: 0.2240  max mem: 8161
Epoch: [0]  [  230/60301]  eta: 8:47:28  lr: 0.000031  min_lr: 0.000001  loss: 5.9380 (5.9827)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4431  data: 0.3306  max mem: 8161
Epoch: [0]  [  240/60301]  eta: 8:48:15  lr: 0.000031  min_lr: 0.000001  loss: 5.9711 (5.9831)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4373  data: 0.3260  max mem: 8161
Epoch: [0]  [  250/60301]  eta: 8:50:06  lr: 0.000031  min_lr: 0.000001  loss: 5.9927 (5.9834)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.3330  max mem: 8161
[2023-07-14 00:21:48,927] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:21:48,927] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [  260/60301]  eta: 8:48:08  lr: 0.000031  min_lr: 0.000001  loss: 5.9721 (5.9831)  loss_scale: 128.0000 (129.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.1149  max mem: 8161
Epoch: [0]  [  270/60301]  eta: 8:46:27  lr: 0.000031  min_lr: 0.000001  loss: 5.9632 (5.9820)  loss_scale: 256.0000 (134.6125)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0005  max mem: 8161
Epoch: [0]  [  280/60301]  eta: 8:44:49  lr: 0.000031  min_lr: 0.000001  loss: 5.9440 (5.9807)  loss_scale: 256.0000 (138.9324)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0005  max mem: 8161
Epoch: [0]  [  290/60301]  eta: 8:43:14  lr: 0.000031  min_lr: 0.000001  loss: 5.9468 (5.9806)  loss_scale: 256.0000 (142.9553)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0005  max mem: 8161
Epoch: [0]  [  300/60301]  eta: 8:41:51  lr: 0.000031  min_lr: 0.000001  loss: 5.9909 (5.9814)  loss_scale: 256.0000 (146.7110)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0004  max mem: 8161
Epoch: [0]  [  310/60301]  eta: 8:40:36  lr: 0.000031  min_lr: 0.000001  loss: 5.9886 (5.9813)  loss_scale: 256.0000 (150.2251)  weight_decay: 0.0500 (0.0500)  time: 0.4850  data: 0.0012  max mem: 8161
Epoch: [0]  [  320/60301]  eta: 8:39:24  lr: 0.000031  min_lr: 0.000001  loss: 5.9651 (5.9807)  loss_scale: 256.0000 (153.5202)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0013  max mem: 8161
Epoch: [0]  [  330/60301]  eta: 8:38:09  lr: 0.000031  min_lr: 0.000001  loss: 5.9299 (5.9805)  loss_scale: 256.0000 (156.6163)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0005  max mem: 8161
Epoch: [0]  [  340/60301]  eta: 8:37:00  lr: 0.000031  min_lr: 0.000001  loss: 5.9666 (5.9812)  loss_scale: 256.0000 (159.5308)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0004  max mem: 8161
Epoch: [0]  [  350/60301]  eta: 8:35:57  lr: 0.000031  min_lr: 0.000001  loss: 5.9513 (5.9799)  loss_scale: 256.0000 (162.2792)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0004  max mem: 8161
video cannot be loaded by decord:  /data/i5O/kinetics400/train/w5ax4GiTkKg_000088_000098.mp4
/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/w5ax4GiTkKg_000088_000098.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [0]  [  360/60301]  eta: 8:39:22  lr: 0.000031  min_lr: 0.000001  loss: 5.9622 (5.9804)  loss_scale: 256.0000 (164.8753)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0816  max mem: 8161
Epoch: [0]  [  370/60301]  eta: 8:38:27  lr: 0.000031  min_lr: 0.000001  loss: 5.9845 (5.9807)  loss_scale: 256.0000 (167.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0855  max mem: 8161
Epoch: [0]  [  380/60301]  eta: 8:37:28  lr: 0.000031  min_lr: 0.000001  loss: 6.0164 (5.9817)  loss_scale: 256.0000 (169.6588)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0052  max mem: 8161
Epoch: [0]  [  390/60301]  eta: 8:32:30  lr: 0.000031  min_lr: 0.000001  loss: 5.9809 (5.9813)  loss_scale: 256.0000 (171.8670)  weight_decay: 0.0500 (0.0500)  time: 0.4060  data: 0.0022  max mem: 8161
Epoch: [0]  [  400/60301]  eta: 8:30:40  lr: 0.000031  min_lr: 0.000001  loss: 5.9603 (5.9813)  loss_scale: 256.0000 (173.9651)  weight_decay: 0.0500 (0.0500)  time: 0.3849  data: 0.1650  max mem: 8161
Epoch: [0]  [  410/60301]  eta: 8:26:37  lr: 0.000031  min_lr: 0.000001  loss: 5.9725 (5.9815)  loss_scale: 256.0000 (175.9611)  weight_decay: 0.0500 (0.0500)  time: 0.3958  data: 0.2821  max mem: 8161
Epoch: [0]  [  420/60301]  eta: 8:26:17  lr: 0.000031  min_lr: 0.000001  loss: 5.9874 (5.9814)  loss_scale: 256.0000 (177.8622)  weight_decay: 0.0500 (0.0500)  time: 0.4227  data: 0.3094  max mem: 8161
Epoch: [0]  [  430/60301]  eta: 8:27:38  lr: 0.000031  min_lr: 0.000001  loss: 5.9412 (5.9799)  loss_scale: 256.0000 (179.6752)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.3758  max mem: 8161
Epoch: [0]  [  440/60301]  eta: 8:27:52  lr: 0.000031  min_lr: 0.000001  loss: 5.9268 (5.9797)  loss_scale: 256.0000 (181.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.2041  max mem: 8161
Epoch: [0]  [  450/60301]  eta: 8:27:20  lr: 0.000031  min_lr: 0.000001  loss: 5.9759 (5.9799)  loss_scale: 256.0000 (183.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0226  max mem: 8161
Epoch: [0]  [  460/60301]  eta: 8:26:43  lr: 0.000031  min_lr: 0.000001  loss: 6.0150 (5.9804)  loss_scale: 256.0000 (184.6421)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0033  max mem: 8161
Epoch: [0]  [  470/60301]  eta: 8:26:08  lr: 0.000031  min_lr: 0.000001  loss: 5.9909 (5.9803)  loss_scale: 256.0000 (186.1571)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0005  max mem: 8161
Epoch: [0]  [  480/60301]  eta: 8:25:33  lr: 0.000031  min_lr: 0.000001  loss: 5.9375 (5.9798)  loss_scale: 256.0000 (187.6091)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0012  max mem: 8161
Epoch: [0]  [  490/60301]  eta: 8:25:06  lr: 0.000031  min_lr: 0.000001  loss: 5.9416 (5.9796)  loss_scale: 256.0000 (189.0020)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0022  max mem: 8161
Epoch: [0]  [  500/60301]  eta: 8:24:32  lr: 0.000031  min_lr: 0.000001  loss: 5.9778 (5.9799)  loss_scale: 256.0000 (190.3393)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0014  max mem: 8161
Epoch: [0]  [  510/60301]  eta: 8:24:01  lr: 0.000031  min_lr: 0.000001  loss: 5.9972 (5.9799)  loss_scale: 256.0000 (191.6243)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0005  max mem: 8161
[2023-07-14 00:23:52,684] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:23:52,684] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [  520/60301]  eta: 8:23:36  lr: 0.000031  min_lr: 0.000001  loss: 5.9599 (5.9797)  loss_scale: 256.0000 (196.7908)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0005  max mem: 8161
Epoch: [0]  [  530/60301]  eta: 8:23:03  lr: 0.000031  min_lr: 0.000001  loss: 5.9249 (5.9788)  loss_scale: 512.0000 (202.7269)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0005  max mem: 8161
Epoch: [0]  [  540/60301]  eta: 8:22:37  lr: 0.000031  min_lr: 0.000001  loss: 5.9240 (5.9778)  loss_scale: 512.0000 (208.4436)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0005  max mem: 8161
Epoch: [0]  [  550/60301]  eta: 8:22:04  lr: 0.000031  min_lr: 0.000001  loss: 5.9499 (5.9780)  loss_scale: 512.0000 (213.9528)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0005  max mem: 8161
Epoch: [0]  [  560/60301]  eta: 8:21:37  lr: 0.000031  min_lr: 0.000001  loss: 5.9643 (5.9783)  loss_scale: 512.0000 (219.2656)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0006  max mem: 8161
Epoch: [0]  [  570/60301]  eta: 8:21:11  lr: 0.000031  min_lr: 0.000001  loss: 5.9845 (5.9781)  loss_scale: 512.0000 (224.3923)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0005  max mem: 8161
Epoch: [0]  [  580/60301]  eta: 8:20:51  lr: 0.000031  min_lr: 0.000001  loss: 5.9610 (5.9777)  loss_scale: 512.0000 (229.3425)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0005  max mem: 8161
Epoch: [0]  [  590/60301]  eta: 8:20:30  lr: 0.000031  min_lr: 0.000001  loss: 5.9391 (5.9770)  loss_scale: 512.0000 (234.1252)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.0005  max mem: 8161
Epoch: [0]  [  600/60301]  eta: 8:20:07  lr: 0.000031  min_lr: 0.000001  loss: 5.9233 (5.9765)  loss_scale: 512.0000 (238.7488)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0012  max mem: 8161
Epoch: [0]  [  610/60301]  eta: 8:19:49  lr: 0.000031  min_lr: 0.000001  loss: 5.9142 (5.9756)  loss_scale: 512.0000 (243.2209)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0012  max mem: 8161
Epoch: [0]  [  620/60301]  eta: 8:19:26  lr: 0.000031  min_lr: 0.000001  loss: 5.9164 (5.9750)  loss_scale: 512.0000 (247.5491)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0004  max mem: 8161
Epoch: [0]  [  630/60301]  eta: 8:19:01  lr: 0.000031  min_lr: 0.000001  loss: 5.9273 (5.9751)  loss_scale: 512.0000 (251.7401)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0004  max mem: 8161
Epoch: [0]  [  640/60301]  eta: 8:18:43  lr: 0.000031  min_lr: 0.000001  loss: 5.9587 (5.9758)  loss_scale: 512.0000 (255.8003)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0010  max mem: 8161
Epoch: [0]  [  650/60301]  eta: 8:18:27  lr: 0.000031  min_lr: 0.000001  loss: 5.9677 (5.9754)  loss_scale: 512.0000 (259.7358)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0021  max mem: 8161
Epoch: [0]  [  660/60301]  eta: 8:18:08  lr: 0.000031  min_lr: 0.000001  loss: 5.9027 (5.9748)  loss_scale: 512.0000 (263.5522)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0024  max mem: 8161
Epoch: [0]  [  670/60301]  eta: 8:17:51  lr: 0.000031  min_lr: 0.000001  loss: 5.9171 (5.9747)  loss_scale: 512.0000 (267.2548)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0021  max mem: 8161
Epoch: [0]  [  680/60301]  eta: 8:17:32  lr: 0.000031  min_lr: 0.000001  loss: 5.9171 (5.9736)  loss_scale: 512.0000 (270.8488)  weight_decay: 0.0500 (0.0500)  time: 0.4865  data: 0.0021  max mem: 8161
Epoch: [0]  [  690/60301]  eta: 8:17:12  lr: 0.000031  min_lr: 0.000001  loss: 5.8853 (5.9730)  loss_scale: 512.0000 (274.3386)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0024  max mem: 8161
Epoch: [0]  [  700/60301]  eta: 8:17:27  lr: 0.000031  min_lr: 0.000001  loss: 5.8959 (5.9724)  loss_scale: 512.0000 (277.7290)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0215  max mem: 8161
Epoch: [0]  [  710/60301]  eta: 8:17:09  lr: 0.000031  min_lr: 0.000001  loss: 5.9035 (5.9720)  loss_scale: 512.0000 (281.0239)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0204  max mem: 8161
Epoch: [0]  [  720/60301]  eta: 8:16:53  lr: 0.000031  min_lr: 0.000001  loss: 5.9354 (5.9714)  loss_scale: 512.0000 (284.2275)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0012  max mem: 8161
Epoch: [0]  [  730/60301]  eta: 8:16:33  lr: 0.000031  min_lr: 0.000001  loss: 5.9674 (5.9712)  loss_scale: 512.0000 (287.3434)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0022  max mem: 8161
Epoch: [0]  [  740/60301]  eta: 8:16:16  lr: 0.000031  min_lr: 0.000001  loss: 5.9588 (5.9710)  loss_scale: 512.0000 (290.3752)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0014  max mem: 8161
Epoch: [0]  [  750/60301]  eta: 8:15:58  lr: 0.000031  min_lr: 0.000001  loss: 5.9365 (5.9705)  loss_scale: 512.0000 (293.3262)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0004  max mem: 8161
Epoch: [0]  [  760/60301]  eta: 8:15:43  lr: 0.000031  min_lr: 0.000001  loss: 5.9496 (5.9704)  loss_scale: 512.0000 (296.1997)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0004  max mem: 8161
[2023-07-14 00:25:57,408] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:25:57,408] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [  770/60301]  eta: 8:15:27  lr: 0.000031  min_lr: 0.000001  loss: 5.9585 (5.9701)  loss_scale: 512.0000 (300.3268)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0013  max mem: 8161
Epoch: [0]  [  780/60301]  eta: 8:17:16  lr: 0.000031  min_lr: 0.000001  loss: 5.9240 (5.9697)  loss_scale: 1024.0000 (309.5928)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0838  max mem: 8161
Epoch: [0]  [  790/60301]  eta: 8:17:00  lr: 0.000031  min_lr: 0.000001  loss: 5.9418 (5.9696)  loss_scale: 1024.0000 (318.6245)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0856  max mem: 8161
Epoch: [0]  [  800/60301]  eta: 8:16:43  lr: 0.000031  min_lr: 0.000001  loss: 5.8909 (5.9689)  loss_scale: 1024.0000 (327.4307)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0041  max mem: 8161
Epoch: [0]  [  810/60301]  eta: 8:16:27  lr: 0.000031  min_lr: 0.000001  loss: 5.8831 (5.9685)  loss_scale: 1024.0000 (336.0197)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0021  max mem: 8161
Epoch: [0]  [  820/60301]  eta: 8:16:13  lr: 0.000031  min_lr: 0.000001  loss: 5.9265 (5.9683)  loss_scale: 1024.0000 (344.3995)  weight_decay: 0.0500 (0.0500)  time: 0.4872  data: 0.0012  max mem: 8161
Epoch: [0]  [  830/60301]  eta: 8:15:59  lr: 0.000031  min_lr: 0.000001  loss: 5.9259 (5.9678)  loss_scale: 1024.0000 (352.5776)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0015  max mem: 8161
Epoch: [0]  [  840/60301]  eta: 8:13:54  lr: 0.000031  min_lr: 0.000001  loss: 5.9249 (5.9671)  loss_scale: 1024.0000 (360.5612)  weight_decay: 0.0500 (0.0500)  time: 0.4092  data: 0.0024  max mem: 8161
Epoch: [0]  [  850/60301]  eta: 8:14:03  lr: 0.000031  min_lr: 0.000001  loss: 5.9444 (5.9673)  loss_scale: 1024.0000 (368.3572)  weight_decay: 0.0500 (0.0500)  time: 0.4250  data: 0.2039  max mem: 8161
Epoch: [0]  [  860/60301]  eta: 8:11:34  lr: 0.000031  min_lr: 0.000001  loss: 5.9465 (5.9664)  loss_scale: 1024.0000 (375.9721)  weight_decay: 0.0500 (0.0500)  time: 0.4047  data: 0.2910  max mem: 8161
Epoch: [0]  [  870/60301]  eta: 8:12:10  lr: 0.000031  min_lr: 0.000001  loss: 5.8939 (5.9660)  loss_scale: 1024.0000 (383.4122)  weight_decay: 0.0500 (0.0500)  time: 0.4229  data: 0.3098  max mem: 8161
Epoch: [0]  [  880/60301]  eta: 8:13:02  lr: 0.000031  min_lr: 0.000001  loss: 5.9117 (5.9661)  loss_scale: 1024.0000 (390.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.4193  max mem: 8161
Epoch: [0]  [  890/60301]  eta: 8:13:08  lr: 0.000031  min_lr: 0.000001  loss: 5.9635 (5.9660)  loss_scale: 1024.0000 (397.7912)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.2132  max mem: 8161
Epoch: [0]  [  900/60301]  eta: 8:13:18  lr: 0.000031  min_lr: 0.000001  loss: 5.8904 (5.9650)  loss_scale: 1024.0000 (404.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0344  max mem: 8161
Epoch: [0]  [  910/60301]  eta: 8:13:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8559 (5.9643)  loss_scale: 1024.0000 (411.5390)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0199  max mem: 8161
Epoch: [0]  [  920/60301]  eta: 8:12:48  lr: 0.000031  min_lr: 0.000001  loss: 5.8937 (5.9637)  loss_scale: 1024.0000 (418.1889)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0013  max mem: 8161
Epoch: [0]  [  930/60301]  eta: 8:12:36  lr: 0.000031  min_lr: 0.000001  loss: 5.8773 (5.9632)  loss_scale: 1024.0000 (424.6960)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0013  max mem: 8161
Epoch: [0]  [  940/60301]  eta: 8:10:36  lr: 0.000031  min_lr: 0.000001  loss: 5.9135 (5.9630)  loss_scale: 1024.0000 (431.0648)  weight_decay: 0.0500 (0.0500)  time: 0.4011  data: 0.0021  max mem: 8161
Epoch: [0]  [  950/60301]  eta: 8:08:25  lr: 0.000031  min_lr: 0.000001  loss: 5.8588 (5.9619)  loss_scale: 1024.0000 (437.2997)  weight_decay: 0.0500 (0.0500)  time: 0.3043  data: 0.0921  max mem: 8161
Epoch: [0]  [  960/60301]  eta: 8:07:56  lr: 0.000031  min_lr: 0.000001  loss: 5.8513 (5.9613)  loss_scale: 1024.0000 (443.4048)  weight_decay: 0.0500 (0.0500)  time: 0.3749  data: 0.2628  max mem: 8161
Epoch: [0]  [  970/60301]  eta: 8:09:58  lr: 0.000031  min_lr: 0.000001  loss: 5.9144 (5.9612)  loss_scale: 1024.0000 (449.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.4660  max mem: 8161
Epoch: [0]  [  980/60301]  eta: 8:09:08  lr: 0.000031  min_lr: 0.000001  loss: 5.9292 (5.9607)  loss_scale: 1024.0000 (455.2416)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.3683  max mem: 8161
Epoch: [0]  [  990/60301]  eta: 8:08:54  lr: 0.000031  min_lr: 0.000001  loss: 5.8735 (5.9604)  loss_scale: 1024.0000 (460.9808)  weight_decay: 0.0500 (0.0500)  time: 0.4510  data: 0.0743  max mem: 8161
[2023-07-14 00:27:50,911] [INFO] [timer.py:181:stop] 0/1000, SamplesPerSec=9.691182984175182
Epoch: [0]  [ 1000/60301]  eta: 8:12:00  lr: 0.000031  min_lr: 0.000001  loss: 5.8487 (5.9592)  loss_scale: 1024.0000 (466.6054)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1697  max mem: 8161
Epoch: [0]  [ 1010/60301]  eta: 8:11:44  lr: 0.000031  min_lr: 0.000001  loss: 5.8592 (5.9587)  loss_scale: 1024.0000 (472.1187)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.1705  max mem: 8161
Epoch: [0]  [ 1020/60301]  eta: 8:11:30  lr: 0.000031  min_lr: 0.000001  loss: 5.8751 (5.9577)  loss_scale: 1024.0000 (477.5240)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0022  max mem: 8161
[2023-07-14 00:28:03,367] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:28:03,367] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 1030/60301]  eta: 8:11:17  lr: 0.000031  min_lr: 0.000001  loss: 5.8179 (5.9566)  loss_scale: 1024.0000 (488.7837)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0022  max mem: 8161
Epoch: [0]  [ 1040/60301]  eta: 8:11:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8132 (5.9556)  loss_scale: 2048.0000 (503.7618)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0022  max mem: 8161
Epoch: [0]  [ 1050/60301]  eta: 8:10:50  lr: 0.000031  min_lr: 0.000001  loss: 5.8443 (5.9548)  loss_scale: 2048.0000 (518.4548)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0021  max mem: 8161
Epoch: [0]  [ 1060/60301]  eta: 8:11:34  lr: 0.000031  min_lr: 0.000001  loss: 5.8435 (5.9540)  loss_scale: 2048.0000 (532.8709)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0546  max mem: 8161
Epoch: [0]  [ 1070/60301]  eta: 8:11:19  lr: 0.000031  min_lr: 0.000001  loss: 5.8267 (5.9531)  loss_scale: 2048.0000 (547.0177)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0538  max mem: 8161
Epoch: [0]  [ 1080/60301]  eta: 8:11:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8618 (5.9525)  loss_scale: 2048.0000 (560.9029)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1090/60301]  eta: 8:10:48  lr: 0.000031  min_lr: 0.000001  loss: 5.8946 (5.9517)  loss_scale: 2048.0000 (574.5335)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1100/60301]  eta: 8:13:41  lr: 0.000031  min_lr: 0.000001  loss: 5.9182 (5.9518)  loss_scale: 2048.0000 (587.9164)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1750  max mem: 8161
Epoch: [0]  [ 1110/60301]  eta: 8:13:24  lr: 0.000031  min_lr: 0.000001  loss: 5.8946 (5.9511)  loss_scale: 2048.0000 (601.0585)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.1750  max mem: 8161
Epoch: [0]  [ 1120/60301]  eta: 8:13:10  lr: 0.000031  min_lr: 0.000001  loss: 5.8667 (5.9507)  loss_scale: 2048.0000 (613.9661)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1130/60301]  eta: 8:13:54  lr: 0.000031  min_lr: 0.000001  loss: 5.9175 (5.9509)  loss_scale: 2048.0000 (626.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0592  max mem: 8161
Epoch: [0]  [ 1140/60301]  eta: 8:13:41  lr: 0.000031  min_lr: 0.000001  loss: 5.9141 (5.9503)  loss_scale: 2048.0000 (639.1025)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0591  max mem: 8161
Epoch: [0]  [ 1150/60301]  eta: 8:13:27  lr: 0.000031  min_lr: 0.000001  loss: 5.8577 (5.9495)  loss_scale: 2048.0000 (651.3432)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1160/60301]  eta: 8:13:13  lr: 0.000031  min_lr: 0.000001  loss: 5.8737 (5.9492)  loss_scale: 2048.0000 (663.3730)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1170/60301]  eta: 8:13:00  lr: 0.000031  min_lr: 0.000001  loss: 5.8528 (5.9485)  loss_scale: 2048.0000 (675.1973)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1180/60301]  eta: 8:12:45  lr: 0.000031  min_lr: 0.000001  loss: 5.8506 (5.9477)  loss_scale: 2048.0000 (686.8213)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1190/60301]  eta: 8:12:33  lr: 0.000031  min_lr: 0.000001  loss: 5.8973 (5.9478)  loss_scale: 2048.0000 (698.2502)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1200/60301]  eta: 8:12:18  lr: 0.000031  min_lr: 0.000001  loss: 5.9502 (5.9479)  loss_scale: 2048.0000 (709.4888)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1210/60301]  eta: 8:12:04  lr: 0.000031  min_lr: 0.000001  loss: 5.9377 (5.9478)  loss_scale: 2048.0000 (720.5417)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0032  max mem: 8161
Epoch: [0]  [ 1220/60301]  eta: 8:11:50  lr: 0.000031  min_lr: 0.000001  loss: 5.8822 (5.9474)  loss_scale: 2048.0000 (731.4136)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0022  max mem: 8161
Epoch: [0]  [ 1230/60301]  eta: 8:11:38  lr: 0.000031  min_lr: 0.000001  loss: 5.8557 (5.9468)  loss_scale: 2048.0000 (742.1089)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1240/60301]  eta: 8:11:24  lr: 0.000031  min_lr: 0.000001  loss: 5.8540 (5.9460)  loss_scale: 2048.0000 (752.6317)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1250/60301]  eta: 8:11:12  lr: 0.000031  min_lr: 0.000001  loss: 5.8903 (5.9455)  loss_scale: 2048.0000 (762.9864)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1260/60301]  eta: 8:10:59  lr: 0.000031  min_lr: 0.000001  loss: 5.9162 (5.9452)  loss_scale: 2048.0000 (773.1768)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0025  max mem: 8161
Epoch: [0]  [ 1270/60301]  eta: 8:10:47  lr: 0.000031  min_lr: 0.000001  loss: 5.9353 (5.9448)  loss_scale: 2048.0000 (783.2069)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0024  max mem: 8161
Epoch: [0]  [ 1280/60301]  eta: 8:10:33  lr: 0.000031  min_lr: 0.000001  loss: 5.8734 (5.9442)  loss_scale: 2048.0000 (793.0804)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0014  max mem: 8161
[2023-07-14 00:30:12,442] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:30:12,442] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 1290/60301]  eta: 8:11:10  lr: 0.000031  min_lr: 0.000001  loss: 5.8487 (5.9434)  loss_scale: 2048.0000 (818.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0562  max mem: 8161
Epoch: [0]  [ 1300/60301]  eta: 8:10:55  lr: 0.000031  min_lr: 0.000001  loss: 5.8541 (5.9433)  loss_scale: 4096.0000 (843.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0561  max mem: 8161
Epoch: [0]  [ 1310/60301]  eta: 8:11:49  lr: 0.000031  min_lr: 0.000001  loss: 5.9103 (5.9433)  loss_scale: 4096.0000 (868.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0752  max mem: 8161
Epoch: [0]  [ 1320/60301]  eta: 8:11:35  lr: 0.000031  min_lr: 0.000001  loss: 5.8759 (5.9426)  loss_scale: 4096.0000 (893.0931)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0752  max mem: 8161
Epoch: [0]  [ 1330/60301]  eta: 8:11:23  lr: 0.000031  min_lr: 0.000001  loss: 5.8825 (5.9423)  loss_scale: 4096.0000 (917.1570)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0041  max mem: 8161
Epoch: [0]  [ 1340/60301]  eta: 8:11:09  lr: 0.000031  min_lr: 0.000001  loss: 5.8825 (5.9413)  loss_scale: 4096.0000 (940.8620)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0042  max mem: 8161
Epoch: [0]  [ 1350/60301]  eta: 8:10:55  lr: 0.000031  min_lr: 0.000001  loss: 5.8320 (5.9411)  loss_scale: 4096.0000 (964.2161)  weight_decay: 0.0500 (0.0500)  time: 0.4794  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1360/60301]  eta: 8:10:43  lr: 0.000031  min_lr: 0.000001  loss: 5.8693 (5.9406)  loss_scale: 4096.0000 (987.2270)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0024  max mem: 8161
Epoch: [0]  [ 1370/60301]  eta: 8:10:28  lr: 0.000031  min_lr: 0.000001  loss: 5.8693 (5.9399)  loss_scale: 4096.0000 (1009.9023)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0015  max mem: 8161
Epoch: [0]  [ 1380/60301]  eta: 8:10:15  lr: 0.000031  min_lr: 0.000001  loss: 5.8387 (5.9390)  loss_scale: 4096.0000 (1032.2491)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1390/60301]  eta: 8:10:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8438 (5.9389)  loss_scale: 4096.0000 (1054.2746)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1400/60301]  eta: 8:09:50  lr: 0.000031  min_lr: 0.000001  loss: 5.8488 (5.9382)  loss_scale: 4096.0000 (1075.9857)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1410/60301]  eta: 8:09:37  lr: 0.000031  min_lr: 0.000001  loss: 5.8702 (5.9380)  loss_scale: 4096.0000 (1097.3891)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0022  max mem: 8161
Epoch: [0]  [ 1420/60301]  eta: 8:09:25  lr: 0.000031  min_lr: 0.000001  loss: 5.8741 (5.9374)  loss_scale: 4096.0000 (1118.4912)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1430/60301]  eta: 8:09:13  lr: 0.000031  min_lr: 0.000001  loss: 5.8402 (5.9367)  loss_scale: 4096.0000 (1139.2984)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1440/60301]  eta: 8:09:02  lr: 0.000031  min_lr: 0.000001  loss: 5.8420 (5.9361)  loss_scale: 4096.0000 (1159.8168)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1450/60301]  eta: 8:08:50  lr: 0.000031  min_lr: 0.000001  loss: 5.8420 (5.9355)  loss_scale: 4096.0000 (1180.0524)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1460/60301]  eta: 8:08:39  lr: 0.000031  min_lr: 0.000001  loss: 5.8010 (5.9351)  loss_scale: 4096.0000 (1200.0110)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0011  max mem: 8161
Epoch: [0]  [ 1470/60301]  eta: 8:08:27  lr: 0.000031  min_lr: 0.000001  loss: 5.8010 (5.9342)  loss_scale: 4096.0000 (1219.6982)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0021  max mem: 8161
Epoch: [0]  [ 1480/60301]  eta: 8:08:15  lr: 0.000031  min_lr: 0.000001  loss: 5.8477 (5.9336)  loss_scale: 4096.0000 (1239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0015  max mem: 8161
Epoch: [0]  [ 1490/60301]  eta: 8:08:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8477 (5.9327)  loss_scale: 4096.0000 (1258.2803)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0024  max mem: 8161
Epoch: [0]  [ 1500/60301]  eta: 8:07:52  lr: 0.000031  min_lr: 0.000001  loss: 5.7932 (5.9319)  loss_scale: 4096.0000 (1277.1859)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0032  max mem: 8161
Epoch: [0]  [ 1510/60301]  eta: 8:07:42  lr: 0.000031  min_lr: 0.000001  loss: 5.8779 (5.9317)  loss_scale: 4096.0000 (1295.8412)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0023  max mem: 8161
Epoch: [0]  [ 1520/60301]  eta: 8:07:30  lr: 0.000031  min_lr: 0.000001  loss: 5.8275 (5.9310)  loss_scale: 4096.0000 (1314.2512)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0015  max mem: 8161
Epoch: [0]  [ 1530/60301]  eta: 8:07:16  lr: 0.000031  min_lr: 0.000001  loss: 5.8048 (5.9304)  loss_scale: 4096.0000 (1332.4206)  weight_decay: 0.0500 (0.0500)  time: 0.4772  data: 0.0004  max mem: 8161
[2023-07-14 00:32:18,227] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:32:18,227] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 1540/60301]  eta: 8:07:06  lr: 0.000031  min_lr: 0.000001  loss: 5.8547 (5.9300)  loss_scale: 4096.0000 (1360.9864)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1550/60301]  eta: 8:06:55  lr: 0.000031  min_lr: 0.000001  loss: 5.8690 (5.9299)  loss_scale: 8192.0000 (1405.0290)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1560/60301]  eta: 8:06:45  lr: 0.000031  min_lr: 0.000001  loss: 5.8602 (5.9294)  loss_scale: 8192.0000 (1448.5074)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1570/60301]  eta: 8:06:34  lr: 0.000031  min_lr: 0.000001  loss: 5.8082 (5.9285)  loss_scale: 8192.0000 (1491.4322)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1580/60301]  eta: 8:06:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7910 (5.9278)  loss_scale: 8192.0000 (1533.8140)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1590/60301]  eta: 8:06:12  lr: 0.000031  min_lr: 0.000001  loss: 5.8115 (5.9272)  loss_scale: 8192.0000 (1575.6631)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1600/60301]  eta: 8:06:01  lr: 0.000031  min_lr: 0.000001  loss: 5.8786 (5.9268)  loss_scale: 8192.0000 (1616.9894)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1610/60301]  eta: 8:05:52  lr: 0.000031  min_lr: 0.000001  loss: 5.8668 (5.9262)  loss_scale: 8192.0000 (1657.8026)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1620/60301]  eta: 8:05:45  lr: 0.000031  min_lr: 0.000001  loss: 5.8359 (5.9256)  loss_scale: 8192.0000 (1698.1123)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0058  max mem: 8161
Epoch: [0]  [ 1630/60301]  eta: 8:05:34  lr: 0.000031  min_lr: 0.000001  loss: 5.7763 (5.9249)  loss_scale: 8192.0000 (1737.9277)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0049  max mem: 8161
Epoch: [0]  [ 1640/60301]  eta: 8:05:25  lr: 0.000031  min_lr: 0.000001  loss: 5.7586 (5.9242)  loss_scale: 8192.0000 (1777.2578)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0023  max mem: 8161
Epoch: [0]  [ 1650/60301]  eta: 8:05:15  lr: 0.000031  min_lr: 0.000001  loss: 5.7539 (5.9230)  loss_scale: 8192.0000 (1816.1114)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0023  max mem: 8161
Epoch: [0]  [ 1660/60301]  eta: 8:05:04  lr: 0.000031  min_lr: 0.000001  loss: 5.8184 (5.9226)  loss_scale: 8192.0000 (1854.4973)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1670/60301]  eta: 8:04:54  lr: 0.000031  min_lr: 0.000001  loss: 5.8390 (5.9217)  loss_scale: 8192.0000 (1892.4237)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0016  max mem: 8161
Epoch: [0]  [ 1680/60301]  eta: 8:04:44  lr: 0.000031  min_lr: 0.000001  loss: 5.8071 (5.9211)  loss_scale: 8192.0000 (1929.8989)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0016  max mem: 8161
Epoch: [0]  [ 1690/60301]  eta: 8:04:33  lr: 0.000031  min_lr: 0.000001  loss: 5.8071 (5.9208)  loss_scale: 8192.0000 (1966.9308)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0004  max mem: 8161
Epoch: [0]  [ 1700/60301]  eta: 8:04:22  lr: 0.000031  min_lr: 0.000001  loss: 5.8863 (5.9207)  loss_scale: 8192.0000 (2003.5273)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1710/60301]  eta: 8:04:14  lr: 0.000031  min_lr: 0.000001  loss: 5.8949 (5.9208)  loss_scale: 8192.0000 (2039.6961)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0032  max mem: 8161
Epoch: [0]  [ 1720/60301]  eta: 8:04:03  lr: 0.000031  min_lr: 0.000001  loss: 5.8317 (5.9201)  loss_scale: 8192.0000 (2075.4445)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0040  max mem: 8161
Epoch: [0]  [ 1730/60301]  eta: 8:05:04  lr: 0.000031  min_lr: 0.000001  loss: 5.8192 (5.9195)  loss_scale: 8192.0000 (2110.7799)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1086  max mem: 8161
Epoch: [0]  [ 1740/60301]  eta: 8:04:53  lr: 0.000031  min_lr: 0.000001  loss: 5.8209 (5.9190)  loss_scale: 8192.0000 (2145.7094)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1077  max mem: 8161
Epoch: [0]  [ 1750/60301]  eta: 8:04:43  lr: 0.000031  min_lr: 0.000001  loss: 5.8395 (5.9187)  loss_scale: 8192.0000 (2180.2399)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0005  max mem: 8161
Epoch: [0]  [ 1760/60301]  eta: 8:04:34  lr: 0.000031  min_lr: 0.000001  loss: 5.8395 (5.9182)  loss_scale: 8192.0000 (2214.3782)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0013  max mem: 8161
Epoch: [0]  [ 1770/60301]  eta: 8:04:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7914 (5.9174)  loss_scale: 8192.0000 (2248.1310)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0022  max mem: 8161
Epoch: [0]  [ 1780/60301]  eta: 8:04:14  lr: 0.000031  min_lr: 0.000001  loss: 5.7772 (5.9170)  loss_scale: 8192.0000 (2281.5048)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0014  max mem: 8161
Epoch: [0]  [ 1790/60301]  eta: 8:04:49  lr: 0.000031  min_lr: 0.000001  loss: 5.8577 (5.9168)  loss_scale: 8192.0000 (2314.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0704  max mem: 8161
[2023-07-14 00:34:25,173] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:34:25,173] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 1800/60301]  eta: 8:04:39  lr: 0.000031  min_lr: 0.000001  loss: 5.8545 (5.9166)  loss_scale: 8192.0000 (2383.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0711  max mem: 8161
Epoch: [0]  [ 1810/60301]  eta: 8:04:29  lr: 0.000031  min_lr: 0.000001  loss: 5.8289 (5.9162)  loss_scale: 16384.0000 (2460.8371)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0033  max mem: 8161
Epoch: [0]  [ 1820/60301]  eta: 8:04:20  lr: 0.000031  min_lr: 0.000001  loss: 5.8289 (5.9157)  loss_scale: 16384.0000 (2537.2960)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0052  max mem: 8161
Epoch: [0]  [ 1830/60301]  eta: 8:04:12  lr: 0.000031  min_lr: 0.000001  loss: 5.7558 (5.9152)  loss_scale: 16384.0000 (2612.9197)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0031  max mem: 8161
Epoch: [0]  [ 1840/60301]  eta: 8:04:02  lr: 0.000031  min_lr: 0.000001  loss: 5.8082 (5.9150)  loss_scale: 16384.0000 (2687.7219)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0023  max mem: 8161
Epoch: [0]  [ 1850/60301]  eta: 8:04:01  lr: 0.000031  min_lr: 0.000001  loss: 5.8464 (5.9145)  loss_scale: 16384.0000 (2761.7158)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0158  max mem: 8161
Epoch: [0]  [ 1860/60301]  eta: 8:03:42  lr: 0.000031  min_lr: 0.000001  loss: 5.8295 (5.9138)  loss_scale: 16384.0000 (2834.9146)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0581  max mem: 8161
Epoch: [0]  [ 1870/60301]  eta: 8:02:39  lr: 0.000031  min_lr: 0.000001  loss: 5.8551 (5.9138)  loss_scale: 16384.0000 (2907.3308)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.1429  max mem: 8161
Epoch: [0]  [ 1880/60301]  eta: 8:01:59  lr: 0.000031  min_lr: 0.000001  loss: 5.8273 (5.9131)  loss_scale: 16384.0000 (2978.9771)  weight_decay: 0.0500 (0.0500)  time: 0.3471  data: 0.2340  max mem: 8161
Epoch: [0]  [ 1890/60301]  eta: 8:01:20  lr: 0.000031  min_lr: 0.000001  loss: 5.7960 (5.9126)  loss_scale: 16384.0000 (3049.8657)  weight_decay: 0.0500 (0.0500)  time: 0.3841  data: 0.2715  max mem: 8161
Epoch: [0]  [ 1900/60301]  eta: 8:01:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7960 (5.9123)  loss_scale: 16384.0000 (3120.0084)  weight_decay: 0.0500 (0.0500)  time: 0.4537  data: 0.3405  max mem: 8161
Epoch: [0]  [ 1910/60301]  eta: 8:02:13  lr: 0.000031  min_lr: 0.000001  loss: 5.8567 (5.9122)  loss_scale: 16384.0000 (3189.4171)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.3000  max mem: 8161
Epoch: [0]  [ 1920/60301]  eta: 8:02:03  lr: 0.000031  min_lr: 0.000001  loss: 5.7935 (5.9113)  loss_scale: 16384.0000 (3258.1031)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0967  max mem: 8161
Epoch: [0]  [ 1930/60301]  eta: 8:04:35  lr: 0.000031  min_lr: 0.000001  loss: 5.7731 (5.9107)  loss_scale: 16384.0000 (3326.0777)  weight_decay: 0.0500 (0.0500)  time: 0.7480  data: 0.2694  max mem: 8161
Epoch: [0]  [ 1940/60301]  eta: 8:04:37  lr: 0.000031  min_lr: 0.000001  loss: 5.7869 (5.9099)  loss_scale: 16384.0000 (3393.3519)  weight_decay: 0.0500 (0.0500)  time: 0.7682  data: 0.2901  max mem: 8161
Epoch: [0]  [ 1950/60301]  eta: 8:04:28  lr: 0.000031  min_lr: 0.000001  loss: 5.7421 (5.9093)  loss_scale: 16384.0000 (3459.9364)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0221  max mem: 8161
Epoch: [0]  [ 1960/60301]  eta: 8:04:18  lr: 0.000031  min_lr: 0.000001  loss: 5.7823 (5.9088)  loss_scale: 16384.0000 (3525.8419)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0015  max mem: 8161
Epoch: [0]  [ 1970/60301]  eta: 8:04:07  lr: 0.000031  min_lr: 0.000001  loss: 5.8064 (5.9084)  loss_scale: 16384.0000 (3591.0786)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0015  max mem: 8161
Epoch: [0]  [ 1980/60301]  eta: 8:03:57  lr: 0.000031  min_lr: 0.000001  loss: 5.8131 (5.9081)  loss_scale: 16384.0000 (3655.6567)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0012  max mem: 8161
Epoch: [0]  [ 1990/60301]  eta: 8:03:47  lr: 0.000031  min_lr: 0.000001  loss: 5.8361 (5.9077)  loss_scale: 16384.0000 (3719.5861)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0012  max mem: 8161
[2023-07-14 00:36:08,911] [INFO] [logging.py:69:log_dist] [Rank 0] step=1000, skipped=0, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 00:36:08,915] [INFO] [timer.py:181:stop] 0/2000, SamplesPerSec=9.222618949655525
Epoch: [0]  [ 2000/60301]  eta: 8:03:36  lr: 0.000031  min_lr: 0.000001  loss: 5.8231 (5.9072)  loss_scale: 16384.0000 (3782.8766)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2010/60301]  eta: 8:03:26  lr: 0.000031  min_lr: 0.000001  loss: 5.8231 (5.9068)  loss_scale: 16384.0000 (3845.5375)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2020/60301]  eta: 8:03:17  lr: 0.000031  min_lr: 0.000001  loss: 5.8714 (5.9068)  loss_scale: 16384.0000 (3907.5784)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2030/60301]  eta: 8:03:07  lr: 0.000031  min_lr: 0.000001  loss: 5.8495 (5.9064)  loss_scale: 16384.0000 (3969.0084)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2040/60301]  eta: 8:02:58  lr: 0.000031  min_lr: 0.000001  loss: 5.7796 (5.9059)  loss_scale: 16384.0000 (4029.8364)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0018  max mem: 8161
[2023-07-14 00:36:31,744] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:36:31,744] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [ 2050/60301]  eta: 8:02:09  lr: 0.000031  min_lr: 0.000001  loss: 5.7604 (5.9049)  loss_scale: 16384.0000 (4106.0478)  weight_decay: 0.0500 (0.0500)  time: 0.4127  data: 0.0018  max mem: 8161
Epoch: [0]  [ 2060/60301]  eta: 8:01:33  lr: 0.000031  min_lr: 0.000001  loss: 5.7652 (5.9048)  loss_scale: 32768.0000 (4245.1160)  weight_decay: 0.0500 (0.0500)  time: 0.3645  data: 0.1364  max mem: 8161
Epoch: [0]  [ 2070/60301]  eta: 8:00:52  lr: 0.000031  min_lr: 0.000001  loss: 5.8562 (5.9044)  loss_scale: 32768.0000 (4382.8411)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.2632  max mem: 8161
Epoch: [0]  [ 2080/60301]  eta: 7:59:59  lr: 0.000031  min_lr: 0.000001  loss: 5.7887 (5.9037)  loss_scale: 32768.0000 (4519.2427)  weight_decay: 0.0500 (0.0500)  time: 0.3459  data: 0.2326  max mem: 8161
Epoch: [0]  [ 2090/60301]  eta: 8:00:04  lr: 0.000031  min_lr: 0.000001  loss: 5.8005 (5.9031)  loss_scale: 32768.0000 (4654.3396)  weight_decay: 0.0500 (0.0500)  time: 0.4266  data: 0.3135  max mem: 8161
Epoch: [0]  [ 2100/60301]  eta: 8:00:39  lr: 0.000031  min_lr: 0.000001  loss: 5.8595 (5.9030)  loss_scale: 32768.0000 (4788.1504)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.2867  max mem: 8161
Epoch: [0]  [ 2110/60301]  eta: 8:00:29  lr: 0.000031  min_lr: 0.000001  loss: 5.8965 (5.9029)  loss_scale: 32768.0000 (4920.6935)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0783  max mem: 8161
Epoch: [0]  [ 2120/60301]  eta: 8:00:20  lr: 0.000031  min_lr: 0.000001  loss: 5.8557 (5.9027)  loss_scale: 32768.0000 (5051.9868)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2130/60301]  eta: 8:00:12  lr: 0.000031  min_lr: 0.000001  loss: 5.7755 (5.9017)  loss_scale: 32768.0000 (5182.0479)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0029  max mem: 8161
Epoch: [0]  [ 2140/60301]  eta: 8:00:04  lr: 0.000031  min_lr: 0.000001  loss: 5.7755 (5.9015)  loss_scale: 32768.0000 (5310.8940)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0019  max mem: 8161
Epoch: [0]  [ 2150/60301]  eta: 8:00:23  lr: 0.000031  min_lr: 0.000001  loss: 5.8624 (5.9013)  loss_scale: 32768.0000 (5438.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0523  max mem: 8161
Epoch: [0]  [ 2160/60301]  eta: 8:00:14  lr: 0.000031  min_lr: 0.000001  loss: 5.7260 (5.9005)  loss_scale: 32768.0000 (5565.0088)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0523  max mem: 8161
Epoch: [0]  [ 2170/60301]  eta: 8:00:06  lr: 0.000031  min_lr: 0.000001  loss: 5.7737 (5.9003)  loss_scale: 32768.0000 (5690.3105)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0014  max mem: 8161
Epoch: [0]  [ 2180/60301]  eta: 7:59:57  lr: 0.000031  min_lr: 0.000001  loss: 5.8193 (5.8998)  loss_scale: 32768.0000 (5814.4631)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0014  max mem: 8161
Epoch: [0]  [ 2190/60301]  eta: 7:59:49  lr: 0.000031  min_lr: 0.000001  loss: 5.8193 (5.8996)  loss_scale: 32768.0000 (5937.4824)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0012  max mem: 8161
Epoch: [0]  [ 2200/60301]  eta: 7:59:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7864 (5.8986)  loss_scale: 32768.0000 (6059.3839)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0019  max mem: 8161
Epoch: [0]  [ 2210/60301]  eta: 7:59:32  lr: 0.000031  min_lr: 0.000001  loss: 5.6807 (5.8978)  loss_scale: 32768.0000 (6180.1827)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0012  max mem: 8161
Epoch: [0]  [ 2220/60301]  eta: 7:59:29  lr: 0.000031  min_lr: 0.000001  loss: 5.7738 (5.8972)  loss_scale: 32768.0000 (6299.8937)  weight_decay: 0.0500 (0.0500)  time: 0.4898  data: 0.0093  max mem: 8161
Epoch: [0]  [ 2230/60301]  eta: 7:59:50  lr: 0.000031  min_lr: 0.000001  loss: 5.7290 (5.8964)  loss_scale: 32768.0000 (6418.5316)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0648  max mem: 8161
Epoch: [0]  [ 2240/60301]  eta: 7:59:50  lr: 0.000031  min_lr: 0.000001  loss: 5.7027 (5.8955)  loss_scale: 32768.0000 (6536.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0723  max mem: 8161
Epoch: [0]  [ 2250/60301]  eta: 7:59:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7037 (5.8947)  loss_scale: 32768.0000 (6652.6450)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0167  max mem: 8161
Epoch: [0]  [ 2260/60301]  eta: 7:59:31  lr: 0.000031  min_lr: 0.000001  loss: 5.7085 (5.8940)  loss_scale: 32768.0000 (6768.1486)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2270/60301]  eta: 7:59:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7320 (5.8935)  loss_scale: 32768.0000 (6882.6350)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0032  max mem: 8161
Epoch: [0]  [ 2280/60301]  eta: 7:59:15  lr: 0.000031  min_lr: 0.000001  loss: 5.7984 (5.8937)  loss_scale: 32768.0000 (6996.1175)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0032  max mem: 8161
Epoch: [0]  [ 2290/60301]  eta: 7:59:07  lr: 0.000031  min_lr: 0.000001  loss: 5.8352 (5.8928)  loss_scale: 32768.0000 (7108.6093)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2300/60301]  eta: 7:59:58  lr: 0.000031  min_lr: 0.000001  loss: 5.7438 (5.8926)  loss_scale: 32768.0000 (7220.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1203  max mem: 8161
[2023-07-14 00:38:38,423] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:38:38,423] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [ 2310/60301]  eta: 7:59:49  lr: 0.000031  min_lr: 0.000001  loss: 5.7885 (5.8922)  loss_scale: 32768.0000 (7415.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1195  max mem: 8161
Epoch: [0]  [ 2320/60301]  eta: 7:59:40  lr: 0.000031  min_lr: 0.000001  loss: 5.8436 (5.8918)  loss_scale: 65536.0000 (7666.1577)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2330/60301]  eta: 7:59:33  lr: 0.000031  min_lr: 0.000001  loss: 5.8113 (5.8915)  loss_scale: 65536.0000 (7914.4196)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2340/60301]  eta: 7:59:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7696 (5.8909)  loss_scale: 65536.0000 (8160.5604)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2350/60301]  eta: 7:59:16  lr: 0.000031  min_lr: 0.000001  loss: 5.7141 (5.8904)  loss_scale: 65536.0000 (8404.6074)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2360/60301]  eta: 7:59:08  lr: 0.000031  min_lr: 0.000001  loss: 5.7801 (5.8901)  loss_scale: 65536.0000 (8646.5870)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0034  max mem: 8161
Epoch: [0]  [ 2370/60301]  eta: 7:59:00  lr: 0.000031  min_lr: 0.000001  loss: 5.7810 (5.8895)  loss_scale: 65536.0000 (8886.5255)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0026  max mem: 8161
Epoch: [0]  [ 2380/60301]  eta: 7:58:53  lr: 0.000031  min_lr: 0.000001  loss: 5.7632 (5.8890)  loss_scale: 65536.0000 (9124.4486)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0014  max mem: 8161
Epoch: [0]  [ 2390/60301]  eta: 7:58:45  lr: 0.000031  min_lr: 0.000001  loss: 5.7056 (5.8885)  loss_scale: 65536.0000 (9360.3814)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0024  max mem: 8161
Epoch: [0]  [ 2400/60301]  eta: 7:58:37  lr: 0.000031  min_lr: 0.000001  loss: 5.7641 (5.8880)  loss_scale: 65536.0000 (9594.3490)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2410/60301]  eta: 7:58:28  lr: 0.000031  min_lr: 0.000001  loss: 5.7221 (5.8874)  loss_scale: 65536.0000 (9826.3758)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2420/60301]  eta: 7:58:21  lr: 0.000031  min_lr: 0.000001  loss: 5.7221 (5.8869)  loss_scale: 65536.0000 (10056.4857)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2430/60301]  eta: 7:58:13  lr: 0.000031  min_lr: 0.000001  loss: 5.7856 (5.8867)  loss_scale: 65536.0000 (10284.7026)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0006  max mem: 8161
Epoch: [0]  [ 2440/60301]  eta: 7:58:05  lr: 0.000031  min_lr: 0.000001  loss: 5.7856 (5.8864)  loss_scale: 65536.0000 (10511.0496)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0016  max mem: 8161
Epoch: [0]  [ 2450/60301]  eta: 7:57:57  lr: 0.000031  min_lr: 0.000001  loss: 5.7718 (5.8861)  loss_scale: 65536.0000 (10735.5496)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2460/60301]  eta: 7:57:48  lr: 0.000031  min_lr: 0.000001  loss: 5.7304 (5.8855)  loss_scale: 65536.0000 (10958.2251)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0014  max mem: 8161
Epoch: [0]  [ 2470/60301]  eta: 7:57:40  lr: 0.000031  min_lr: 0.000001  loss: 5.7180 (5.8850)  loss_scale: 65536.0000 (11179.0983)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2480/60301]  eta: 7:57:44  lr: 0.000031  min_lr: 0.000001  loss: 5.7700 (5.8847)  loss_scale: 65536.0000 (11398.1911)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0250  max mem: 8161
Epoch: [0]  [ 2490/60301]  eta: 7:57:42  lr: 0.000031  min_lr: 0.000001  loss: 5.7700 (5.8839)  loss_scale: 65536.0000 (11615.5247)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0405  max mem: 8161
Epoch: [0]  [ 2500/60301]  eta: 7:57:35  lr: 0.000031  min_lr: 0.000001  loss: 5.7109 (5.8834)  loss_scale: 65536.0000 (11831.1204)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0159  max mem: 8161
Epoch: [0]  [ 2510/60301]  eta: 7:57:28  lr: 0.000031  min_lr: 0.000001  loss: 5.6117 (5.8824)  loss_scale: 65536.0000 (12044.9988)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2520/60301]  eta: 7:57:20  lr: 0.000031  min_lr: 0.000001  loss: 5.6854 (5.8821)  loss_scale: 65536.0000 (12257.1805)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2530/60301]  eta: 7:57:48  lr: 0.000031  min_lr: 0.000001  loss: 5.7999 (5.8818)  loss_scale: 65536.0000 (12467.6855)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0803  max mem: 8161
Epoch: [0]  [ 2540/60301]  eta: 7:57:39  lr: 0.000031  min_lr: 0.000001  loss: 5.7076 (5.8809)  loss_scale: 65536.0000 (12676.5336)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0803  max mem: 8161
Epoch: [0]  [ 2550/60301]  eta: 7:57:02  lr: 0.000031  min_lr: 0.000001  loss: 5.6782 (5.8803)  loss_scale: 65536.0000 (12883.7444)  weight_decay: 0.0500 (0.0500)  time: 0.4159  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2560/60301]  eta: 7:56:02  lr: 0.000031  min_lr: 0.000001  loss: 5.7531 (5.8798)  loss_scale: 65536.0000 (13089.3370)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0699  max mem: 8161
[2023-07-14 00:40:41,335] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:40:41,335] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [ 2570/60301]  eta: 7:57:15  lr: 0.000031  min_lr: 0.000001  loss: 5.8303 (5.8797)  loss_scale: 65536.0000 (13548.2349)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.4341  max mem: 8161
Epoch: [0]  [ 2580/60301]  eta: 7:57:03  lr: 0.000031  min_lr: 0.000001  loss: 5.7234 (5.8789)  loss_scale: 131072.0000 (14003.5769)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.5415  max mem: 8161
Epoch: [0]  [ 2590/60301]  eta: 7:56:52  lr: 0.000031  min_lr: 0.000001  loss: 5.6970 (5.8784)  loss_scale: 131072.0000 (14455.4041)  weight_decay: 0.0500 (0.0500)  time: 0.4677  data: 0.2007  max mem: 8161
Epoch: [0]  [ 2600/60301]  eta: 7:56:47  lr: 0.000031  min_lr: 0.000001  loss: 5.7127 (5.8778)  loss_scale: 131072.0000 (14903.7570)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0261  max mem: 8161
Epoch: [0]  [ 2610/60301]  eta: 7:56:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7470 (5.8774)  loss_scale: 131072.0000 (15348.6756)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0035  max mem: 8161
Epoch: [0]  [ 2620/60301]  eta: 7:56:33  lr: 0.000031  min_lr: 0.000001  loss: 5.7580 (5.8772)  loss_scale: 131072.0000 (15790.1992)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2630/60301]  eta: 7:56:27  lr: 0.000031  min_lr: 0.000001  loss: 5.8101 (5.8767)  loss_scale: 131072.0000 (16228.3664)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2640/60301]  eta: 7:56:19  lr: 0.000031  min_lr: 0.000001  loss: 5.7298 (5.8762)  loss_scale: 131072.0000 (16663.2154)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2650/60301]  eta: 7:56:14  lr: 0.000031  min_lr: 0.000001  loss: 5.7163 (5.8757)  loss_scale: 131072.0000 (17094.7839)  weight_decay: 0.0500 (0.0500)  time: 0.4889  data: 0.0061  max mem: 8161
Epoch: [0]  [ 2660/60301]  eta: 7:56:07  lr: 0.000031  min_lr: 0.000001  loss: 5.7163 (5.8751)  loss_scale: 131072.0000 (17523.1086)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0061  max mem: 8161
Epoch: [0]  [ 2670/60301]  eta: 7:56:00  lr: 0.000031  min_lr: 0.000001  loss: 5.7106 (5.8745)  loss_scale: 131072.0000 (17948.2261)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2680/60301]  eta: 7:55:53  lr: 0.000031  min_lr: 0.000001  loss: 5.7503 (5.8743)  loss_scale: 131072.0000 (18370.1723)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2690/60301]  eta: 7:55:47  lr: 0.000031  min_lr: 0.000001  loss: 5.8052 (5.8738)  loss_scale: 131072.0000 (18788.9825)  weight_decay: 0.0500 (0.0500)  time: 0.4893  data: 0.0023  max mem: 8161
Epoch: [0]  [ 2700/60301]  eta: 7:55:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7743 (5.8736)  loss_scale: 131072.0000 (19204.6916)  weight_decay: 0.0500 (0.0500)  time: 0.4897  data: 0.0023  max mem: 8161
Epoch: [0]  [ 2710/60301]  eta: 7:55:33  lr: 0.000031  min_lr: 0.000001  loss: 5.7278 (5.8726)  loss_scale: 131072.0000 (19617.3338)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2720/60301]  eta: 7:55:27  lr: 0.000031  min_lr: 0.000001  loss: 5.6939 (5.8724)  loss_scale: 131072.0000 (20026.9430)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2730/60301]  eta: 7:55:20  lr: 0.000031  min_lr: 0.000001  loss: 5.8150 (5.8719)  loss_scale: 131072.0000 (20433.5525)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0024  max mem: 8161
Epoch: [0]  [ 2740/60301]  eta: 7:55:13  lr: 0.000031  min_lr: 0.000001  loss: 5.6521 (5.8710)  loss_scale: 131072.0000 (20837.1952)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0015  max mem: 8161
Epoch: [0]  [ 2750/60301]  eta: 7:55:57  lr: 0.000031  min_lr: 0.000001  loss: 5.6907 (5.8709)  loss_scale: 131072.0000 (21237.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1211  max mem: 8161
Epoch: [0]  [ 2760/60301]  eta: 7:55:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6990 (5.8702)  loss_scale: 131072.0000 (21635.7088)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1220  max mem: 8161
Epoch: [0]  [ 2770/60301]  eta: 7:55:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6990 (5.8700)  loss_scale: 131072.0000 (22030.6431)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0166  max mem: 8161
Epoch: [0]  [ 2780/60301]  eta: 7:55:42  lr: 0.000031  min_lr: 0.000001  loss: 5.7610 (5.8694)  loss_scale: 131072.0000 (22422.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0156  max mem: 8161
Epoch: [0]  [ 2790/60301]  eta: 7:55:35  lr: 0.000031  min_lr: 0.000001  loss: 5.6935 (5.8690)  loss_scale: 131072.0000 (22812.0215)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0004  max mem: 8161
Epoch: [0]  [ 2800/60301]  eta: 7:55:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6949 (5.8685)  loss_scale: 131072.0000 (23198.5262)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0012  max mem: 8161
Epoch: [0]  [ 2810/60301]  eta: 7:55:39  lr: 0.000031  min_lr: 0.000001  loss: 5.7433 (5.8680)  loss_scale: 131072.0000 (23582.2810)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0427  max mem: 8161
[2023-07-14 00:42:52,511] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:42:52,511] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [ 2820/60301]  eta: 7:55:32  lr: 0.000031  min_lr: 0.000001  loss: 5.7297 (5.8674)  loss_scale: 131072.0000 (24149.1670)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0420  max mem: 8161
[2023-07-14 00:42:55,437] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1411
[2023-07-14 00:42:55,437] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2023-07-14 00:42:55,437] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [ 2830/60301]  eta: 7:55:28  lr: 0.000031  min_lr: 0.000001  loss: 5.7297 (5.8674)  loss_scale: 131072.0000 (24619.4504)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0114  max mem: 8161
Epoch: [0]  [ 2840/60301]  eta: 7:55:21  lr: 0.000031  min_lr: 0.000001  loss: 5.7195 (5.8670)  loss_scale: 131072.0000 (24994.1514)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0121  max mem: 8161
Epoch: [0]  [ 2850/60301]  eta: 7:55:22  lr: 0.000031  min_lr: 0.000001  loss: 5.6950 (5.8664)  loss_scale: 131072.0000 (25366.2238)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0216  max mem: 8161
Epoch: [0]  [ 2860/60301]  eta: 7:55:15  lr: 0.000031  min_lr: 0.000001  loss: 5.7033 (5.8662)  loss_scale: 131072.0000 (25735.6952)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0210  max mem: 8161
Epoch: [0]  [ 2870/60301]  eta: 7:55:08  lr: 0.000031  min_lr: 0.000001  loss: 5.7621 (5.8660)  loss_scale: 131072.0000 (26102.5928)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2880/60301]  eta: 7:55:11  lr: 0.000031  min_lr: 0.000001  loss: 5.7507 (5.8653)  loss_scale: 131072.0000 (26466.9434)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0263  max mem: 8161
Epoch: [0]  [ 2890/60301]  eta: 7:55:03  lr: 0.000031  min_lr: 0.000001  loss: 5.7507 (5.8649)  loss_scale: 131072.0000 (26828.7734)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0263  max mem: 8161
Epoch: [0]  [ 2900/60301]  eta: 7:54:56  lr: 0.000031  min_lr: 0.000001  loss: 5.7110 (5.8643)  loss_scale: 131072.0000 (27188.1089)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2910/60301]  eta: 7:54:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6870 (5.8637)  loss_scale: 131072.0000 (27544.9756)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0013  max mem: 8161
Epoch: [0]  [ 2920/60301]  eta: 7:55:10  lr: 0.000031  min_lr: 0.000001  loss: 5.6961 (5.8629)  loss_scale: 131072.0000 (27899.3988)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0718  max mem: 8161
Epoch: [0]  [ 2930/60301]  eta: 7:55:03  lr: 0.000031  min_lr: 0.000001  loss: 5.6649 (5.8623)  loss_scale: 131072.0000 (28251.4036)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0723  max mem: 8161
Epoch: [0]  [ 2940/60301]  eta: 7:54:57  lr: 0.000031  min_lr: 0.000001  loss: 5.7285 (5.8620)  loss_scale: 131072.0000 (28601.0146)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0009  max mem: 8161
Epoch: [0]  [ 2950/60301]  eta: 7:54:52  lr: 0.000031  min_lr: 0.000001  loss: 5.7757 (5.8617)  loss_scale: 131072.0000 (28948.2562)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0070  max mem: 8161
Epoch: [0]  [ 2960/60301]  eta: 7:54:45  lr: 0.000031  min_lr: 0.000001  loss: 5.7250 (5.8612)  loss_scale: 131072.0000 (29293.1523)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0070  max mem: 8161
Epoch: [0]  [ 2970/60301]  eta: 7:54:38  lr: 0.000031  min_lr: 0.000001  loss: 5.6764 (5.8607)  loss_scale: 131072.0000 (29635.7267)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2980/60301]  eta: 7:54:31  lr: 0.000031  min_lr: 0.000001  loss: 5.7921 (5.8604)  loss_scale: 131072.0000 (29976.0027)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0005  max mem: 8161
Epoch: [0]  [ 2990/60301]  eta: 7:54:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7959 (5.8600)  loss_scale: 131072.0000 (30314.0033)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0014  max mem: 8161
[2023-07-14 00:44:23,817] [INFO] [timer.py:181:stop] 0/3000, SamplesPerSec=9.145204528069652
Epoch: [0]  [ 3000/60301]  eta: 7:54:18  lr: 0.000031  min_lr: 0.000001  loss: 5.7495 (5.8597)  loss_scale: 131072.0000 (30649.7514)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0033  max mem: 8161
Epoch: [0]  [ 3010/60301]  eta: 7:54:12  lr: 0.000031  min_lr: 0.000001  loss: 5.6873 (5.8590)  loss_scale: 131072.0000 (30983.2693)  weight_decay: 0.0500 (0.0500)  time: 0.4899  data: 0.0023  max mem: 8161
Epoch: [0]  [ 3020/60301]  eta: 7:54:04  lr: 0.000031  min_lr: 0.000001  loss: 5.6710 (5.8583)  loss_scale: 131072.0000 (31314.5793)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3030/60301]  eta: 7:53:56  lr: 0.000031  min_lr: 0.000001  loss: 5.6992 (5.8580)  loss_scale: 131072.0000 (31643.7031)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3040/60301]  eta: 7:53:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6986 (5.8576)  loss_scale: 131072.0000 (31970.6623)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0260  max mem: 8161
Epoch: [0]  [ 3050/60301]  eta: 7:53:56  lr: 0.000031  min_lr: 0.000001  loss: 5.6863 (5.8571)  loss_scale: 131072.0000 (32295.4782)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0362  max mem: 8161
Epoch: [0]  [ 3060/60301]  eta: 7:53:50  lr: 0.000031  min_lr: 0.000001  loss: 5.7128 (5.8567)  loss_scale: 131072.0000 (32618.1718)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0116  max mem: 8161
Epoch: [0]  [ 3070/60301]  eta: 7:53:43  lr: 0.000031  min_lr: 0.000001  loss: 5.7074 (5.8562)  loss_scale: 131072.0000 (32938.7639)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0014  max mem: 8161
Epoch: [0]  [ 3080/60301]  eta: 7:53:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6731 (5.8555)  loss_scale: 131072.0000 (33257.2749)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0013  max mem: 8161
[2023-07-14 00:45:04,391] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:45:04,391] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 00:45:05,350] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1541
[2023-07-14 00:45:05,350] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:45:05,350] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 3090/60301]  eta: 7:53:28  lr: 0.000031  min_lr: 0.000001  loss: 5.6578 (5.8550)  loss_scale: 131072.0000 (33658.5338)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0013  max mem: 8161
Epoch: [0]  [ 3100/60301]  eta: 7:53:24  lr: 0.000031  min_lr: 0.000001  loss: 5.7506 (5.8547)  loss_scale: 131072.0000 (33972.6695)  weight_decay: 0.0500 (0.0500)  time: 0.4903  data: 0.0087  max mem: 8161
Epoch: [0]  [ 3110/60301]  eta: 7:53:18  lr: 0.000031  min_lr: 0.000001  loss: 5.7506 (5.8542)  loss_scale: 131072.0000 (34284.7856)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0087  max mem: 8161
Epoch: [0]  [ 3120/60301]  eta: 7:53:11  lr: 0.000031  min_lr: 0.000001  loss: 5.7483 (5.8538)  loss_scale: 131072.0000 (34594.9016)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3130/60301]  eta: 7:53:05  lr: 0.000031  min_lr: 0.000001  loss: 5.7410 (5.8535)  loss_scale: 131072.0000 (34903.0367)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0014  max mem: 8161
Epoch: [0]  [ 3140/60301]  eta: 7:53:01  lr: 0.000031  min_lr: 0.000001  loss: 5.6827 (5.8526)  loss_scale: 131072.0000 (35209.2098)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0094  max mem: 8161
Epoch: [0]  [ 3150/60301]  eta: 7:52:55  lr: 0.000031  min_lr: 0.000001  loss: 5.6022 (5.8519)  loss_scale: 131072.0000 (35513.4395)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0092  max mem: 8161
Epoch: [0]  [ 3160/60301]  eta: 7:52:48  lr: 0.000031  min_lr: 0.000001  loss: 5.6316 (5.8513)  loss_scale: 131072.0000 (35815.7444)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0021  max mem: 8161
Epoch: [0]  [ 3170/60301]  eta: 7:52:42  lr: 0.000031  min_lr: 0.000001  loss: 5.7072 (5.8511)  loss_scale: 131072.0000 (36116.1425)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0013  max mem: 8161
Epoch: [0]  [ 3180/60301]  eta: 7:52:35  lr: 0.000031  min_lr: 0.000001  loss: 5.7615 (5.8506)  loss_scale: 131072.0000 (36414.6520)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0004  max mem: 8161
Epoch: [0]  [ 3190/60301]  eta: 7:53:23  lr: 0.000031  min_lr: 0.000001  loss: 5.7352 (5.8502)  loss_scale: 131072.0000 (36711.2905)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1545  max mem: 8161
Epoch: [0]  [ 3200/60301]  eta: 7:53:16  lr: 0.000031  min_lr: 0.000001  loss: 5.7555 (5.8500)  loss_scale: 131072.0000 (37006.0756)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1546  max mem: 8161
Epoch: [0]  [ 3210/60301]  eta: 7:53:39  lr: 0.000031  min_lr: 0.000001  loss: 5.7159 (5.8493)  loss_scale: 131072.0000 (37299.0246)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0860  max mem: 8161
Epoch: [0]  [ 3220/60301]  eta: 7:53:32  lr: 0.000031  min_lr: 0.000001  loss: 5.6064 (5.8487)  loss_scale: 131072.0000 (37590.1546)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0860  max mem: 8161
Epoch: [0]  [ 3230/60301]  eta: 7:53:49  lr: 0.000031  min_lr: 0.000001  loss: 5.5775 (5.8480)  loss_scale: 131072.0000 (37879.4825)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0699  max mem: 8161
Epoch: [0]  [ 3240/60301]  eta: 7:53:42  lr: 0.000031  min_lr: 0.000001  loss: 5.6685 (5.8478)  loss_scale: 131072.0000 (38167.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0699  max mem: 8161
Epoch: [0]  [ 3250/60301]  eta: 7:53:35  lr: 0.000031  min_lr: 0.000001  loss: 5.7504 (5.8474)  loss_scale: 131072.0000 (38452.7985)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3260/60301]  eta: 7:53:30  lr: 0.000031  min_lr: 0.000001  loss: 5.7028 (5.8470)  loss_scale: 131072.0000 (38736.8194)  weight_decay: 0.0500 (0.0500)  time: 0.4905  data: 0.0048  max mem: 8161
Epoch: [0]  [ 3270/60301]  eta: 7:53:27  lr: 0.000031  min_lr: 0.000001  loss: 5.5582 (5.8460)  loss_scale: 131072.0000 (39019.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0191  max mem: 8161
Epoch: [0]  [ 3280/60301]  eta: 7:53:22  lr: 0.000031  min_lr: 0.000001  loss: 5.6402 (5.8457)  loss_scale: 131072.0000 (39299.6672)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0216  max mem: 8161
Epoch: [0]  [ 3290/60301]  eta: 7:53:15  lr: 0.000031  min_lr: 0.000001  loss: 5.7283 (5.8453)  loss_scale: 131072.0000 (39578.5257)  weight_decay: 0.0500 (0.0500)  time: 0.4911  data: 0.0072  max mem: 8161
Epoch: [0]  [ 3300/60301]  eta: 7:53:50  lr: 0.000031  min_lr: 0.000001  loss: 5.7480 (5.8452)  loss_scale: 131072.0000 (39855.6946)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1233  max mem: 8161
Epoch: [0]  [ 3310/60301]  eta: 7:53:49  lr: 0.000031  min_lr: 0.000001  loss: 5.7756 (5.8449)  loss_scale: 131072.0000 (40131.1894)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1420  max mem: 8161
Epoch: [0]  [ 3320/60301]  eta: 7:53:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6510 (5.8443)  loss_scale: 131072.0000 (40405.0250)  weight_decay: 0.0500 (0.0500)  time: 0.4436  data: 0.0676  max mem: 8161
Epoch: [0]  [ 3330/60301]  eta: 7:53:15  lr: 0.000031  min_lr: 0.000001  loss: 5.6781 (5.8439)  loss_scale: 131072.0000 (40677.2165)  weight_decay: 0.0500 (0.0500)  time: 0.4295  data: 0.2396  max mem: 8161
Epoch: [0]  [ 3340/60301]  eta: 7:53:07  lr: 0.000031  min_lr: 0.000001  loss: 5.7161 (5.8435)  loss_scale: 131072.0000 (40947.7785)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.3749  max mem: 8161
[2023-07-14 00:47:18,908] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:47:18,908] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 00:47:19,376] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1672
[2023-07-14 00:47:19,376] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:47:19,376] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 3350/60301]  eta: 7:53:11  lr: 0.000031  min_lr: 0.000001  loss: 5.7286 (5.8433)  loss_scale: 131072.0000 (41373.1829)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.4022  max mem: 8161
Epoch: [0]  [ 3360/60301]  eta: 7:53:09  lr: 0.000031  min_lr: 0.000001  loss: 5.6023 (5.8426)  loss_scale: 131072.0000 (41640.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.3063  max mem: 8161
Epoch: [0]  [ 3370/60301]  eta: 7:53:02  lr: 0.000031  min_lr: 0.000001  loss: 5.6900 (5.8425)  loss_scale: 131072.0000 (41905.3622)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0883  max mem: 8161
Epoch: [0]  [ 3380/60301]  eta: 7:52:54  lr: 0.000031  min_lr: 0.000001  loss: 5.7330 (5.8422)  loss_scale: 131072.0000 (42169.0908)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0004  max mem: 8161
Epoch: [0]  [ 3390/60301]  eta: 7:52:53  lr: 0.000031  min_lr: 0.000001  loss: 5.7193 (5.8418)  loss_scale: 131072.0000 (42431.2639)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0190  max mem: 8161
Epoch: [0]  [ 3400/60301]  eta: 7:52:45  lr: 0.000031  min_lr: 0.000001  loss: 5.7151 (5.8415)  loss_scale: 131072.0000 (42691.8953)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0190  max mem: 8161
Epoch: [0]  [ 3410/60301]  eta: 7:52:40  lr: 0.000031  min_lr: 0.000001  loss: 5.7917 (5.8413)  loss_scale: 131072.0000 (42950.9985)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0050  max mem: 8161
Epoch: [0]  [ 3420/60301]  eta: 7:52:42  lr: 0.000031  min_lr: 0.000001  loss: 5.7264 (5.8407)  loss_scale: 131072.0000 (43208.5870)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0309  max mem: 8161
Epoch: [0]  [ 3430/60301]  eta: 7:52:48  lr: 0.000031  min_lr: 0.000001  loss: 5.6093 (5.8400)  loss_scale: 131072.0000 (43464.6739)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0638  max mem: 8161
Epoch: [0]  [ 3440/60301]  eta: 7:52:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5801 (5.8395)  loss_scale: 131072.0000 (43719.2723)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0379  max mem: 8161
Epoch: [0]  [ 3450/60301]  eta: 7:52:33  lr: 0.000031  min_lr: 0.000001  loss: 5.6507 (5.8391)  loss_scale: 131072.0000 (43972.3952)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3460/60301]  eta: 7:52:26  lr: 0.000031  min_lr: 0.000001  loss: 5.6588 (5.8387)  loss_scale: 131072.0000 (44224.0555)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3470/60301]  eta: 7:52:19  lr: 0.000031  min_lr: 0.000001  loss: 5.6955 (5.8383)  loss_scale: 131072.0000 (44474.2656)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0016  max mem: 8161
Epoch: [0]  [ 3480/60301]  eta: 7:52:12  lr: 0.000031  min_lr: 0.000001  loss: 5.7700 (5.8384)  loss_scale: 131072.0000 (44723.0382)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0016  max mem: 8161
Epoch: [0]  [ 3490/60301]  eta: 7:52:05  lr: 0.000031  min_lr: 0.000001  loss: 5.7538 (5.8379)  loss_scale: 131072.0000 (44970.3856)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0013  max mem: 8161
Epoch: [0]  [ 3500/60301]  eta: 7:51:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6313 (5.8373)  loss_scale: 131072.0000 (45216.3199)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0031  max mem: 8161
Epoch: [0]  [ 3510/60301]  eta: 7:51:52  lr: 0.000031  min_lr: 0.000001  loss: 5.5754 (5.8366)  loss_scale: 131072.0000 (45460.8533)  weight_decay: 0.0500 (0.0500)  time: 0.4885  data: 0.0023  max mem: 8161
Epoch: [0]  [ 3520/60301]  eta: 7:51:46  lr: 0.000031  min_lr: 0.000001  loss: 5.5802 (5.8359)  loss_scale: 131072.0000 (45703.9977)  weight_decay: 0.0500 (0.0500)  time: 0.4898  data: 0.0026  max mem: 8161
Epoch: [0]  [ 3530/60301]  eta: 7:51:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5541 (5.8352)  loss_scale: 131072.0000 (45945.7649)  weight_decay: 0.0500 (0.0500)  time: 0.4909  data: 0.0025  max mem: 8161
Epoch: [0]  [ 3540/60301]  eta: 7:51:49  lr: 0.000031  min_lr: 0.000001  loss: 5.5286 (5.8345)  loss_scale: 131072.0000 (46186.1666)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0494  max mem: 8161
Epoch: [0]  [ 3550/60301]  eta: 7:51:42  lr: 0.000031  min_lr: 0.000001  loss: 5.6434 (5.8340)  loss_scale: 131072.0000 (46425.2143)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0495  max mem: 8161
Epoch: [0]  [ 3560/60301]  eta: 7:51:35  lr: 0.000031  min_lr: 0.000001  loss: 5.7011 (5.8337)  loss_scale: 131072.0000 (46662.9194)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3570/60301]  eta: 7:51:29  lr: 0.000031  min_lr: 0.000001  loss: 5.7196 (5.8332)  loss_scale: 131072.0000 (46899.2932)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3580/60301]  eta: 7:51:40  lr: 0.000031  min_lr: 0.000001  loss: 5.7021 (5.8328)  loss_scale: 131072.0000 (47134.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0600  max mem: 8161
Epoch: [0]  [ 3590/60301]  eta: 7:51:33  lr: 0.000031  min_lr: 0.000001  loss: 5.7021 (5.8324)  loss_scale: 131072.0000 (47368.0913)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0609  max mem: 8161
Epoch: [0]  [ 3600/60301]  eta: 7:51:44  lr: 0.000031  min_lr: 0.000001  loss: 5.6452 (5.8319)  loss_scale: 131072.0000 (47600.5376)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0567  max mem: 8161
[2023-07-14 00:49:32,958] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:49:32,958] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 3610/60301]  eta: 7:51:37  lr: 0.000031  min_lr: 0.000001  loss: 5.6455 (5.8315)  loss_scale: 131072.0000 (48122.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0557  max mem: 8161
Epoch: [0]  [ 3620/60301]  eta: 7:51:30  lr: 0.000031  min_lr: 0.000001  loss: 5.6455 (5.8309)  loss_scale: 262144.0000 (48713.1378)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.0005  max mem: 8161
[2023-07-14 00:49:41,745] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1810
[2023-07-14 00:49:41,745] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:49:41,746] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 3630/60301]  eta: 7:51:22  lr: 0.000031  min_lr: 0.000001  loss: 5.6514 (5.8304)  loss_scale: 131072.0000 (48939.9592)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3640/60301]  eta: 7:51:15  lr: 0.000031  min_lr: 0.000001  loss: 5.6659 (5.8299)  loss_scale: 131072.0000 (49165.5347)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3650/60301]  eta: 7:51:08  lr: 0.000031  min_lr: 0.000001  loss: 5.7112 (5.8297)  loss_scale: 131072.0000 (49389.8746)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0012  max mem: 8161
Epoch: [0]  [ 3660/60301]  eta: 7:51:01  lr: 0.000031  min_lr: 0.000001  loss: 5.7054 (5.8292)  loss_scale: 131072.0000 (49612.9888)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0019  max mem: 8161
Epoch: [0]  [ 3670/60301]  eta: 7:50:55  lr: 0.000031  min_lr: 0.000001  loss: 5.7054 (5.8292)  loss_scale: 131072.0000 (49834.8875)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0021  max mem: 8161
Epoch: [0]  [ 3680/60301]  eta: 7:50:48  lr: 0.000031  min_lr: 0.000001  loss: 5.7641 (5.8288)  loss_scale: 131072.0000 (50055.5805)  weight_decay: 0.0500 (0.0500)  time: 0.4891  data: 0.0014  max mem: 8161
Epoch: [0]  [ 3690/60301]  eta: 7:50:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7251 (5.8285)  loss_scale: 131072.0000 (50275.0778)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0004  max mem: 8161
Epoch: [0]  [ 3700/60301]  eta: 7:50:34  lr: 0.000031  min_lr: 0.000001  loss: 5.6240 (5.8279)  loss_scale: 131072.0000 (50493.3888)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3710/60301]  eta: 7:50:28  lr: 0.000031  min_lr: 0.000001  loss: 5.6471 (5.8275)  loss_scale: 131072.0000 (50710.5233)  weight_decay: 0.0500 (0.0500)  time: 0.4891  data: 0.0025  max mem: 8161
Epoch: [0]  [ 3720/60301]  eta: 7:50:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6741 (5.8270)  loss_scale: 131072.0000 (50926.4907)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0025  max mem: 8161
Epoch: [0]  [ 3730/60301]  eta: 7:50:14  lr: 0.000031  min_lr: 0.000001  loss: 5.6505 (5.8267)  loss_scale: 131072.0000 (51141.3005)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3740/60301]  eta: 7:50:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6536 (5.8264)  loss_scale: 131072.0000 (51354.9618)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0005  max mem: 8161
Epoch: [0]  [ 3750/60301]  eta: 7:50:01  lr: 0.000031  min_lr: 0.000001  loss: 5.6536 (5.8259)  loss_scale: 131072.0000 (51567.4839)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0006  max mem: 8161
Epoch: [0]  [ 3760/60301]  eta: 7:49:54  lr: 0.000031  min_lr: 0.000001  loss: 5.6355 (5.8257)  loss_scale: 131072.0000 (51778.8758)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0006  max mem: 8161
Epoch: [0]  [ 3770/60301]  eta: 7:49:47  lr: 0.000031  min_lr: 0.000001  loss: 5.6843 (5.8255)  loss_scale: 131072.0000 (51989.1466)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0015  max mem: 8161
Epoch: [0]  [ 3780/60301]  eta: 7:49:41  lr: 0.000031  min_lr: 0.000001  loss: 5.6164 (5.8252)  loss_scale: 131072.0000 (52198.3052)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0015  max mem: 8161
Epoch: [0]  [ 3790/60301]  eta: 7:49:34  lr: 0.000031  min_lr: 0.000001  loss: 5.7171 (5.8249)  loss_scale: 131072.0000 (52406.3603)  weight_decay: 0.0500 (0.0500)  time: 0.4883  data: 0.0011  max mem: 8161
Epoch: [0]  [ 3800/60301]  eta: 7:49:17  lr: 0.000031  min_lr: 0.000001  loss: 5.7171 (5.8244)  loss_scale: 131072.0000 (52613.3207)  weight_decay: 0.0500 (0.0500)  time: 0.4516  data: 0.0011  max mem: 8161
Epoch: [0]  [ 3810/60301]  eta: 7:49:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6858 (5.8240)  loss_scale: 131072.0000 (52819.1950)  weight_decay: 0.0500 (0.0500)  time: 0.4425  data: 0.1794  max mem: 8161
Epoch: [0]  [ 3820/60301]  eta: 7:49:51  lr: 0.000031  min_lr: 0.000001  loss: 5.7262 (5.8238)  loss_scale: 131072.0000 (53023.9916)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.5362  max mem: 8161
Epoch: [0]  [ 3830/60301]  eta: 7:49:42  lr: 0.000031  min_lr: 0.000001  loss: 5.7262 (5.8234)  loss_scale: 131072.0000 (53227.7191)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.5023  max mem: 8161
Epoch: [0]  [ 3840/60301]  eta: 7:49:49  lr: 0.000031  min_lr: 0.000001  loss: 5.7029 (5.8230)  loss_scale: 131072.0000 (53430.3858)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.1951  max mem: 8161
Epoch: [0]  [ 3850/60301]  eta: 7:49:41  lr: 0.000031  min_lr: 0.000001  loss: 5.7016 (5.8228)  loss_scale: 131072.0000 (53632.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0500  max mem: 8161
Epoch: [0]  [ 3860/60301]  eta: 7:49:43  lr: 0.000031  min_lr: 0.000001  loss: 5.6593 (5.8221)  loss_scale: 131072.0000 (53832.5698)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0297  max mem: 8161
Epoch: [0]  [ 3870/60301]  eta: 7:49:35  lr: 0.000031  min_lr: 0.000001  loss: 5.6014 (5.8215)  loss_scale: 131072.0000 (54032.1033)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0298  max mem: 8161
[2023-07-14 00:51:51,232] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:51:51,232] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 3880/60301]  eta: 7:49:28  lr: 0.000031  min_lr: 0.000001  loss: 5.6372 (5.8211)  loss_scale: 131072.0000 (54298.1541)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0013  max mem: 8161
Epoch: [0]  [ 3890/60301]  eta: 7:49:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6065 (5.8206)  loss_scale: 262144.0000 (54832.3249)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0013  max mem: 8161
Epoch: [0]  [ 3900/60301]  eta: 7:49:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5482 (5.8199)  loss_scale: 262144.0000 (55363.7570)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0014  max mem: 8161
Epoch: [0]  [ 3910/60301]  eta: 7:49:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5209 (5.8193)  loss_scale: 262144.0000 (55892.4715)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0808  max mem: 8161
[2023-07-14 00:52:11,716] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1958
[2023-07-14 00:52:11,716] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:52:11,717] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 3920/60301]  eta: 7:49:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6497 (5.8190)  loss_scale: 262144.0000 (56284.7763)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1099  max mem: 8161
Epoch: [0]  [ 3930/60301]  eta: 7:49:21  lr: 0.000031  min_lr: 0.000001  loss: 5.7208 (5.8187)  loss_scale: 131072.0000 (56475.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0305  max mem: 8161
Epoch: [0]  [ 3940/60301]  eta: 7:49:16  lr: 0.000031  min_lr: 0.000001  loss: 5.5716 (5.8179)  loss_scale: 131072.0000 (56664.3106)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0064  max mem: 8161
Epoch: [0]  [ 3950/60301]  eta: 7:49:08  lr: 0.000031  min_lr: 0.000001  loss: 5.5909 (5.8175)  loss_scale: 131072.0000 (56852.6368)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0064  max mem: 8161
Epoch: [0]  [ 3960/60301]  eta: 7:49:01  lr: 0.000031  min_lr: 0.000001  loss: 5.7395 (5.8172)  loss_scale: 131072.0000 (57040.0121)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0014  max mem: 8161
Epoch: [0]  [ 3970/60301]  eta: 7:48:54  lr: 0.000031  min_lr: 0.000001  loss: 5.6849 (5.8168)  loss_scale: 131072.0000 (57226.4437)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0023  max mem: 8161
Epoch: [0]  [ 3980/60301]  eta: 7:48:46  lr: 0.000031  min_lr: 0.000001  loss: 5.7043 (5.8167)  loss_scale: 131072.0000 (57411.9387)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0023  max mem: 8161
Epoch: [0]  [ 3990/60301]  eta: 7:48:22  lr: 0.000031  min_lr: 0.000001  loss: 5.7078 (5.8163)  loss_scale: 131072.0000 (57596.5041)  weight_decay: 0.0500 (0.0500)  time: 0.4207  data: 0.0417  max mem: 8161
[2023-07-14 00:52:48,398] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=5, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 00:52:48,398] [INFO] [timer.py:181:stop] 0/4000, SamplesPerSec=9.11047216600711
Epoch: [0]  [ 4000/60301]  eta: 7:48:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6697 (5.8160)  loss_scale: 131072.0000 (57780.1470)  weight_decay: 0.0500 (0.0500)  time: 0.3998  data: 0.2049  max mem: 8161
Epoch: [0]  [ 4010/60301]  eta: 7:47:43  lr: 0.000031  min_lr: 0.000001  loss: 5.6078 (5.8154)  loss_scale: 131072.0000 (57962.8741)  weight_decay: 0.0500 (0.0500)  time: 0.3960  data: 0.2835  max mem: 8161
Epoch: [0]  [ 4020/60301]  eta: 7:47:46  lr: 0.000031  min_lr: 0.000001  loss: 5.5796 (5.8149)  loss_scale: 131072.0000 (58144.6924)  weight_decay: 0.0500 (0.0500)  time: 0.4535  data: 0.3408  max mem: 8161
Epoch: [0]  [ 4030/60301]  eta: 7:47:45  lr: 0.000031  min_lr: 0.000001  loss: 5.5661 (5.8143)  loss_scale: 131072.0000 (58325.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.3145  max mem: 8161
Epoch: [0]  [ 4040/60301]  eta: 7:47:44  lr: 0.000031  min_lr: 0.000001  loss: 5.6929 (5.8141)  loss_scale: 131072.0000 (58505.6293)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.1166  max mem: 8161
Epoch: [0]  [ 4050/60301]  eta: 7:47:37  lr: 0.000031  min_lr: 0.000001  loss: 5.6287 (5.8136)  loss_scale: 131072.0000 (58684.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0242  max mem: 8161
Epoch: [0]  [ 4060/60301]  eta: 7:47:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6577 (5.8133)  loss_scale: 131072.0000 (58863.0111)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4070/60301]  eta: 7:47:22  lr: 0.000031  min_lr: 0.000001  loss: 5.6923 (5.8129)  loss_scale: 131072.0000 (59040.3852)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0013  max mem: 8161
Epoch: [0]  [ 4080/60301]  eta: 7:48:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5683 (5.8122)  loss_scale: 131072.0000 (59216.8900)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.1878  max mem: 8161
Epoch: [0]  [ 4090/60301]  eta: 7:47:58  lr: 0.000031  min_lr: 0.000001  loss: 5.4711 (5.8116)  loss_scale: 131072.0000 (59392.5319)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.1878  max mem: 8161
Epoch: [0]  [ 4100/60301]  eta: 7:47:51  lr: 0.000031  min_lr: 0.000001  loss: 5.6134 (5.8111)  loss_scale: 131072.0000 (59567.3172)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0004  max mem: 8161
Epoch: [0]  [ 4110/60301]  eta: 7:47:43  lr: 0.000031  min_lr: 0.000001  loss: 5.6306 (5.8107)  loss_scale: 131072.0000 (59741.2523)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0004  max mem: 8161
Epoch: [0]  [ 4120/60301]  eta: 7:47:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6974 (5.8104)  loss_scale: 131072.0000 (59914.3431)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0004  max mem: 8161
Epoch: [0]  [ 4130/60301]  eta: 7:47:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6552 (5.8101)  loss_scale: 131072.0000 (60086.5960)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4140/60301]  eta: 7:47:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6335 (5.8098)  loss_scale: 131072.0000 (60258.0169)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4150/60301]  eta: 7:47:15  lr: 0.000031  min_lr: 0.000001  loss: 5.6308 (5.8092)  loss_scale: 131072.0000 (60428.6119)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4160/60301]  eta: 7:46:38  lr: 0.000031  min_lr: 0.000001  loss: 5.6533 (5.8088)  loss_scale: 131072.0000 (60598.3869)  weight_decay: 0.0500 (0.0500)  time: 0.3753  data: 0.0025  max mem: 8161
Epoch: [0]  [ 4170/60301]  eta: 7:47:58  lr: 0.000031  min_lr: 0.000001  loss: 5.6190 (5.8083)  loss_scale: 131072.0000 (60767.3479)  weight_decay: 0.0500 (0.0500)  time: 0.6949  data: 0.5091  max mem: 8161
[2023-07-14 00:54:23,694] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:54:23,695] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 00:54:23,923] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2088
[2023-07-14 00:54:23,923] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:54:23,923] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 4180/60301]  eta: 7:48:02  lr: 0.000031  min_lr: 0.000001  loss: 5.7463 (5.8084)  loss_scale: 131072.0000 (60998.1995)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.7317  max mem: 8161
Epoch: [0]  [ 4190/60301]  eta: 7:48:02  lr: 0.000031  min_lr: 0.000001  loss: 5.7113 (5.8080)  loss_scale: 131072.0000 (61165.4001)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.2502  max mem: 8161
Epoch: [0]  [ 4200/60301]  eta: 7:48:06  lr: 0.000031  min_lr: 0.000001  loss: 5.6192 (5.8075)  loss_scale: 131072.0000 (61331.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0724  max mem: 8161
Epoch: [0]  [ 4210/60301]  eta: 7:47:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6474 (5.8073)  loss_scale: 131072.0000 (61497.4191)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0464  max mem: 8161
Epoch: [0]  [ 4220/60301]  eta: 7:47:51  lr: 0.000031  min_lr: 0.000001  loss: 5.6621 (5.8068)  loss_scale: 131072.0000 (61662.2488)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4230/60301]  eta: 7:47:44  lr: 0.000031  min_lr: 0.000001  loss: 5.6621 (5.8067)  loss_scale: 131072.0000 (61826.2992)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0022  max mem: 8161
Epoch: [0]  [ 4240/60301]  eta: 7:47:37  lr: 0.000031  min_lr: 0.000001  loss: 5.7073 (5.8065)  loss_scale: 131072.0000 (61989.5760)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0032  max mem: 8161
Epoch: [0]  [ 4250/60301]  eta: 7:47:33  lr: 0.000031  min_lr: 0.000001  loss: 5.6529 (5.8060)  loss_scale: 131072.0000 (62152.0847)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0140  max mem: 8161
Epoch: [0]  [ 4260/60301]  eta: 7:47:25  lr: 0.000031  min_lr: 0.000001  loss: 5.6629 (5.8058)  loss_scale: 131072.0000 (62313.8306)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0121  max mem: 8161
Epoch: [0]  [ 4270/60301]  eta: 7:47:17  lr: 0.000031  min_lr: 0.000001  loss: 5.6692 (5.8052)  loss_scale: 131072.0000 (62474.8190)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4280/60301]  eta: 7:47:10  lr: 0.000031  min_lr: 0.000001  loss: 5.4280 (5.8045)  loss_scale: 131072.0000 (62635.0554)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4290/60301]  eta: 7:47:02  lr: 0.000031  min_lr: 0.000001  loss: 5.4658 (5.8039)  loss_scale: 131072.0000 (62794.5449)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4300/60301]  eta: 7:46:55  lr: 0.000031  min_lr: 0.000001  loss: 5.5554 (5.8034)  loss_scale: 131072.0000 (62953.2927)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4310/60301]  eta: 7:46:56  lr: 0.000031  min_lr: 0.000001  loss: 5.5931 (5.8031)  loss_scale: 131072.0000 (63111.3041)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0339  max mem: 8161
Epoch: [0]  [ 4320/60301]  eta: 7:46:51  lr: 0.000031  min_lr: 0.000001  loss: 5.7087 (5.8031)  loss_scale: 131072.0000 (63268.5841)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0433  max mem: 8161
Epoch: [0]  [ 4330/60301]  eta: 7:46:43  lr: 0.000031  min_lr: 0.000001  loss: 5.7117 (5.8029)  loss_scale: 131072.0000 (63425.1378)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0099  max mem: 8161
Epoch: [0]  [ 4340/60301]  eta: 7:46:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6865 (5.8026)  loss_scale: 131072.0000 (63580.9703)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4350/60301]  eta: 7:46:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6304 (5.8022)  loss_scale: 131072.0000 (63736.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4360/60301]  eta: 7:46:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6301 (5.8019)  loss_scale: 131072.0000 (63890.4912)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4370/60301]  eta: 7:46:14  lr: 0.000031  min_lr: 0.000001  loss: 5.7265 (5.8018)  loss_scale: 131072.0000 (64044.1894)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0011  max mem: 8161
Epoch: [0]  [ 4380/60301]  eta: 7:46:06  lr: 0.000031  min_lr: 0.000001  loss: 5.6966 (5.8014)  loss_scale: 131072.0000 (64197.1860)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0011  max mem: 8161
Epoch: [0]  [ 4390/60301]  eta: 7:45:59  lr: 0.000031  min_lr: 0.000001  loss: 5.5741 (5.8010)  loss_scale: 131072.0000 (64349.4858)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4400/60301]  eta: 7:45:52  lr: 0.000031  min_lr: 0.000001  loss: 5.6061 (5.8007)  loss_scale: 131072.0000 (64501.0934)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0013  max mem: 8161
Epoch: [0]  [ 4410/60301]  eta: 7:45:45  lr: 0.000031  min_lr: 0.000001  loss: 5.6815 (5.8007)  loss_scale: 131072.0000 (64652.0136)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0022  max mem: 8161
Epoch: [0]  [ 4420/60301]  eta: 7:45:35  lr: 0.000031  min_lr: 0.000001  loss: 5.6329 (5.8000)  loss_scale: 131072.0000 (64802.2511)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.1623  max mem: 8161
Epoch: [0]  [ 4430/60301]  eta: 7:45:09  lr: 0.000031  min_lr: 0.000001  loss: 5.5063 (5.7995)  loss_scale: 131072.0000 (64951.8104)  weight_decay: 0.0500 (0.0500)  time: 0.3978  data: 0.2748  max mem: 8161
[2023-07-14 00:56:30,108] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:56:30,108] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 4440/60301]  eta: 7:45:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6993 (5.7995)  loss_scale: 131072.0000 (65277.7807)  weight_decay: 0.0500 (0.0500)  time: 0.4354  data: 0.3215  max mem: 8161
Epoch: [0]  [ 4450/60301]  eta: 7:45:10  lr: 0.000031  min_lr: 0.000001  loss: 5.6993 (5.7992)  loss_scale: 262144.0000 (65720.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.4237  max mem: 8161
Epoch: [0]  [ 4460/60301]  eta: 7:45:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5835 (5.7985)  loss_scale: 262144.0000 (66160.3909)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.2280  max mem: 8161
Epoch: [0]  [ 4470/60301]  eta: 7:44:58  lr: 0.000031  min_lr: 0.000001  loss: 5.5286 (5.7982)  loss_scale: 262144.0000 (66598.7350)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0124  max mem: 8161
Epoch: [0]  [ 4480/60301]  eta: 7:44:52  lr: 0.000031  min_lr: 0.000001  loss: 5.5696 (5.7975)  loss_scale: 262144.0000 (67035.1225)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0074  max mem: 8161
[2023-07-14 00:56:56,902] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2243
[2023-07-14 00:56:56,902] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:56:56,902] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 4490/60301]  eta: 7:44:45  lr: 0.000031  min_lr: 0.000001  loss: 5.5768 (5.7973)  loss_scale: 262144.0000 (67352.8248)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0127  max mem: 8161
Epoch: [0]  [ 4500/60301]  eta: 7:44:38  lr: 0.000031  min_lr: 0.000001  loss: 5.7156 (5.7968)  loss_scale: 131072.0000 (67494.3915)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0067  max mem: 8161
Epoch: [0]  [ 4510/60301]  eta: 7:44:31  lr: 0.000031  min_lr: 0.000001  loss: 5.6512 (5.7965)  loss_scale: 131072.0000 (67635.3305)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0013  max mem: 8161
Epoch: [0]  [ 4520/60301]  eta: 7:44:24  lr: 0.000031  min_lr: 0.000001  loss: 5.6233 (5.7961)  loss_scale: 131072.0000 (67775.6461)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4530/60301]  eta: 7:44:17  lr: 0.000031  min_lr: 0.000001  loss: 5.6177 (5.7958)  loss_scale: 131072.0000 (67915.3423)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4540/60301]  eta: 7:44:12  lr: 0.000031  min_lr: 0.000001  loss: 5.6050 (5.7953)  loss_scale: 131072.0000 (68054.4233)  weight_decay: 0.0500 (0.0500)  time: 0.4929  data: 0.0126  max mem: 8161
Epoch: [0]  [ 4550/60301]  eta: 7:43:51  lr: 0.000031  min_lr: 0.000001  loss: 5.5236 (5.7949)  loss_scale: 131072.0000 (68192.8930)  weight_decay: 0.0500 (0.0500)  time: 0.4361  data: 0.0202  max mem: 8161
Epoch: [0]  [ 4560/60301]  eta: 7:43:41  lr: 0.000031  min_lr: 0.000001  loss: 5.6419 (5.7946)  loss_scale: 131072.0000 (68330.7555)  weight_decay: 0.0500 (0.0500)  time: 0.4114  data: 0.1787  max mem: 8161
Epoch: [0]  [ 4570/60301]  eta: 7:44:01  lr: 0.000031  min_lr: 0.000001  loss: 5.6619 (5.7944)  loss_scale: 131072.0000 (68468.0149)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.4691  max mem: 8161
Epoch: [0]  [ 4580/60301]  eta: 7:43:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6619 (5.7941)  loss_scale: 131072.0000 (68604.6750)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.4988  max mem: 8161
Epoch: [0]  [ 4590/60301]  eta: 7:43:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6050 (5.7937)  loss_scale: 131072.0000 (68740.7397)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.2298  max mem: 8161
Epoch: [0]  [ 4600/60301]  eta: 7:44:13  lr: 0.000031  min_lr: 0.000001  loss: 5.6050 (5.7933)  loss_scale: 131072.0000 (68876.2130)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1191  max mem: 8161
Epoch: [0]  [ 4610/60301]  eta: 7:44:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6685 (5.7929)  loss_scale: 131072.0000 (69011.0987)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0964  max mem: 8161
Epoch: [0]  [ 4620/60301]  eta: 7:44:01  lr: 0.000031  min_lr: 0.000001  loss: 5.6560 (5.7926)  loss_scale: 131072.0000 (69145.4006)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0078  max mem: 8161
Epoch: [0]  [ 4630/60301]  eta: 7:43:54  lr: 0.000031  min_lr: 0.000001  loss: 5.5470 (5.7922)  loss_scale: 131072.0000 (69279.1224)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0025  max mem: 8161
Epoch: [0]  [ 4640/60301]  eta: 7:43:48  lr: 0.000031  min_lr: 0.000001  loss: 5.5470 (5.7918)  loss_scale: 131072.0000 (69412.2680)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4650/60301]  eta: 7:43:41  lr: 0.000031  min_lr: 0.000001  loss: 5.6607 (5.7914)  loss_scale: 131072.0000 (69544.8411)  weight_decay: 0.0500 (0.0500)  time: 0.4850  data: 0.0030  max mem: 8161
Epoch: [0]  [ 4660/60301]  eta: 7:43:34  lr: 0.000031  min_lr: 0.000001  loss: 5.6432 (5.7911)  loss_scale: 131072.0000 (69676.8453)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0030  max mem: 8161
Epoch: [0]  [ 4670/60301]  eta: 7:43:27  lr: 0.000031  min_lr: 0.000001  loss: 5.5961 (5.7907)  loss_scale: 131072.0000 (69808.2843)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0012  max mem: 8161
Epoch: [0]  [ 4680/60301]  eta: 7:43:20  lr: 0.000031  min_lr: 0.000001  loss: 5.5989 (5.7903)  loss_scale: 131072.0000 (69939.1617)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0023  max mem: 8161
Epoch: [0]  [ 4690/60301]  eta: 7:43:13  lr: 0.000031  min_lr: 0.000001  loss: 5.7066 (5.7902)  loss_scale: 131072.0000 (70069.4811)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4700/60301]  eta: 7:43:06  lr: 0.000031  min_lr: 0.000001  loss: 5.6649 (5.7900)  loss_scale: 131072.0000 (70199.2461)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4710/60301]  eta: 7:42:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6557 (5.7895)  loss_scale: 131072.0000 (70328.4602)  weight_decay: 0.0500 (0.0500)  time: 0.3544  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4720/60301]  eta: 7:43:04  lr: 0.000031  min_lr: 0.000001  loss: 5.6443 (5.7892)  loss_scale: 131072.0000 (70457.1269)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.3626  max mem: 8161
Epoch: [0]  [ 4730/60301]  eta: 7:43:07  lr: 0.000031  min_lr: 0.000001  loss: 5.6884 (5.7889)  loss_scale: 131072.0000 (70585.2496)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.5926  max mem: 8161
Epoch: [0]  [ 4740/60301]  eta: 7:43:20  lr: 0.000031  min_lr: 0.000001  loss: 5.7203 (5.7889)  loss_scale: 131072.0000 (70712.8319)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.3574  max mem: 8161
[2023-07-14 00:59:08,996] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 00:59:08,996] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 4750/60301]  eta: 7:43:12  lr: 0.000031  min_lr: 0.000001  loss: 5.7182 (5.7886)  loss_scale: 131072.0000 (71005.4069)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1275  max mem: 8161
Epoch: [0]  [ 4760/60301]  eta: 7:43:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5742 (5.7882)  loss_scale: 262144.0000 (71406.8742)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0045  max mem: 8161
[2023-07-14 00:59:16,809] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2380
[2023-07-14 00:59:16,810] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 00:59:16,810] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 4770/60301]  eta: 7:42:58  lr: 0.000031  min_lr: 0.000001  loss: 5.6028 (5.7878)  loss_scale: 131072.0000 (71531.9321)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0054  max mem: 8161
Epoch: [0]  [ 4780/60301]  eta: 7:42:51  lr: 0.000031  min_lr: 0.000001  loss: 5.6026 (5.7873)  loss_scale: 131072.0000 (71656.4668)  weight_decay: 0.0500 (0.0500)  time: 0.4782  data: 0.0030  max mem: 8161
Epoch: [0]  [ 4790/60301]  eta: 7:42:43  lr: 0.000031  min_lr: 0.000001  loss: 5.6072 (5.7871)  loss_scale: 131072.0000 (71780.4817)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0021  max mem: 8161
Epoch: [0]  [ 4800/60301]  eta: 7:42:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6517 (5.7867)  loss_scale: 131072.0000 (71903.9800)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4810/60301]  eta: 7:42:29  lr: 0.000031  min_lr: 0.000001  loss: 5.4848 (5.7861)  loss_scale: 131072.0000 (72026.9649)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4820/60301]  eta: 7:42:22  lr: 0.000031  min_lr: 0.000001  loss: 5.6224 (5.7861)  loss_scale: 131072.0000 (72149.4395)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0014  max mem: 8161
Epoch: [0]  [ 4830/60301]  eta: 7:42:43  lr: 0.000031  min_lr: 0.000001  loss: 5.7176 (5.7858)  loss_scale: 131072.0000 (72271.4072)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1265  max mem: 8161
Epoch: [0]  [ 4840/60301]  eta: 7:42:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6037 (5.7855)  loss_scale: 131072.0000 (72392.8709)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1256  max mem: 8161
Epoch: [0]  [ 4850/60301]  eta: 7:42:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5333 (5.7850)  loss_scale: 131072.0000 (72513.8338)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0015  max mem: 8161
Epoch: [0]  [ 4860/60301]  eta: 7:42:22  lr: 0.000031  min_lr: 0.000001  loss: 5.5333 (5.7844)  loss_scale: 131072.0000 (72634.2991)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0015  max mem: 8161
Epoch: [0]  [ 4870/60301]  eta: 7:42:18  lr: 0.000031  min_lr: 0.000001  loss: 5.5539 (5.7840)  loss_scale: 131072.0000 (72754.2698)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0128  max mem: 8161
Epoch: [0]  [ 4880/60301]  eta: 7:42:12  lr: 0.000031  min_lr: 0.000001  loss: 5.6161 (5.7838)  loss_scale: 131072.0000 (72873.7488)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0161  max mem: 8161
Epoch: [0]  [ 4890/60301]  eta: 7:42:37  lr: 0.000031  min_lr: 0.000001  loss: 5.6564 (5.7834)  loss_scale: 131072.0000 (72992.7393)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1499  max mem: 8161
Epoch: [0]  [ 4900/60301]  eta: 7:42:30  lr: 0.000031  min_lr: 0.000001  loss: 5.6557 (5.7831)  loss_scale: 131072.0000 (73111.2442)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1466  max mem: 8161
Epoch: [0]  [ 4910/60301]  eta: 7:42:23  lr: 0.000031  min_lr: 0.000001  loss: 5.5892 (5.7827)  loss_scale: 131072.0000 (73229.2665)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0004  max mem: 8161
Epoch: [0]  [ 4920/60301]  eta: 7:42:16  lr: 0.000031  min_lr: 0.000001  loss: 5.6241 (5.7824)  loss_scale: 131072.0000 (73346.8092)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0016  max mem: 8161
Epoch: [0]  [ 4930/60301]  eta: 7:42:09  lr: 0.000031  min_lr: 0.000001  loss: 5.6245 (5.7821)  loss_scale: 131072.0000 (73463.8751)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0016  max mem: 8161
Epoch: [0]  [ 4940/60301]  eta: 7:42:01  lr: 0.000031  min_lr: 0.000001  loss: 5.6301 (5.7819)  loss_scale: 131072.0000 (73580.4671)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0006  max mem: 8161
Epoch: [0]  [ 4950/60301]  eta: 7:41:54  lr: 0.000031  min_lr: 0.000001  loss: 5.6314 (5.7814)  loss_scale: 131072.0000 (73696.5882)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0009  max mem: 8161
Epoch: [0]  [ 4960/60301]  eta: 7:41:47  lr: 0.000031  min_lr: 0.000001  loss: 5.6340 (5.7811)  loss_scale: 131072.0000 (73812.2411)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0007  max mem: 8161
Epoch: [0]  [ 4970/60301]  eta: 7:41:40  lr: 0.000031  min_lr: 0.000001  loss: 5.6201 (5.7806)  loss_scale: 131072.0000 (73927.4287)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4980/60301]  eta: 7:41:33  lr: 0.000031  min_lr: 0.000001  loss: 5.5995 (5.7803)  loss_scale: 131072.0000 (74042.1538)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0005  max mem: 8161
Epoch: [0]  [ 4990/60301]  eta: 7:41:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5776 (5.7797)  loss_scale: 131072.0000 (74156.4192)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0171  max mem: 8161
[2023-07-14 01:01:17,584] [INFO] [timer.py:181:stop] 0/5000, SamplesPerSec=9.207545649510356
Epoch: [0]  [ 5000/60301]  eta: 7:41:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4898 (5.7794)  loss_scale: 131072.0000 (74270.2276)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0171  max mem: 8161
Epoch: [0]  [ 5010/60301]  eta: 7:41:15  lr: 0.000031  min_lr: 0.000001  loss: 5.5813 (5.7790)  loss_scale: 131072.0000 (74383.5817)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0005  max mem: 8161
[2023-07-14 01:01:27,121] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:01:27,122] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 5020/60301]  eta: 7:41:08  lr: 0.000031  min_lr: 0.000001  loss: 5.6315 (5.7787)  loss_scale: 131072.0000 (74548.6939)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
[2023-07-14 01:01:30,159] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2512
[2023-07-14 01:01:30,159] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:01:30,159] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 5030/60301]  eta: 7:41:02  lr: 0.000031  min_lr: 0.000001  loss: 5.5856 (5.7784)  loss_scale: 131072.0000 (74765.2554)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0086  max mem: 8161
Epoch: [0]  [ 5040/60301]  eta: 7:41:01  lr: 0.000031  min_lr: 0.000001  loss: 5.5839 (5.7781)  loss_scale: 131072.0000 (74876.9530)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0394  max mem: 8161
Epoch: [0]  [ 5050/60301]  eta: 7:40:54  lr: 0.000031  min_lr: 0.000001  loss: 5.6659 (5.7778)  loss_scale: 131072.0000 (74988.2083)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0321  max mem: 8161
Epoch: [0]  [ 5060/60301]  eta: 7:40:51  lr: 0.000031  min_lr: 0.000001  loss: 5.6280 (5.7774)  loss_scale: 131072.0000 (75099.0239)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0210  max mem: 8161
Epoch: [0]  [ 5070/60301]  eta: 7:40:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4380 (5.7767)  loss_scale: 131072.0000 (75209.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0333  max mem: 8161
Epoch: [0]  [ 5080/60301]  eta: 7:41:19  lr: 0.000031  min_lr: 0.000001  loss: 5.5811 (5.7764)  loss_scale: 131072.0000 (75319.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.1949  max mem: 8161
Epoch: [0]  [ 5090/60301]  eta: 7:41:08  lr: 0.000031  min_lr: 0.000001  loss: 5.5819 (5.7759)  loss_scale: 131072.0000 (75428.8588)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.2377  max mem: 8161
Epoch: [0]  [ 5100/60301]  eta: 7:40:47  lr: 0.000031  min_lr: 0.000001  loss: 5.5819 (5.7757)  loss_scale: 131072.0000 (75537.9416)  weight_decay: 0.0500 (0.0500)  time: 0.3956  data: 0.1727  max mem: 8161
Epoch: [0]  [ 5110/60301]  eta: 7:40:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6838 (5.7755)  loss_scale: 131072.0000 (75646.5975)  weight_decay: 0.0500 (0.0500)  time: 0.4570  data: 0.3423  max mem: 8161
Epoch: [0]  [ 5120/60301]  eta: 7:40:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6397 (5.7752)  loss_scale: 131072.0000 (75754.8291)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.4463  max mem: 8161
Epoch: [0]  [ 5130/60301]  eta: 7:40:51  lr: 0.000031  min_lr: 0.000001  loss: 5.6268 (5.7749)  loss_scale: 131072.0000 (75862.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.3650  max mem: 8161
Epoch: [0]  [ 5140/60301]  eta: 7:40:49  lr: 0.000031  min_lr: 0.000001  loss: 5.6213 (5.7746)  loss_scale: 131072.0000 (75970.0292)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1673  max mem: 8161
Epoch: [0]  [ 5150/60301]  eta: 7:40:45  lr: 0.000031  min_lr: 0.000001  loss: 5.5692 (5.7741)  loss_scale: 131072.0000 (76077.0025)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0374  max mem: 8161
Epoch: [0]  [ 5160/60301]  eta: 7:40:46  lr: 0.000031  min_lr: 0.000001  loss: 5.6034 (5.7738)  loss_scale: 131072.0000 (76183.5613)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0522  max mem: 8161
Epoch: [0]  [ 5170/60301]  eta: 7:40:38  lr: 0.000031  min_lr: 0.000001  loss: 5.6222 (5.7735)  loss_scale: 131072.0000 (76289.7080)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0380  max mem: 8161
Epoch: [0]  [ 5180/60301]  eta: 7:40:32  lr: 0.000031  min_lr: 0.000001  loss: 5.6036 (5.7732)  loss_scale: 131072.0000 (76395.4449)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5190/60301]  eta: 7:40:25  lr: 0.000031  min_lr: 0.000001  loss: 5.6047 (5.7727)  loss_scale: 131072.0000 (76500.7744)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5200/60301]  eta: 7:40:17  lr: 0.000031  min_lr: 0.000001  loss: 5.6607 (5.7725)  loss_scale: 131072.0000 (76605.6989)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5210/60301]  eta: 7:40:10  lr: 0.000031  min_lr: 0.000001  loss: 5.6388 (5.7722)  loss_scale: 131072.0000 (76710.2207)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5220/60301]  eta: 7:40:03  lr: 0.000031  min_lr: 0.000001  loss: 5.5512 (5.7716)  loss_scale: 131072.0000 (76814.3421)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5230/60301]  eta: 7:39:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4954 (5.7711)  loss_scale: 131072.0000 (76918.0654)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0021  max mem: 8161
Epoch: [0]  [ 5240/60301]  eta: 7:39:49  lr: 0.000031  min_lr: 0.000001  loss: 5.5033 (5.7707)  loss_scale: 131072.0000 (77021.3929)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0020  max mem: 8161
Epoch: [0]  [ 5250/60301]  eta: 7:39:42  lr: 0.000031  min_lr: 0.000001  loss: 5.6721 (5.7704)  loss_scale: 131072.0000 (77124.3268)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0012  max mem: 8161
Epoch: [0]  [ 5260/60301]  eta: 7:39:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6853 (5.7701)  loss_scale: 131072.0000 (77226.8694)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0012  max mem: 8161
Epoch: [0]  [ 5270/60301]  eta: 7:39:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5774 (5.7697)  loss_scale: 131072.0000 (77329.0230)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0010  max mem: 8161
Epoch: [0]  [ 5280/60301]  eta: 7:39:35  lr: 0.000031  min_lr: 0.000001  loss: 5.5596 (5.7694)  loss_scale: 131072.0000 (77430.7896)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0644  max mem: 8161
[2023-07-14 01:03:42,865] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:03:42,866] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 5290/60301]  eta: 7:39:37  lr: 0.000031  min_lr: 0.000001  loss: 5.5265 (5.7689)  loss_scale: 131072.0000 (77730.3527)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1053  max mem: 8161
[2023-07-14 01:03:47,546] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2645
[2023-07-14 01:03:47,546] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:03:47,546] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 5300/60301]  eta: 7:39:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5324 (5.7686)  loss_scale: 131072.0000 (77830.9783)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0418  max mem: 8161
Epoch: [0]  [ 5310/60301]  eta: 7:39:21  lr: 0.000031  min_lr: 0.000001  loss: 5.5400 (5.7683)  loss_scale: 131072.0000 (77931.2250)  weight_decay: 0.0500 (0.0500)  time: 0.4765  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5320/60301]  eta: 7:39:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5400 (5.7679)  loss_scale: 131072.0000 (78031.0949)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5330/60301]  eta: 7:39:05  lr: 0.000031  min_lr: 0.000001  loss: 5.5688 (5.7677)  loss_scale: 131072.0000 (78130.5901)  weight_decay: 0.0500 (0.0500)  time: 0.4693  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5340/60301]  eta: 7:39:19  lr: 0.000031  min_lr: 0.000001  loss: 5.6060 (5.7673)  loss_scale: 131072.0000 (78229.7128)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.2841  max mem: 8161
Epoch: [0]  [ 5350/60301]  eta: 7:39:26  lr: 0.000031  min_lr: 0.000001  loss: 5.6258 (5.7671)  loss_scale: 131072.0000 (78328.4650)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.5346  max mem: 8161
Epoch: [0]  [ 5360/60301]  eta: 7:39:04  lr: 0.000031  min_lr: 0.000001  loss: 5.5675 (5.7667)  loss_scale: 131072.0000 (78426.8487)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.3642  max mem: 8161
Epoch: [0]  [ 5370/60301]  eta: 7:39:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5607 (5.7664)  loss_scale: 131072.0000 (78524.8661)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.2222  max mem: 8161
Epoch: [0]  [ 5380/60301]  eta: 7:39:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5917 (5.7662)  loss_scale: 131072.0000 (78622.5192)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1427  max mem: 8161
Epoch: [0]  [ 5390/60301]  eta: 7:39:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5481 (5.7657)  loss_scale: 131072.0000 (78719.8101)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0342  max mem: 8161
Epoch: [0]  [ 5400/60301]  eta: 7:38:59  lr: 0.000031  min_lr: 0.000001  loss: 5.5519 (5.7656)  loss_scale: 131072.0000 (78816.7406)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5410/60301]  eta: 7:38:52  lr: 0.000031  min_lr: 0.000001  loss: 5.6050 (5.7652)  loss_scale: 131072.0000 (78913.3129)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5420/60301]  eta: 7:39:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5905 (5.7649)  loss_scale: 131072.0000 (79009.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1431  max mem: 8161
Epoch: [0]  [ 5430/60301]  eta: 7:39:06  lr: 0.000031  min_lr: 0.000001  loss: 5.6486 (5.7647)  loss_scale: 131072.0000 (79105.3905)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1422  max mem: 8161
Epoch: [0]  [ 5440/60301]  eta: 7:38:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6865 (5.7646)  loss_scale: 131072.0000 (79200.8998)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0013  max mem: 8161
Epoch: [0]  [ 5450/60301]  eta: 7:38:50  lr: 0.000031  min_lr: 0.000001  loss: 5.6214 (5.7641)  loss_scale: 131072.0000 (79296.0587)  weight_decay: 0.0500 (0.0500)  time: 0.4701  data: 0.0013  max mem: 8161
Epoch: [0]  [ 5460/60301]  eta: 7:38:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4625 (5.7636)  loss_scale: 131072.0000 (79390.8691)  weight_decay: 0.0500 (0.0500)  time: 0.3693  data: 0.0820  max mem: 8161
Epoch: [0]  [ 5470/60301]  eta: 7:38:21  lr: 0.000031  min_lr: 0.000001  loss: 5.5538 (5.7633)  loss_scale: 131072.0000 (79485.3328)  weight_decay: 0.0500 (0.0500)  time: 0.4102  data: 0.2975  max mem: 8161
Epoch: [0]  [ 5480/60301]  eta: 7:38:04  lr: 0.000031  min_lr: 0.000001  loss: 5.5606 (5.7628)  loss_scale: 131072.0000 (79579.4519)  weight_decay: 0.0500 (0.0500)  time: 0.4618  data: 0.3491  max mem: 8161
Epoch: [0]  [ 5490/60301]  eta: 7:38:09  lr: 0.000031  min_lr: 0.000001  loss: 5.5606 (5.7625)  loss_scale: 131072.0000 (79673.2282)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.3391  max mem: 8161
Epoch: [0]  [ 5500/60301]  eta: 7:38:02  lr: 0.000031  min_lr: 0.000001  loss: 5.6081 (5.7623)  loss_scale: 131072.0000 (79766.6635)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.2060  max mem: 8161
Epoch: [0]  [ 5510/60301]  eta: 7:38:04  lr: 0.000031  min_lr: 0.000001  loss: 5.6224 (5.7619)  loss_scale: 131072.0000 (79859.7598)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0436  max mem: 8161
Epoch: [0]  [ 5520/60301]  eta: 7:37:57  lr: 0.000031  min_lr: 0.000001  loss: 5.5219 (5.7613)  loss_scale: 131072.0000 (79952.5187)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0436  max mem: 8161
Epoch: [0]  [ 5530/60301]  eta: 7:37:50  lr: 0.000031  min_lr: 0.000001  loss: 5.5219 (5.7611)  loss_scale: 131072.0000 (80044.9423)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5540/60301]  eta: 7:37:43  lr: 0.000031  min_lr: 0.000001  loss: 5.5411 (5.7606)  loss_scale: 131072.0000 (80137.0323)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0023  max mem: 8161
[2023-07-14 01:05:58,457] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:05:58,457] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 5550/60301]  eta: 7:37:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5229 (5.7603)  loss_scale: 131072.0000 (80276.0151)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0181  max mem: 8161
Epoch: [0]  [ 5560/60301]  eta: 7:37:33  lr: 0.000031  min_lr: 0.000001  loss: 5.5812 (5.7598)  loss_scale: 262144.0000 (80603.0570)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0197  max mem: 8161
[2023-07-14 01:06:06,238] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2782
[2023-07-14 01:06:06,239] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:06:06,239] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 5570/60301]  eta: 7:37:25  lr: 0.000031  min_lr: 0.000001  loss: 5.6156 (5.7597)  loss_scale: 262144.0000 (80787.7595)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0039  max mem: 8161
Epoch: [0]  [ 5580/60301]  eta: 7:37:44  lr: 0.000031  min_lr: 0.000001  loss: 5.5681 (5.7591)  loss_scale: 131072.0000 (80877.8584)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1326  max mem: 8161
Epoch: [0]  [ 5590/60301]  eta: 7:37:37  lr: 0.000031  min_lr: 0.000001  loss: 5.5609 (5.7589)  loss_scale: 131072.0000 (80967.6351)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1335  max mem: 8161
Epoch: [0]  [ 5600/60301]  eta: 7:37:30  lr: 0.000031  min_lr: 0.000001  loss: 5.5683 (5.7586)  loss_scale: 131072.0000 (81057.0912)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0022  max mem: 8161
Epoch: [0]  [ 5610/60301]  eta: 7:37:23  lr: 0.000031  min_lr: 0.000001  loss: 5.5124 (5.7582)  loss_scale: 131072.0000 (81146.2285)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5620/60301]  eta: 7:37:16  lr: 0.000031  min_lr: 0.000001  loss: 5.5868 (5.7579)  loss_scale: 131072.0000 (81235.0486)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5630/60301]  eta: 7:37:39  lr: 0.000031  min_lr: 0.000001  loss: 5.5890 (5.7577)  loss_scale: 131072.0000 (81323.5532)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1527  max mem: 8161
Epoch: [0]  [ 5640/60301]  eta: 7:37:32  lr: 0.000031  min_lr: 0.000001  loss: 5.5890 (5.7575)  loss_scale: 131072.0000 (81411.7440)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.1538  max mem: 8161
Epoch: [0]  [ 5650/60301]  eta: 7:37:25  lr: 0.000031  min_lr: 0.000001  loss: 5.6085 (5.7570)  loss_scale: 131072.0000 (81499.6227)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0025  max mem: 8161
Epoch: [0]  [ 5660/60301]  eta: 7:37:18  lr: 0.000031  min_lr: 0.000001  loss: 5.6169 (5.7569)  loss_scale: 131072.0000 (81587.1910)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5670/60301]  eta: 7:37:11  lr: 0.000031  min_lr: 0.000001  loss: 5.6090 (5.7564)  loss_scale: 131072.0000 (81674.4504)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0013  max mem: 8161
Epoch: [0]  [ 5680/60301]  eta: 7:37:23  lr: 0.000031  min_lr: 0.000001  loss: 5.5240 (5.7561)  loss_scale: 131072.0000 (81761.4026)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1023  max mem: 8161
Epoch: [0]  [ 5690/60301]  eta: 7:37:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5160 (5.7558)  loss_scale: 131072.0000 (81848.0492)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1033  max mem: 8161
Epoch: [0]  [ 5700/60301]  eta: 7:37:16  lr: 0.000031  min_lr: 0.000001  loss: 5.6689 (5.7556)  loss_scale: 131072.0000 (81934.3919)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0773  max mem: 8161
Epoch: [0]  [ 5710/60301]  eta: 7:36:53  lr: 0.000031  min_lr: 0.000001  loss: 5.6316 (5.7552)  loss_scale: 131072.0000 (82020.4321)  weight_decay: 0.0500 (0.0500)  time: 0.4340  data: 0.1772  max mem: 8161
Epoch: [0]  [ 5720/60301]  eta: 7:36:56  lr: 0.000031  min_lr: 0.000001  loss: 5.6244 (5.7551)  loss_scale: 131072.0000 (82106.1716)  weight_decay: 0.0500 (0.0500)  time: 0.4472  data: 0.3346  max mem: 8161
Epoch: [0]  [ 5730/60301]  eta: 7:37:00  lr: 0.000031  min_lr: 0.000001  loss: 5.6257 (5.7548)  loss_scale: 131072.0000 (82191.6119)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.4756  max mem: 8161
Epoch: [0]  [ 5740/60301]  eta: 7:36:59  lr: 0.000031  min_lr: 0.000001  loss: 5.6270 (5.7547)  loss_scale: 131072.0000 (82276.7546)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.3792  max mem: 8161
Epoch: [0]  [ 5750/60301]  eta: 7:36:37  lr: 0.000031  min_lr: 0.000001  loss: 5.6744 (5.7545)  loss_scale: 131072.0000 (82361.6011)  weight_decay: 0.0500 (0.0500)  time: 0.4376  data: 0.2439  max mem: 8161
Epoch: [0]  [ 5760/60301]  eta: 7:36:33  lr: 0.000031  min_lr: 0.000001  loss: 5.6744 (5.7544)  loss_scale: 131072.0000 (82446.1531)  weight_decay: 0.0500 (0.0500)  time: 0.4191  data: 0.3065  max mem: 8161
Epoch: [0]  [ 5770/60301]  eta: 7:36:13  lr: 0.000031  min_lr: 0.000001  loss: 5.6015 (5.7538)  loss_scale: 131072.0000 (82530.4121)  weight_decay: 0.0500 (0.0500)  time: 0.4243  data: 0.3124  max mem: 8161
Epoch: [0]  [ 5780/60301]  eta: 7:36:26  lr: 0.000031  min_lr: 0.000001  loss: 5.4360 (5.7533)  loss_scale: 131072.0000 (82614.3795)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.3624  max mem: 8161
Epoch: [0]  [ 5790/60301]  eta: 7:36:21  lr: 0.000031  min_lr: 0.000001  loss: 5.4689 (5.7530)  loss_scale: 131072.0000 (82698.0570)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.2560  max mem: 8161
Epoch: [0]  [ 5800/60301]  eta: 7:36:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4908 (5.7525)  loss_scale: 131072.0000 (82781.4460)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0951  max mem: 8161
Epoch: [0]  [ 5810/60301]  eta: 7:36:29  lr: 0.000031  min_lr: 0.000001  loss: 5.5772 (5.7522)  loss_scale: 131072.0000 (82864.5479)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1198  max mem: 8161
Epoch: [0]  [ 5820/60301]  eta: 7:36:22  lr: 0.000031  min_lr: 0.000001  loss: 5.5772 (5.7517)  loss_scale: 131072.0000 (82947.3644)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0344  max mem: 8161
[2023-07-14 01:08:22,108] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:08:22,108] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 5830/60301]  eta: 7:36:15  lr: 0.000031  min_lr: 0.000001  loss: 5.5126 (5.7514)  loss_scale: 131072.0000 (83209.7246)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0030  max mem: 8161
Epoch: [0]  [ 5840/60301]  eta: 7:36:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4887 (5.7509)  loss_scale: 262144.0000 (83516.0664)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5850/60301]  eta: 7:36:01  lr: 0.000031  min_lr: 0.000001  loss: 5.4868 (5.7504)  loss_scale: 262144.0000 (83821.3611)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5860/60301]  eta: 7:35:55  lr: 0.000031  min_lr: 0.000001  loss: 5.6114 (5.7503)  loss_scale: 262144.0000 (84125.6141)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0015  max mem: 8161
Epoch: [0]  [ 5870/60301]  eta: 7:35:48  lr: 0.000031  min_lr: 0.000001  loss: 5.6951 (5.7502)  loss_scale: 262144.0000 (84428.8305)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5880/60301]  eta: 7:35:41  lr: 0.000031  min_lr: 0.000001  loss: 5.6928 (5.7499)  loss_scale: 262144.0000 (84731.0158)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0004  max mem: 8161
Epoch: [0]  [ 5890/60301]  eta: 7:35:35  lr: 0.000031  min_lr: 0.000001  loss: 5.5780 (5.7494)  loss_scale: 262144.0000 (85032.1752)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0005  max mem: 8161
[2023-07-14 01:08:58,857] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2949
[2023-07-14 01:08:58,857] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:08:58,857] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 5900/60301]  eta: 7:35:27  lr: 0.000031  min_lr: 0.000001  loss: 5.3964 (5.7486)  loss_scale: 262144.0000 (85287.8902)  weight_decay: 0.0500 (0.0500)  time: 0.4781  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5910/60301]  eta: 7:35:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4654 (5.7484)  loss_scale: 131072.0000 (85365.3460)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5920/60301]  eta: 7:35:14  lr: 0.000031  min_lr: 0.000001  loss: 5.7017 (5.7484)  loss_scale: 131072.0000 (85442.5401)  weight_decay: 0.0500 (0.0500)  time: 0.4850  data: 0.0023  max mem: 8161
Epoch: [0]  [ 5930/60301]  eta: 7:35:07  lr: 0.000031  min_lr: 0.000001  loss: 5.6730 (5.7482)  loss_scale: 131072.0000 (85519.4740)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0023  max mem: 8161
Epoch: [0]  [ 5940/60301]  eta: 7:35:00  lr: 0.000031  min_lr: 0.000001  loss: 5.6680 (5.7481)  loss_scale: 131072.0000 (85596.1488)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5950/60301]  eta: 7:34:53  lr: 0.000031  min_lr: 0.000001  loss: 5.6083 (5.7478)  loss_scale: 131072.0000 (85672.5660)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0005  max mem: 8161
Epoch: [0]  [ 5960/60301]  eta: 7:34:46  lr: 0.000031  min_lr: 0.000001  loss: 5.5875 (5.7474)  loss_scale: 131072.0000 (85748.7267)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0014  max mem: 8161
Epoch: [0]  [ 5970/60301]  eta: 7:34:40  lr: 0.000031  min_lr: 0.000001  loss: 5.6217 (5.7473)  loss_scale: 131072.0000 (85824.6324)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0023  max mem: 8161
Epoch: [0]  [ 5980/60301]  eta: 7:34:33  lr: 0.000031  min_lr: 0.000001  loss: 5.6454 (5.7472)  loss_scale: 131072.0000 (85900.2842)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0013  max mem: 8161
Epoch: [0]  [ 5990/60301]  eta: 7:34:42  lr: 0.000031  min_lr: 0.000001  loss: 5.4829 (5.7466)  loss_scale: 131072.0000 (85975.6835)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0881  max mem: 8161
[2023-07-14 01:09:48,968] [INFO] [logging.py:69:log_dist] [Rank 0] step=3000, skipped=12, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 01:09:48,972] [INFO] [timer.py:181:stop] 0/6000, SamplesPerSec=9.31835794846659
Epoch: [0]  [ 6000/60301]  eta: 7:34:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4565 (5.7464)  loss_scale: 131072.0000 (86050.8315)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0889  max mem: 8161
Epoch: [0]  [ 6010/60301]  eta: 7:34:28  lr: 0.000031  min_lr: 0.000001  loss: 5.7424 (5.7462)  loss_scale: 131072.0000 (86125.7295)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0013  max mem: 8161
Epoch: [0]  [ 6020/60301]  eta: 7:34:21  lr: 0.000031  min_lr: 0.000001  loss: 5.7424 (5.7462)  loss_scale: 131072.0000 (86200.3787)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6030/60301]  eta: 7:34:15  lr: 0.000031  min_lr: 0.000001  loss: 5.5987 (5.7458)  loss_scale: 131072.0000 (86274.7803)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6040/60301]  eta: 7:34:08  lr: 0.000031  min_lr: 0.000001  loss: 5.5585 (5.7454)  loss_scale: 131072.0000 (86348.9356)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0005  max mem: 8161
Epoch: [0]  [ 6050/60301]  eta: 7:34:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6116 (5.7452)  loss_scale: 131072.0000 (86422.8458)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1607  max mem: 8161
Epoch: [0]  [ 6060/60301]  eta: 7:34:04  lr: 0.000031  min_lr: 0.000001  loss: 5.6116 (5.7449)  loss_scale: 131072.0000 (86496.5121)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.2178  max mem: 8161
Epoch: [0]  [ 6070/60301]  eta: 7:34:02  lr: 0.000031  min_lr: 0.000001  loss: 5.6440 (5.7448)  loss_scale: 131072.0000 (86569.9358)  weight_decay: 0.0500 (0.0500)  time: 0.4062  data: 0.2659  max mem: 8161
Epoch: [0]  [ 6080/60301]  eta: 7:33:43  lr: 0.000031  min_lr: 0.000001  loss: 5.6155 (5.7443)  loss_scale: 131072.0000 (86643.1179)  weight_decay: 0.0500 (0.0500)  time: 0.4414  data: 0.3283  max mem: 8161
Epoch: [0]  [ 6090/60301]  eta: 7:33:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4919 (5.7440)  loss_scale: 131072.0000 (86716.0598)  weight_decay: 0.0500 (0.0500)  time: 0.4721  data: 0.3594  max mem: 8161
Epoch: [0]  [ 6100/60301]  eta: 7:33:58  lr: 0.000031  min_lr: 0.000001  loss: 5.5577 (5.7438)  loss_scale: 131072.0000 (86788.7625)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.3790  max mem: 8161
Epoch: [0]  [ 6110/60301]  eta: 7:33:51  lr: 0.000031  min_lr: 0.000001  loss: 5.5307 (5.7432)  loss_scale: 131072.0000 (86861.2273)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1405  max mem: 8161
Epoch: [0]  [ 6120/60301]  eta: 7:33:48  lr: 0.000031  min_lr: 0.000001  loss: 5.3326 (5.7428)  loss_scale: 131072.0000 (86933.4553)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0213  max mem: 8161
Epoch: [0]  [ 6130/60301]  eta: 7:33:41  lr: 0.000031  min_lr: 0.000001  loss: 5.5788 (5.7428)  loss_scale: 131072.0000 (87005.4477)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0202  max mem: 8161
Epoch: [0]  [ 6140/60301]  eta: 7:33:34  lr: 0.000031  min_lr: 0.000001  loss: 5.5015 (5.7423)  loss_scale: 131072.0000 (87077.2057)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0005  max mem: 8161
Epoch: [0]  [ 6150/60301]  eta: 7:33:27  lr: 0.000031  min_lr: 0.000001  loss: 5.5015 (5.7421)  loss_scale: 131072.0000 (87148.7303)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0005  max mem: 8161
[2023-07-14 01:11:09,071] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:11:09,071] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 6160/60301]  eta: 7:33:21  lr: 0.000031  min_lr: 0.000001  loss: 5.5936 (5.7418)  loss_scale: 131072.0000 (87305.1206)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6170/60301]  eta: 7:33:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5007 (5.7412)  loss_scale: 262144.0000 (87588.4440)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6180/60301]  eta: 7:32:49  lr: 0.000031  min_lr: 0.000001  loss: 5.5007 (5.7409)  loss_scale: 262144.0000 (87870.8507)  weight_decay: 0.0500 (0.0500)  time: 0.3789  data: 0.0012  max mem: 8161
Epoch: [0]  [ 6190/60301]  eta: 7:32:41  lr: 0.000031  min_lr: 0.000001  loss: 5.5022 (5.7405)  loss_scale: 262144.0000 (88152.3450)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.1792  max mem: 8161
[2023-07-14 01:11:27,549] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3099
[2023-07-14 01:11:27,549] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:11:27,550] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 6200/60301]  eta: 7:32:36  lr: 0.000031  min_lr: 0.000001  loss: 5.5126 (5.7401)  loss_scale: 262144.0000 (88390.6570)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.3733  max mem: 8161
Epoch: [0]  [ 6210/60301]  eta: 7:32:25  lr: 0.000031  min_lr: 0.000001  loss: 5.5126 (5.7397)  loss_scale: 131072.0000 (88459.3759)  weight_decay: 0.0500 (0.0500)  time: 0.4653  data: 0.3516  max mem: 8161
Epoch: [0]  [ 6220/60301]  eta: 7:32:22  lr: 0.000031  min_lr: 0.000001  loss: 5.3983 (5.7391)  loss_scale: 131072.0000 (88527.8740)  weight_decay: 0.0500 (0.0500)  time: 0.4772  data: 0.2505  max mem: 8161
video cannot be loaded by decord:  /data/i5O/kinetics400/train/OErKBwdGJIk_000057_000067.mp4
/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/OErKBwdGJIk_000057_000067.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [0]  [ 6230/60301]  eta: 7:32:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5348 (5.7390)  loss_scale: 131072.0000 (88596.1521)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.1042  max mem: 8161
Epoch: [0]  [ 6240/60301]  eta: 7:32:12  lr: 0.000031  min_lr: 0.000001  loss: 5.7866 (5.7388)  loss_scale: 131072.0000 (88664.2115)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0140  max mem: 8161
Epoch: [0]  [ 6250/60301]  eta: 7:32:16  lr: 0.000031  min_lr: 0.000001  loss: 5.6027 (5.7387)  loss_scale: 131072.0000 (88732.0531)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0712  max mem: 8161
Epoch: [0]  [ 6260/60301]  eta: 7:32:10  lr: 0.000031  min_lr: 0.000001  loss: 5.5656 (5.7384)  loss_scale: 131072.0000 (88799.6780)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0700  max mem: 8161
Epoch: [0]  [ 6270/60301]  eta: 7:32:03  lr: 0.000031  min_lr: 0.000001  loss: 5.5653 (5.7381)  loss_scale: 131072.0000 (88867.0872)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0026  max mem: 8161
Epoch: [0]  [ 6280/60301]  eta: 7:31:57  lr: 0.000031  min_lr: 0.000001  loss: 5.5653 (5.7378)  loss_scale: 131072.0000 (88934.2818)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0015  max mem: 8161
Epoch: [0]  [ 6290/60301]  eta: 7:31:50  lr: 0.000031  min_lr: 0.000001  loss: 5.5314 (5.7374)  loss_scale: 131072.0000 (89001.2628)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0025  max mem: 8161
Epoch: [0]  [ 6300/60301]  eta: 7:31:44  lr: 0.000031  min_lr: 0.000001  loss: 5.5084 (5.7370)  loss_scale: 131072.0000 (89068.0311)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0025  max mem: 8161
Epoch: [0]  [ 6310/60301]  eta: 7:31:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5178 (5.7367)  loss_scale: 131072.0000 (89134.5879)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0172  max mem: 8161
Epoch: [0]  [ 6320/60301]  eta: 7:31:34  lr: 0.000031  min_lr: 0.000001  loss: 5.4976 (5.7362)  loss_scale: 131072.0000 (89200.9340)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0183  max mem: 8161
Epoch: [0]  [ 6330/60301]  eta: 7:31:27  lr: 0.000031  min_lr: 0.000001  loss: 5.3711 (5.7359)  loss_scale: 131072.0000 (89267.0706)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0034  max mem: 8161
Epoch: [0]  [ 6340/60301]  eta: 7:31:23  lr: 0.000031  min_lr: 0.000001  loss: 5.4378 (5.7355)  loss_scale: 131072.0000 (89332.9986)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0177  max mem: 8161
Epoch: [0]  [ 6350/60301]  eta: 7:31:17  lr: 0.000031  min_lr: 0.000001  loss: 5.4443 (5.7350)  loss_scale: 131072.0000 (89398.7189)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0177  max mem: 8161
Epoch: [0]  [ 6360/60301]  eta: 7:31:10  lr: 0.000031  min_lr: 0.000001  loss: 5.5135 (5.7349)  loss_scale: 131072.0000 (89464.2327)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0021  max mem: 8161
Epoch: [0]  [ 6370/60301]  eta: 7:31:04  lr: 0.000031  min_lr: 0.000001  loss: 5.5100 (5.7343)  loss_scale: 131072.0000 (89529.5407)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0013  max mem: 8161
Epoch: [0]  [ 6380/60301]  eta: 7:30:57  lr: 0.000031  min_lr: 0.000001  loss: 5.5200 (5.7341)  loss_scale: 131072.0000 (89594.6441)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6390/60301]  eta: 7:30:51  lr: 0.000031  min_lr: 0.000001  loss: 5.5200 (5.7338)  loss_scale: 131072.0000 (89659.5437)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0015  max mem: 8161
Epoch: [0]  [ 6400/60301]  eta: 7:31:27  lr: 0.000031  min_lr: 0.000001  loss: 5.3752 (5.7332)  loss_scale: 131072.0000 (89724.2406)  weight_decay: 0.0500 (0.0500)  time: 0.7370  data: 0.2550  max mem: 8161
Epoch: [0]  [ 6410/60301]  eta: 7:31:20  lr: 0.000031  min_lr: 0.000001  loss: 5.3705 (5.7328)  loss_scale: 131072.0000 (89788.7356)  weight_decay: 0.0500 (0.0500)  time: 0.7361  data: 0.2561  max mem: 8161
Epoch: [0]  [ 6420/60301]  eta: 7:31:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5242 (5.7326)  loss_scale: 131072.0000 (89853.0297)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0024  max mem: 8161
Epoch: [0]  [ 6430/60301]  eta: 7:31:16  lr: 0.000031  min_lr: 0.000001  loss: 5.6515 (5.7325)  loss_scale: 131072.0000 (89917.1239)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0552  max mem: 8161
Epoch: [0]  [ 6440/60301]  eta: 7:31:35  lr: 0.000031  min_lr: 0.000001  loss: 5.6538 (5.7322)  loss_scale: 131072.0000 (89981.0191)  weight_decay: 0.0500 (0.0500)  time: 0.6898  data: 0.2095  max mem: 8161
Epoch: [0]  [ 6450/60301]  eta: 7:31:28  lr: 0.000031  min_lr: 0.000001  loss: 5.5415 (5.7318)  loss_scale: 131072.0000 (90044.7162)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1563  max mem: 8161
[2023-07-14 01:13:43,716] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:13:43,716] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 01:13:44,692] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3229
[2023-07-14 01:13:44,692] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:13:44,692] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 6460/60301]  eta: 7:31:21  lr: 0.000031  min_lr: 0.000001  loss: 5.5415 (5.7315)  loss_scale: 131072.0000 (90148.7894)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0012  max mem: 8161
Epoch: [0]  [ 6470/60301]  eta: 7:31:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5160 (5.7311)  loss_scale: 131072.0000 (90212.0303)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6480/60301]  eta: 7:31:07  lr: 0.000031  min_lr: 0.000001  loss: 5.5160 (5.7309)  loss_scale: 131072.0000 (90275.0761)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0005  max mem: 8161
Epoch: [0]  [ 6490/60301]  eta: 7:31:06  lr: 0.000031  min_lr: 0.000001  loss: 5.6156 (5.7307)  loss_scale: 131072.0000 (90337.9276)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0354  max mem: 8161
Epoch: [0]  [ 6500/60301]  eta: 7:31:00  lr: 0.000031  min_lr: 0.000001  loss: 5.4903 (5.7302)  loss_scale: 131072.0000 (90400.5858)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0354  max mem: 8161
Epoch: [0]  [ 6510/60301]  eta: 7:30:53  lr: 0.000031  min_lr: 0.000001  loss: 5.3740 (5.7298)  loss_scale: 131072.0000 (90463.0515)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6520/60301]  eta: 7:30:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4226 (5.7295)  loss_scale: 131072.0000 (90525.3256)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0005  max mem: 8161
Epoch: [0]  [ 6530/60301]  eta: 7:30:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5766 (5.7292)  loss_scale: 131072.0000 (90587.4090)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6540/60301]  eta: 7:30:27  lr: 0.000031  min_lr: 0.000001  loss: 5.5766 (5.7290)  loss_scale: 131072.0000 (90649.3026)  weight_decay: 0.0500 (0.0500)  time: 0.4469  data: 0.0013  max mem: 8161
Epoch: [0]  [ 6550/60301]  eta: 7:30:00  lr: 0.000031  min_lr: 0.000001  loss: 5.5182 (5.7288)  loss_scale: 131072.0000 (90711.0072)  weight_decay: 0.0500 (0.0500)  time: 0.3174  data: 0.0554  max mem: 8161
Epoch: [0]  [ 6560/60301]  eta: 7:29:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4892 (5.7285)  loss_scale: 131072.0000 (90772.5237)  weight_decay: 0.0500 (0.0500)  time: 0.3598  data: 0.2448  max mem: 8161
Epoch: [0]  [ 6570/60301]  eta: 7:29:46  lr: 0.000031  min_lr: 0.000001  loss: 5.5263 (5.7282)  loss_scale: 131072.0000 (90833.8530)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.3679  max mem: 8161
Epoch: [0]  [ 6580/60301]  eta: 7:29:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4485 (5.7277)  loss_scale: 131072.0000 (90894.9959)  weight_decay: 0.0500 (0.0500)  time: 0.4706  data: 0.3575  max mem: 8161
Epoch: [0]  [ 6590/60301]  eta: 7:29:49  lr: 0.000031  min_lr: 0.000001  loss: 5.4848 (5.7275)  loss_scale: 131072.0000 (90955.9533)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.2800  max mem: 8161
Epoch: [0]  [ 6600/60301]  eta: 7:29:42  lr: 0.000031  min_lr: 0.000001  loss: 5.4205 (5.7270)  loss_scale: 131072.0000 (91016.7260)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1010  max mem: 8161
Epoch: [0]  [ 6610/60301]  eta: 7:29:48  lr: 0.000031  min_lr: 0.000001  loss: 5.3423 (5.7266)  loss_scale: 131072.0000 (91077.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0816  max mem: 8161
Epoch: [0]  [ 6620/60301]  eta: 7:29:41  lr: 0.000031  min_lr: 0.000001  loss: 5.5439 (5.7264)  loss_scale: 131072.0000 (91137.7206)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0816  max mem: 8161
Epoch: [0]  [ 6630/60301]  eta: 7:29:35  lr: 0.000031  min_lr: 0.000001  loss: 5.5558 (5.7263)  loss_scale: 131072.0000 (91197.9442)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0021  max mem: 8161
Epoch: [0]  [ 6640/60301]  eta: 7:29:28  lr: 0.000031  min_lr: 0.000001  loss: 5.5857 (5.7262)  loss_scale: 131072.0000 (91257.9864)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0037  max mem: 8161
Epoch: [0]  [ 6650/60301]  eta: 7:29:22  lr: 0.000031  min_lr: 0.000001  loss: 5.5700 (5.7259)  loss_scale: 131072.0000 (91317.8481)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0020  max mem: 8161
Epoch: [0]  [ 6660/60301]  eta: 7:29:15  lr: 0.000031  min_lr: 0.000001  loss: 5.5354 (5.7257)  loss_scale: 131072.0000 (91377.5301)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6670/60301]  eta: 7:29:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3841 (5.7251)  loss_scale: 131072.0000 (91437.0331)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6680/60301]  eta: 7:29:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3294 (5.7247)  loss_scale: 131072.0000 (91496.3580)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0013  max mem: 8161
Epoch: [0]  [ 6690/60301]  eta: 7:28:55  lr: 0.000031  min_lr: 0.000001  loss: 5.4968 (5.7244)  loss_scale: 131072.0000 (91555.5056)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0022  max mem: 8161
Epoch: [0]  [ 6700/60301]  eta: 7:28:49  lr: 0.000031  min_lr: 0.000001  loss: 5.4612 (5.7241)  loss_scale: 131072.0000 (91614.4766)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0023  max mem: 8161
Epoch: [0]  [ 6710/60301]  eta: 7:28:43  lr: 0.000031  min_lr: 0.000001  loss: 5.5226 (5.7238)  loss_scale: 131072.0000 (91673.2719)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0025  max mem: 8161
[2023-07-14 01:15:50,278] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:15:50,279] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 01:15:51,237] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3359
[2023-07-14 01:15:51,237] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:15:51,237] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 6720/60301]  eta: 7:28:35  lr: 0.000031  min_lr: 0.000001  loss: 5.6144 (5.7238)  loss_scale: 131072.0000 (91770.8960)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0016  max mem: 8161
Epoch: [0]  [ 6730/60301]  eta: 7:28:29  lr: 0.000031  min_lr: 0.000001  loss: 5.6543 (5.7236)  loss_scale: 131072.0000 (91829.2842)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0015  max mem: 8161
Epoch: [0]  [ 6740/60301]  eta: 7:28:22  lr: 0.000031  min_lr: 0.000001  loss: 5.5450 (5.7232)  loss_scale: 131072.0000 (91887.4992)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0015  max mem: 8161
Epoch: [0]  [ 6750/60301]  eta: 7:28:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5450 (5.7229)  loss_scale: 131072.0000 (91945.5417)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0068  max mem: 8161
Epoch: [0]  [ 6760/60301]  eta: 7:28:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5376 (5.7226)  loss_scale: 131072.0000 (92003.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0459  max mem: 8161
Epoch: [0]  [ 6770/60301]  eta: 7:28:10  lr: 0.000031  min_lr: 0.000001  loss: 5.4206 (5.7222)  loss_scale: 131072.0000 (92061.1124)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0403  max mem: 8161
Epoch: [0]  [ 6780/60301]  eta: 7:28:04  lr: 0.000031  min_lr: 0.000001  loss: 5.6044 (5.7221)  loss_scale: 131072.0000 (92118.6421)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0012  max mem: 8161
Epoch: [0]  [ 6790/60301]  eta: 7:27:57  lr: 0.000031  min_lr: 0.000001  loss: 5.5740 (5.7218)  loss_scale: 131072.0000 (92176.0024)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6800/60301]  eta: 7:27:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4398 (5.7215)  loss_scale: 131072.0000 (92233.1939)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0004  max mem: 8161
Epoch: [0]  [ 6810/60301]  eta: 7:27:25  lr: 0.000031  min_lr: 0.000001  loss: 5.4930 (5.7212)  loss_scale: 131072.0000 (92290.2176)  weight_decay: 0.0500 (0.0500)  time: 0.3604  data: 0.0607  max mem: 8161
Epoch: [0]  [ 6820/60301]  eta: 7:27:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4930 (5.7208)  loss_scale: 131072.0000 (92347.0740)  weight_decay: 0.0500 (0.0500)  time: 0.3696  data: 0.2546  max mem: 8161
Epoch: [0]  [ 6830/60301]  eta: 7:27:25  lr: 0.000031  min_lr: 0.000001  loss: 5.4415 (5.7206)  loss_scale: 131072.0000 (92403.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.4534  max mem: 8161
Epoch: [0]  [ 6840/60301]  eta: 7:27:01  lr: 0.000031  min_lr: 0.000001  loss: 5.5300 (5.7203)  loss_scale: 131072.0000 (92460.2883)  weight_decay: 0.0500 (0.0500)  time: 0.4445  data: 0.3321  max mem: 8161
Epoch: [0]  [ 6850/60301]  eta: 7:27:13  lr: 0.000031  min_lr: 0.000001  loss: 5.4944 (5.7198)  loss_scale: 131072.0000 (92516.6475)  weight_decay: 0.0500 (0.0500)  time: 0.4925  data: 0.1960  max mem: 8161
Epoch: [0]  [ 6860/60301]  eta: 7:27:07  lr: 0.000031  min_lr: 0.000001  loss: 5.4944 (5.7195)  loss_scale: 131072.0000 (92572.8424)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1255  max mem: 8161
Epoch: [0]  [ 6870/60301]  eta: 7:27:13  lr: 0.000031  min_lr: 0.000001  loss: 5.5662 (5.7191)  loss_scale: 131072.0000 (92628.8738)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0770  max mem: 8161
Epoch: [0]  [ 6880/60301]  eta: 7:27:07  lr: 0.000031  min_lr: 0.000001  loss: 5.4993 (5.7187)  loss_scale: 131072.0000 (92684.7423)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0749  max mem: 8161
Epoch: [0]  [ 6890/60301]  eta: 7:27:01  lr: 0.000031  min_lr: 0.000001  loss: 5.4573 (5.7183)  loss_scale: 131072.0000 (92740.4487)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6900/60301]  eta: 7:26:55  lr: 0.000031  min_lr: 0.000001  loss: 5.4466 (5.7179)  loss_scale: 131072.0000 (92795.9936)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6910/60301]  eta: 7:26:48  lr: 0.000031  min_lr: 0.000001  loss: 5.5163 (5.7175)  loss_scale: 131072.0000 (92851.3778)  weight_decay: 0.0500 (0.0500)  time: 0.4877  data: 0.0014  max mem: 8161
Epoch: [0]  [ 6920/60301]  eta: 7:26:42  lr: 0.000031  min_lr: 0.000001  loss: 5.6173 (5.7175)  loss_scale: 131072.0000 (92906.6019)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0021  max mem: 8161
Epoch: [0]  [ 6930/60301]  eta: 7:26:36  lr: 0.000031  min_lr: 0.000001  loss: 5.6905 (5.7174)  loss_scale: 131072.0000 (92961.6667)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0023  max mem: 8161
Epoch: [0]  [ 6940/60301]  eta: 7:26:30  lr: 0.000031  min_lr: 0.000001  loss: 5.6358 (5.7170)  loss_scale: 131072.0000 (93016.5728)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0016  max mem: 8161
Epoch: [0]  [ 6950/60301]  eta: 7:26:24  lr: 0.000031  min_lr: 0.000001  loss: 5.3619 (5.7167)  loss_scale: 131072.0000 (93071.3210)  weight_decay: 0.0500 (0.0500)  time: 0.4885  data: 0.0024  max mem: 8161
Epoch: [0]  [ 6960/60301]  eta: 7:26:17  lr: 0.000031  min_lr: 0.000001  loss: 5.3555 (5.7162)  loss_scale: 131072.0000 (93125.9118)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0033  max mem: 8161
Epoch: [0]  [ 6970/60301]  eta: 7:26:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3723 (5.7159)  loss_scale: 131072.0000 (93180.3460)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0015  max mem: 8161
[2023-07-14 01:18:02,440] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:18:02,441] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 6980/60301]  eta: 7:26:36  lr: 0.000031  min_lr: 0.000001  loss: 5.4023 (5.7154)  loss_scale: 131072.0000 (93309.7264)  weight_decay: 0.0500 (0.0500)  time: 0.6930  data: 0.2089  max mem: 8161
[2023-07-14 01:18:08,421] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3493
[2023-07-14 01:18:08,422] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:18:08,422] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 6990/60301]  eta: 7:26:38  lr: 0.000031  min_lr: 0.000001  loss: 5.3754 (5.7149)  loss_scale: 131072.0000 (93476.2340)  weight_decay: 0.0500 (0.0500)  time: 0.7441  data: 0.2638  max mem: 8161
[2023-07-14 01:18:15,264] [INFO] [timer.py:181:stop] 0/7000, SamplesPerSec=9.367474502264665
Epoch: [0]  [ 7000/60301]  eta: 7:26:39  lr: 0.000031  min_lr: 0.000001  loss: 5.3136 (5.7143)  loss_scale: 131072.0000 (93529.9346)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1087  max mem: 8161
Epoch: [0]  [ 7010/60301]  eta: 7:26:33  lr: 0.000031  min_lr: 0.000001  loss: 5.3136 (5.7137)  loss_scale: 131072.0000 (93583.4820)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0543  max mem: 8161
Epoch: [0]  [ 7020/60301]  eta: 7:26:27  lr: 0.000031  min_lr: 0.000001  loss: 5.4423 (5.7133)  loss_scale: 131072.0000 (93636.8768)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0012  max mem: 8161
Epoch: [0]  [ 7030/60301]  eta: 7:26:21  lr: 0.000031  min_lr: 0.000001  loss: 5.4919 (5.7131)  loss_scale: 131072.0000 (93690.1198)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7040/60301]  eta: 7:26:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4375 (5.7126)  loss_scale: 131072.0000 (93743.2115)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7050/60301]  eta: 7:26:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4375 (5.7124)  loss_scale: 131072.0000 (93796.1526)  weight_decay: 0.0500 (0.0500)  time: 0.4916  data: 0.0071  max mem: 8161
Epoch: [0]  [ 7060/60301]  eta: 7:26:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5628 (5.7121)  loss_scale: 131072.0000 (93848.9438)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0290  max mem: 8161
Epoch: [0]  [ 7070/60301]  eta: 7:26:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4879 (5.7118)  loss_scale: 131072.0000 (93901.5856)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.1762  max mem: 8161
Epoch: [0]  [ 7080/60301]  eta: 7:26:16  lr: 0.000031  min_lr: 0.000001  loss: 5.4780 (5.7113)  loss_scale: 131072.0000 (93954.0788)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1543  max mem: 8161
Epoch: [0]  [ 7090/60301]  eta: 7:26:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4624 (5.7111)  loss_scale: 131072.0000 (94006.4239)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7100/60301]  eta: 7:26:03  lr: 0.000031  min_lr: 0.000001  loss: 5.6458 (5.7110)  loss_scale: 131072.0000 (94058.6216)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7110/60301]  eta: 7:25:57  lr: 0.000031  min_lr: 0.000001  loss: 5.4858 (5.7106)  loss_scale: 131072.0000 (94110.6725)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7120/60301]  eta: 7:25:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4671 (5.7103)  loss_scale: 131072.0000 (94162.5772)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7130/60301]  eta: 7:25:44  lr: 0.000031  min_lr: 0.000001  loss: 5.5063 (5.7099)  loss_scale: 131072.0000 (94214.3363)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7140/60301]  eta: 7:25:38  lr: 0.000031  min_lr: 0.000001  loss: 5.4930 (5.7095)  loss_scale: 131072.0000 (94265.9504)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7150/60301]  eta: 7:25:31  lr: 0.000031  min_lr: 0.000001  loss: 5.4744 (5.7092)  loss_scale: 131072.0000 (94317.4202)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7160/60301]  eta: 7:25:25  lr: 0.000031  min_lr: 0.000001  loss: 5.6081 (5.7091)  loss_scale: 131072.0000 (94368.7463)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7170/60301]  eta: 7:25:38  lr: 0.000031  min_lr: 0.000001  loss: 5.6081 (5.7089)  loss_scale: 131072.0000 (94419.9292)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1379  max mem: 8161
Epoch: [0]  [ 7180/60301]  eta: 7:25:32  lr: 0.000031  min_lr: 0.000001  loss: 5.3949 (5.7084)  loss_scale: 131072.0000 (94470.9695)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1380  max mem: 8161
Epoch: [0]  [ 7190/60301]  eta: 7:25:26  lr: 0.000031  min_lr: 0.000001  loss: 5.3248 (5.7081)  loss_scale: 131072.0000 (94521.8679)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.0016  max mem: 8161
Epoch: [0]  [ 7200/60301]  eta: 7:25:20  lr: 0.000031  min_lr: 0.000001  loss: 5.3333 (5.7077)  loss_scale: 131072.0000 (94572.6249)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0016  max mem: 8161
Epoch: [0]  [ 7210/60301]  eta: 7:25:15  lr: 0.000031  min_lr: 0.000001  loss: 5.4034 (5.7072)  loss_scale: 131072.0000 (94623.2412)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0126  max mem: 8161
Epoch: [0]  [ 7220/60301]  eta: 7:25:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4034 (5.7068)  loss_scale: 131072.0000 (94673.7172)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0137  max mem: 8161
Epoch: [0]  [ 7230/60301]  eta: 7:25:03  lr: 0.000031  min_lr: 0.000001  loss: 5.2500 (5.7062)  loss_scale: 131072.0000 (94724.0537)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0016  max mem: 8161
Epoch: [0]  [ 7240/60301]  eta: 7:24:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3062 (5.7058)  loss_scale: 131072.0000 (94774.2511)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0005  max mem: 8161
[2023-07-14 01:20:21,329] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:20:21,329] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 7250/60301]  eta: 7:24:50  lr: 0.000031  min_lr: 0.000001  loss: 5.5109 (5.7056)  loss_scale: 131072.0000 (94932.7684)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0016  max mem: 8161
Epoch: [0]  [ 7260/60301]  eta: 7:24:44  lr: 0.000031  min_lr: 0.000001  loss: 5.5264 (5.7054)  loss_scale: 262144.0000 (95163.0552)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0016  max mem: 8161
[2023-07-14 01:20:31,055] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3632
[2023-07-14 01:20:31,055] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:20:31,055] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 7270/60301]  eta: 7:24:37  lr: 0.000031  min_lr: 0.000001  loss: 5.5377 (5.7051)  loss_scale: 262144.0000 (95284.5485)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7280/60301]  eta: 7:24:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4302 (5.7047)  loss_scale: 131072.0000 (95333.7003)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0015  max mem: 8161
Epoch: [0]  [ 7290/60301]  eta: 7:24:24  lr: 0.000031  min_lr: 0.000001  loss: 5.4557 (5.7044)  loss_scale: 131072.0000 (95382.7173)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7300/60301]  eta: 7:24:07  lr: 0.000031  min_lr: 0.000001  loss: 5.3946 (5.7040)  loss_scale: 131072.0000 (95431.6001)  weight_decay: 0.0500 (0.0500)  time: 0.4118  data: 0.1044  max mem: 8161
Epoch: [0]  [ 7310/60301]  eta: 7:23:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4114 (5.7037)  loss_scale: 131072.0000 (95480.3491)  weight_decay: 0.0500 (0.0500)  time: 0.3797  data: 0.2581  max mem: 8161
Epoch: [0]  [ 7320/60301]  eta: 7:24:15  lr: 0.000031  min_lr: 0.000001  loss: 5.4119 (5.7032)  loss_scale: 131072.0000 (95528.9649)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.5168  max mem: 8161
Epoch: [0]  [ 7330/60301]  eta: 7:24:18  lr: 0.000031  min_lr: 0.000001  loss: 5.4772 (5.7030)  loss_scale: 131072.0000 (95577.4481)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.4928  max mem: 8161
Epoch: [0]  [ 7340/60301]  eta: 7:24:11  lr: 0.000031  min_lr: 0.000001  loss: 5.5434 (5.7028)  loss_scale: 131072.0000 (95625.7992)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1338  max mem: 8161
Epoch: [0]  [ 7350/60301]  eta: 7:24:05  lr: 0.000031  min_lr: 0.000001  loss: 5.5434 (5.7027)  loss_scale: 131072.0000 (95674.0188)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0046  max mem: 8161
Epoch: [0]  [ 7360/60301]  eta: 7:23:58  lr: 0.000031  min_lr: 0.000001  loss: 5.4898 (5.7023)  loss_scale: 131072.0000 (95722.1073)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0019  max mem: 8161
Epoch: [0]  [ 7370/60301]  eta: 7:23:52  lr: 0.000031  min_lr: 0.000001  loss: 5.4227 (5.7019)  loss_scale: 131072.0000 (95770.0654)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7380/60301]  eta: 7:23:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3522 (5.7016)  loss_scale: 131072.0000 (95817.8935)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7390/60301]  eta: 7:23:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4327 (5.7013)  loss_scale: 131072.0000 (95865.5922)  weight_decay: 0.0500 (0.0500)  time: 0.4807  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7400/60301]  eta: 7:23:32  lr: 0.000031  min_lr: 0.000001  loss: 5.5173 (5.7010)  loss_scale: 131072.0000 (95913.1620)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0016  max mem: 8161
Epoch: [0]  [ 7410/60301]  eta: 7:23:26  lr: 0.000031  min_lr: 0.000001  loss: 5.4339 (5.7006)  loss_scale: 131072.0000 (95960.6034)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0042  max mem: 8161
Epoch: [0]  [ 7420/60301]  eta: 7:23:19  lr: 0.000031  min_lr: 0.000001  loss: 5.3717 (5.7001)  loss_scale: 131072.0000 (96007.9170)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0046  max mem: 8161
Epoch: [0]  [ 7430/60301]  eta: 7:23:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4904 (5.7000)  loss_scale: 131072.0000 (96055.1032)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0021  max mem: 8161
Epoch: [0]  [ 7440/60301]  eta: 7:23:05  lr: 0.000031  min_lr: 0.000001  loss: 5.5639 (5.6998)  loss_scale: 131072.0000 (96102.1626)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7450/60301]  eta: 7:22:59  lr: 0.000031  min_lr: 0.000001  loss: 5.3812 (5.6994)  loss_scale: 131072.0000 (96149.0957)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0022  max mem: 8161
Epoch: [0]  [ 7460/60301]  eta: 7:22:53  lr: 0.000031  min_lr: 0.000001  loss: 5.4312 (5.6992)  loss_scale: 131072.0000 (96195.9030)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0023  max mem: 8161
Epoch: [0]  [ 7470/60301]  eta: 7:22:46  lr: 0.000031  min_lr: 0.000001  loss: 5.5683 (5.6991)  loss_scale: 131072.0000 (96242.5849)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7480/60301]  eta: 7:22:39  lr: 0.000031  min_lr: 0.000001  loss: 5.5841 (5.6989)  loss_scale: 131072.0000 (96289.1421)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0011  max mem: 8161
Epoch: [0]  [ 7490/60301]  eta: 7:22:33  lr: 0.000031  min_lr: 0.000001  loss: 5.4340 (5.6986)  loss_scale: 131072.0000 (96335.5750)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0019  max mem: 8161
Epoch: [0]  [ 7500/60301]  eta: 7:22:26  lr: 0.000031  min_lr: 0.000001  loss: 5.4086 (5.6983)  loss_scale: 131072.0000 (96381.8840)  weight_decay: 0.0500 (0.0500)  time: 0.4784  data: 0.0023  max mem: 8161
Epoch: [0]  [ 7510/60301]  eta: 7:22:19  lr: 0.000031  min_lr: 0.000001  loss: 5.3938 (5.6979)  loss_scale: 131072.0000 (96428.0698)  weight_decay: 0.0500 (0.0500)  time: 0.4780  data: 0.0015  max mem: 8161
Epoch: [0]  [ 7520/60301]  eta: 7:22:13  lr: 0.000031  min_lr: 0.000001  loss: 5.4390 (5.6976)  loss_scale: 131072.0000 (96474.1327)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0053  max mem: 8161
[2023-07-14 01:22:38,172] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:22:38,172] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 7530/60301]  eta: 7:22:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4785 (5.6975)  loss_scale: 131072.0000 (96659.3079)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0443  max mem: 8161
[2023-07-14 01:22:44,840] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3767
[2023-07-14 01:22:44,840] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:22:44,840] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 7540/60301]  eta: 7:21:59  lr: 0.000031  min_lr: 0.000001  loss: 5.4093 (5.6969)  loss_scale: 262144.0000 (96774.4670)  weight_decay: 0.0500 (0.0500)  time: 0.4702  data: 0.0746  max mem: 8161
Epoch: [0]  [ 7550/60301]  eta: 7:21:50  lr: 0.000031  min_lr: 0.000001  loss: 5.2815 (5.6966)  loss_scale: 131072.0000 (96819.8882)  weight_decay: 0.0500 (0.0500)  time: 0.4178  data: 0.2044  max mem: 8161
Epoch: [0]  [ 7560/60301]  eta: 7:21:32  lr: 0.000031  min_lr: 0.000001  loss: 5.4451 (5.6962)  loss_scale: 131072.0000 (96865.1893)  weight_decay: 0.0500 (0.0500)  time: 0.3869  data: 0.2756  max mem: 8161
Epoch: [0]  [ 7570/60301]  eta: 7:21:29  lr: 0.000031  min_lr: 0.000001  loss: 5.4024 (5.6958)  loss_scale: 131072.0000 (96910.3706)  weight_decay: 0.0500 (0.0500)  time: 0.4269  data: 0.3151  max mem: 8161
Epoch: [0]  [ 7580/60301]  eta: 7:21:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3409 (5.6954)  loss_scale: 131072.0000 (96955.4328)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.3873  max mem: 8161
Epoch: [0]  [ 7590/60301]  eta: 7:21:38  lr: 0.000031  min_lr: 0.000001  loss: 5.4464 (5.6950)  loss_scale: 131072.0000 (97000.3762)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1940  max mem: 8161
Epoch: [0]  [ 7600/60301]  eta: 7:21:32  lr: 0.000031  min_lr: 0.000001  loss: 5.4464 (5.6948)  loss_scale: 131072.0000 (97045.2014)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0189  max mem: 8161
Epoch: [0]  [ 7610/60301]  eta: 7:21:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4158 (5.6945)  loss_scale: 131072.0000 (97089.9088)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0746  max mem: 8161
Epoch: [0]  [ 7620/60301]  eta: 7:21:44  lr: 0.000031  min_lr: 0.000001  loss: 5.4158 (5.6942)  loss_scale: 131072.0000 (97134.4989)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.1870  max mem: 8161
Epoch: [0]  [ 7630/60301]  eta: 7:21:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4481 (5.6938)  loss_scale: 131072.0000 (97178.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1164  max mem: 8161
Epoch: [0]  [ 7640/60301]  eta: 7:21:31  lr: 0.000031  min_lr: 0.000001  loss: 5.4481 (5.6937)  loss_scale: 131072.0000 (97223.3289)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7650/60301]  eta: 7:21:24  lr: 0.000031  min_lr: 0.000001  loss: 5.6276 (5.6936)  loss_scale: 131072.0000 (97267.5697)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7660/60301]  eta: 7:21:18  lr: 0.000031  min_lr: 0.000001  loss: 5.4708 (5.6932)  loss_scale: 131072.0000 (97311.6951)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7670/60301]  eta: 7:21:11  lr: 0.000031  min_lr: 0.000001  loss: 5.4339 (5.6929)  loss_scale: 131072.0000 (97355.7054)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7680/60301]  eta: 7:21:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4292 (5.6926)  loss_scale: 131072.0000 (97399.6011)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1175  max mem: 8161
Epoch: [0]  [ 7690/60301]  eta: 7:21:13  lr: 0.000031  min_lr: 0.000001  loss: 5.4767 (5.6923)  loss_scale: 131072.0000 (97443.3827)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1176  max mem: 8161
Epoch: [0]  [ 7700/60301]  eta: 7:21:06  lr: 0.000031  min_lr: 0.000001  loss: 5.5514 (5.6921)  loss_scale: 131072.0000 (97487.0505)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7710/60301]  eta: 7:21:00  lr: 0.000031  min_lr: 0.000001  loss: 5.5939 (5.6920)  loss_scale: 131072.0000 (97530.6051)  weight_decay: 0.0500 (0.0500)  time: 0.4781  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7720/60301]  eta: 7:20:53  lr: 0.000031  min_lr: 0.000001  loss: 5.4530 (5.6915)  loss_scale: 131072.0000 (97574.0469)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7730/60301]  eta: 7:20:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4966 (5.6914)  loss_scale: 131072.0000 (97617.3763)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7740/60301]  eta: 7:20:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5059 (5.6910)  loss_scale: 131072.0000 (97660.5937)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7750/60301]  eta: 7:20:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2863 (5.6905)  loss_scale: 131072.0000 (97703.6997)  weight_decay: 0.0500 (0.0500)  time: 0.3242  data: 0.0291  max mem: 8161
Epoch: [0]  [ 7760/60301]  eta: 7:20:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4523 (5.6902)  loss_scale: 131072.0000 (97746.6945)  weight_decay: 0.0500 (0.0500)  time: 0.3454  data: 0.2323  max mem: 8161
Epoch: [0]  [ 7770/60301]  eta: 7:19:52  lr: 0.000031  min_lr: 0.000001  loss: 5.4995 (5.6899)  loss_scale: 131072.0000 (97789.5787)  weight_decay: 0.0500 (0.0500)  time: 0.4282  data: 0.3160  max mem: 8161
Epoch: [0]  [ 7780/60301]  eta: 7:19:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4522 (5.6896)  loss_scale: 131072.0000 (97832.3527)  weight_decay: 0.0500 (0.0500)  time: 0.4712  data: 0.3592  max mem: 8161
Epoch: [0]  [ 7790/60301]  eta: 7:19:45  lr: 0.000031  min_lr: 0.000001  loss: 5.4522 (5.6893)  loss_scale: 131072.0000 (97875.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.3328  max mem: 8161
[2023-07-14 01:24:52,173] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:24:52,173] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 7800/60301]  eta: 7:19:47  lr: 0.000031  min_lr: 0.000001  loss: 5.3112 (5.6888)  loss_scale: 131072.0000 (98051.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.1539  max mem: 8161
[2023-07-14 01:25:01,189] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3904
[2023-07-14 01:25:01,189] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:25:01,189] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 7810/60301]  eta: 7:19:40  lr: 0.000031  min_lr: 0.000001  loss: 5.4873 (5.6887)  loss_scale: 262144.0000 (98228.5044)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0689  max mem: 8161
Epoch: [0]  [ 7820/60301]  eta: 7:19:34  lr: 0.000031  min_lr: 0.000001  loss: 5.5425 (5.6884)  loss_scale: 131072.0000 (98270.4984)  weight_decay: 0.0500 (0.0500)  time: 0.4753  data: 0.0014  max mem: 8161
Epoch: [0]  [ 7830/60301]  eta: 7:19:27  lr: 0.000031  min_lr: 0.000001  loss: 5.3713 (5.6880)  loss_scale: 131072.0000 (98312.3851)  weight_decay: 0.0500 (0.0500)  time: 0.4791  data: 0.0004  max mem: 8161
Epoch: [0]  [ 7840/60301]  eta: 7:19:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3713 (5.6876)  loss_scale: 131072.0000 (98354.1650)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0013  max mem: 8161
Epoch: [0]  [ 7850/60301]  eta: 7:19:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5178 (5.6875)  loss_scale: 131072.0000 (98395.8385)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0023  max mem: 8161
Epoch: [0]  [ 7860/60301]  eta: 7:19:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5509 (5.6874)  loss_scale: 131072.0000 (98437.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0485  max mem: 8161
Epoch: [0]  [ 7870/60301]  eta: 7:19:20  lr: 0.000031  min_lr: 0.000001  loss: 5.6289 (5.6873)  loss_scale: 131072.0000 (98478.8677)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1460  max mem: 8161
Epoch: [0]  [ 7880/60301]  eta: 7:19:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5577 (5.6869)  loss_scale: 131072.0000 (98520.2243)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0998  max mem: 8161
Epoch: [0]  [ 7890/60301]  eta: 7:19:09  lr: 0.000031  min_lr: 0.000001  loss: 5.5271 (5.6867)  loss_scale: 131072.0000 (98561.4761)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0086  max mem: 8161
Epoch: [0]  [ 7900/60301]  eta: 7:19:02  lr: 0.000031  min_lr: 0.000001  loss: 5.6093 (5.6865)  loss_scale: 131072.0000 (98602.6235)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0078  max mem: 8161
Epoch: [0]  [ 7910/60301]  eta: 7:18:56  lr: 0.000031  min_lr: 0.000001  loss: 5.6269 (5.6864)  loss_scale: 131072.0000 (98643.6668)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0005  max mem: 8161
Epoch: [0]  [ 7920/60301]  eta: 7:18:50  lr: 0.000031  min_lr: 0.000001  loss: 5.5197 (5.6861)  loss_scale: 131072.0000 (98684.6065)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0057  max mem: 8161
Epoch: [0]  [ 7930/60301]  eta: 7:18:44  lr: 0.000031  min_lr: 0.000001  loss: 5.5047 (5.6859)  loss_scale: 131072.0000 (98725.4429)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0067  max mem: 8161
Epoch: [0]  [ 7940/60301]  eta: 7:18:37  lr: 0.000031  min_lr: 0.000001  loss: 5.5448 (5.6858)  loss_scale: 131072.0000 (98766.1766)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0015  max mem: 8161
Epoch: [0]  [ 7950/60301]  eta: 7:18:28  lr: 0.000031  min_lr: 0.000001  loss: 5.5448 (5.6856)  loss_scale: 131072.0000 (98806.8077)  weight_decay: 0.0500 (0.0500)  time: 0.4635  data: 0.0015  max mem: 8161
Epoch: [0]  [ 7960/60301]  eta: 7:18:18  lr: 0.000031  min_lr: 0.000001  loss: 5.4299 (5.6853)  loss_scale: 131072.0000 (98847.3368)  weight_decay: 0.0500 (0.0500)  time: 0.4336  data: 0.1552  max mem: 8161
Epoch: [0]  [ 7970/60301]  eta: 7:18:00  lr: 0.000031  min_lr: 0.000001  loss: 5.3973 (5.6849)  loss_scale: 131072.0000 (98887.7641)  weight_decay: 0.0500 (0.0500)  time: 0.3645  data: 0.2528  max mem: 8161
Epoch: [0]  [ 7980/60301]  eta: 7:17:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2418 (5.6844)  loss_scale: 131072.0000 (98928.0902)  weight_decay: 0.0500 (0.0500)  time: 0.3997  data: 0.2886  max mem: 8161
Epoch: [0]  [ 7990/60301]  eta: 7:17:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3860 (5.6842)  loss_scale: 131072.0000 (98968.3154)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.4330  max mem: 8161
[2023-07-14 01:26:34,454] [INFO] [logging.py:69:log_dist] [Rank 0] step=4000, skipped=19, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 01:26:34,458] [INFO] [timer.py:181:stop] 0/8000, SamplesPerSec=9.40308770496662
Epoch: [0]  [ 8000/60301]  eta: 7:17:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4825 (5.6838)  loss_scale: 131072.0000 (99008.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.2434  max mem: 8161
Epoch: [0]  [ 8010/60301]  eta: 7:17:45  lr: 0.000031  min_lr: 0.000001  loss: 5.4397 (5.6835)  loss_scale: 131072.0000 (99048.4644)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0139  max mem: 8161
Epoch: [0]  [ 8020/60301]  eta: 7:17:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4671 (5.6833)  loss_scale: 131072.0000 (99088.3890)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0149  max mem: 8161
Epoch: [0]  [ 8030/60301]  eta: 7:17:33  lr: 0.000031  min_lr: 0.000001  loss: 5.4671 (5.6830)  loss_scale: 131072.0000 (99128.2142)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0032  max mem: 8161
Epoch: [0]  [ 8040/60301]  eta: 7:17:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4376 (5.6828)  loss_scale: 131072.0000 (99167.9403)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0323  max mem: 8161
Epoch: [0]  [ 8050/60301]  eta: 7:17:40  lr: 0.000031  min_lr: 0.000001  loss: 5.5549 (5.6825)  loss_scale: 131072.0000 (99207.5678)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1606  max mem: 8161
Epoch: [0]  [ 8060/60301]  eta: 7:17:34  lr: 0.000031  min_lr: 0.000001  loss: 5.5596 (5.6824)  loss_scale: 131072.0000 (99247.0969)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1305  max mem: 8161
[2023-07-14 01:27:10,607] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:27:10,608] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 8070/60301]  eta: 7:17:28  lr: 0.000031  min_lr: 0.000001  loss: 5.6197 (5.6823)  loss_scale: 131072.0000 (99351.4875)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0004  max mem: 8161
Epoch: [0]  [ 8080/60301]  eta: 7:17:21  lr: 0.000031  min_lr: 0.000001  loss: 5.4278 (5.6819)  loss_scale: 262144.0000 (99552.9385)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0004  max mem: 8161
[2023-07-14 01:27:20,207] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4043
[2023-07-14 01:27:20,207] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:27:20,207] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 8090/60301]  eta: 7:17:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4035 (5.6816)  loss_scale: 262144.0000 (99689.0926)  weight_decay: 0.0500 (0.0500)  time: 0.4759  data: 0.0005  max mem: 8161
Epoch: [0]  [ 8100/60301]  eta: 7:17:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4665 (5.6815)  loss_scale: 131072.0000 (99727.8321)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0050  max mem: 8161
Epoch: [0]  [ 8110/60301]  eta: 7:17:02  lr: 0.000031  min_lr: 0.000001  loss: 5.5293 (5.6811)  loss_scale: 131072.0000 (99766.4761)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0050  max mem: 8161
Epoch: [0]  [ 8120/60301]  eta: 7:16:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3805 (5.6808)  loss_scale: 131072.0000 (99805.0250)  weight_decay: 0.0500 (0.0500)  time: 0.4912  data: 0.0105  max mem: 8161
Epoch: [0]  [ 8130/60301]  eta: 7:16:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4486 (5.6805)  loss_scale: 131072.0000 (99843.4790)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0330  max mem: 8161
Epoch: [0]  [ 8140/60301]  eta: 7:16:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4486 (5.6801)  loss_scale: 131072.0000 (99881.8386)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0230  max mem: 8161
Epoch: [0]  [ 8150/60301]  eta: 7:16:57  lr: 0.000031  min_lr: 0.000001  loss: 5.4667 (5.6799)  loss_scale: 131072.0000 (99920.1040)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1272  max mem: 8161
Epoch: [0]  [ 8160/60301]  eta: 7:16:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4609 (5.6794)  loss_scale: 131072.0000 (99958.2757)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1272  max mem: 8161
Epoch: [0]  [ 8170/60301]  eta: 7:16:44  lr: 0.000031  min_lr: 0.000001  loss: 5.1781 (5.6790)  loss_scale: 131072.0000 (99996.3539)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0004  max mem: 8161
Epoch: [0]  [ 8180/60301]  eta: 7:16:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3460 (5.6786)  loss_scale: 131072.0000 (100034.3391)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1538  max mem: 8161
Epoch: [0]  [ 8190/60301]  eta: 7:16:55  lr: 0.000031  min_lr: 0.000001  loss: 5.3835 (5.6783)  loss_scale: 131072.0000 (100072.2315)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1817  max mem: 8161
Epoch: [0]  [ 8200/60301]  eta: 7:16:41  lr: 0.000031  min_lr: 0.000001  loss: 5.3951 (5.6779)  loss_scale: 131072.0000 (100110.0315)  weight_decay: 0.0500 (0.0500)  time: 0.4534  data: 0.1086  max mem: 8161
Epoch: [0]  [ 8210/60301]  eta: 7:16:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4267 (5.6776)  loss_scale: 131072.0000 (100147.7394)  weight_decay: 0.0500 (0.0500)  time: 0.4278  data: 0.2680  max mem: 8161
Epoch: [0]  [ 8220/60301]  eta: 7:16:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4789 (5.6773)  loss_scale: 131072.0000 (100185.3556)  weight_decay: 0.0500 (0.0500)  time: 0.4157  data: 0.3027  max mem: 8161
Epoch: [0]  [ 8230/60301]  eta: 7:16:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4590 (5.6770)  loss_scale: 131072.0000 (100222.8803)  weight_decay: 0.0500 (0.0500)  time: 0.4779  data: 0.3652  max mem: 8161
Epoch: [0]  [ 8240/60301]  eta: 7:16:16  lr: 0.000031  min_lr: 0.000001  loss: 5.3626 (5.6766)  loss_scale: 131072.0000 (100260.3140)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.2502  max mem: 8161
Epoch: [0]  [ 8250/60301]  eta: 7:16:10  lr: 0.000031  min_lr: 0.000001  loss: 5.3498 (5.6763)  loss_scale: 131072.0000 (100297.6570)  weight_decay: 0.0500 (0.0500)  time: 0.4850  data: 0.0028  max mem: 8161
Epoch: [0]  [ 8260/60301]  eta: 7:16:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4641 (5.6761)  loss_scale: 131072.0000 (100334.9096)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0027  max mem: 8161
Epoch: [0]  [ 8270/60301]  eta: 7:15:59  lr: 0.000031  min_lr: 0.000001  loss: 5.5368 (5.6759)  loss_scale: 131072.0000 (100372.0721)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0195  max mem: 8161
Epoch: [0]  [ 8280/60301]  eta: 7:16:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4612 (5.6755)  loss_scale: 131072.0000 (100409.1448)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1050  max mem: 8161
Epoch: [0]  [ 8290/60301]  eta: 7:15:58  lr: 0.000031  min_lr: 0.000001  loss: 5.3582 (5.6753)  loss_scale: 131072.0000 (100446.1281)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0934  max mem: 8161
Epoch: [0]  [ 8300/60301]  eta: 7:15:52  lr: 0.000031  min_lr: 0.000001  loss: 5.3785 (5.6750)  loss_scale: 131072.0000 (100483.0223)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0106  max mem: 8161
Epoch: [0]  [ 8310/60301]  eta: 7:15:48  lr: 0.000031  min_lr: 0.000001  loss: 5.2875 (5.6746)  loss_scale: 131072.0000 (100519.8277)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0232  max mem: 8161
Epoch: [0]  [ 8320/60301]  eta: 7:15:41  lr: 0.000031  min_lr: 0.000001  loss: 5.2875 (5.6743)  loss_scale: 131072.0000 (100556.5446)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0206  max mem: 8161
Epoch: [0]  [ 8330/60301]  eta: 7:15:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3427 (5.6738)  loss_scale: 131072.0000 (100593.1734)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0861  max mem: 8161
Epoch: [0]  [ 8340/60301]  eta: 7:15:39  lr: 0.000031  min_lr: 0.000001  loss: 5.2957 (5.6735)  loss_scale: 131072.0000 (100629.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0860  max mem: 8161
[2023-07-14 01:29:34,672] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:29:34,673] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 8350/60301]  eta: 7:15:33  lr: 0.000031  min_lr: 0.000001  loss: 5.2880 (5.6730)  loss_scale: 131072.0000 (100760.3401)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0052  max mem: 8161
Epoch: [0]  [ 8360/60301]  eta: 7:15:27  lr: 0.000031  min_lr: 0.000001  loss: 5.4135 (5.6730)  loss_scale: 262144.0000 (100953.3596)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0052  max mem: 8161
Epoch: [0]  [ 8370/60301]  eta: 7:15:21  lr: 0.000031  min_lr: 0.000001  loss: 5.6374 (5.6728)  loss_scale: 262144.0000 (101145.9181)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0004  max mem: 8161
Epoch: [0]  [ 8380/60301]  eta: 7:15:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4100 (5.6726)  loss_scale: 262144.0000 (101338.0169)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0005  max mem: 8161
Epoch: [0]  [ 8390/60301]  eta: 7:15:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5614 (5.6724)  loss_scale: 262144.0000 (101529.6580)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0723  max mem: 8161
Epoch: [0]  [ 8400/60301]  eta: 7:15:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3287 (5.6718)  loss_scale: 262144.0000 (101720.8428)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0723  max mem: 8161
Epoch: [0]  [ 8410/60301]  eta: 7:15:04  lr: 0.000031  min_lr: 0.000001  loss: 5.3809 (5.6716)  loss_scale: 262144.0000 (101911.5729)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0004  max mem: 8161
[2023-07-14 01:30:08,910] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4206
[2023-07-14 01:30:08,910] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:30:08,910] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 8420/60301]  eta: 7:14:57  lr: 0.000031  min_lr: 0.000001  loss: 5.4191 (5.6713)  loss_scale: 262144.0000 (101977.3310)  weight_decay: 0.0500 (0.0500)  time: 0.4781  data: 0.0013  max mem: 8161
Epoch: [0]  [ 8430/60301]  eta: 7:14:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4308 (5.6710)  loss_scale: 131072.0000 (102011.8401)  weight_decay: 0.0500 (0.0500)  time: 0.4775  data: 0.0013  max mem: 8161
Epoch: [0]  [ 8440/60301]  eta: 7:14:45  lr: 0.000031  min_lr: 0.000001  loss: 5.4704 (5.6708)  loss_scale: 131072.0000 (102046.2675)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0025  max mem: 8161
Epoch: [0]  [ 8450/60301]  eta: 7:14:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4704 (5.6706)  loss_scale: 131072.0000 (102080.6134)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0034  max mem: 8161
Epoch: [0]  [ 8460/60301]  eta: 7:14:32  lr: 0.000031  min_lr: 0.000001  loss: 5.4594 (5.6703)  loss_scale: 131072.0000 (102114.8781)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0013  max mem: 8161
Epoch: [0]  [ 8470/60301]  eta: 7:14:26  lr: 0.000031  min_lr: 0.000001  loss: 5.3633 (5.6700)  loss_scale: 131072.0000 (102149.0620)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0005  max mem: 8161
Epoch: [0]  [ 8480/60301]  eta: 7:14:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4621 (5.6698)  loss_scale: 131072.0000 (102183.1652)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0013  max mem: 8161
Epoch: [0]  [ 8490/60301]  eta: 7:14:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4887 (5.6696)  loss_scale: 131072.0000 (102217.1881)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0023  max mem: 8161
Epoch: [0]  [ 8500/60301]  eta: 7:14:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4630 (5.6693)  loss_scale: 131072.0000 (102251.1309)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0033  max mem: 8161
Epoch: [0]  [ 8510/60301]  eta: 7:14:18  lr: 0.000031  min_lr: 0.000001  loss: 5.5454 (5.6693)  loss_scale: 131072.0000 (102284.9940)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1355  max mem: 8161
Epoch: [0]  [ 8520/60301]  eta: 7:14:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4576 (5.6691)  loss_scale: 131072.0000 (102318.7776)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1595  max mem: 8161
Epoch: [0]  [ 8530/60301]  eta: 7:14:11  lr: 0.000031  min_lr: 0.000001  loss: 5.4021 (5.6687)  loss_scale: 131072.0000 (102352.4820)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0482  max mem: 8161
Epoch: [0]  [ 8540/60301]  eta: 7:14:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4046 (5.6685)  loss_scale: 131072.0000 (102386.1075)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0514  max mem: 8161
Epoch: [0]  [ 8550/60301]  eta: 7:14:01  lr: 0.000031  min_lr: 0.000001  loss: 5.4164 (5.6683)  loss_scale: 131072.0000 (102419.6543)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0295  max mem: 8161
Epoch: [0]  [ 8560/60301]  eta: 7:13:55  lr: 0.000031  min_lr: 0.000001  loss: 5.4326 (5.6680)  loss_scale: 131072.0000 (102453.1228)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0004  max mem: 8161
Epoch: [0]  [ 8570/60301]  eta: 7:14:01  lr: 0.000031  min_lr: 0.000001  loss: 5.4845 (5.6678)  loss_scale: 131072.0000 (102486.5131)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0991  max mem: 8161
Epoch: [0]  [ 8580/60301]  eta: 7:13:57  lr: 0.000031  min_lr: 0.000001  loss: 5.5294 (5.6676)  loss_scale: 131072.0000 (102519.8257)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1241  max mem: 8161
Epoch: [0]  [ 8590/60301]  eta: 7:13:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4730 (5.6673)  loss_scale: 131072.0000 (102553.0606)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0254  max mem: 8161
Epoch: [0]  [ 8600/60301]  eta: 7:13:45  lr: 0.000031  min_lr: 0.000001  loss: 5.4730 (5.6670)  loss_scale: 131072.0000 (102586.2183)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0005  max mem: 8161
Epoch: [0]  [ 8610/60301]  eta: 7:13:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4784 (5.6668)  loss_scale: 131072.0000 (102619.2990)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0014  max mem: 8161
Epoch: [0]  [ 8620/60301]  eta: 7:13:48  lr: 0.000031  min_lr: 0.000001  loss: 5.4684 (5.6665)  loss_scale: 131072.0000 (102652.3030)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1355  max mem: 8161
Epoch: [0]  [ 8630/60301]  eta: 7:13:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4107 (5.6663)  loss_scale: 131072.0000 (102685.2304)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.1805  max mem: 8161
Epoch: [0]  [ 8640/60301]  eta: 7:13:42  lr: 0.000031  min_lr: 0.000001  loss: 5.3621 (5.6660)  loss_scale: 131072.0000 (102718.0817)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0506  max mem: 8161
Epoch: [0]  [ 8650/60301]  eta: 7:13:42  lr: 0.000031  min_lr: 0.000001  loss: 5.3057 (5.6656)  loss_scale: 131072.0000 (102750.8570)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0641  max mem: 8161
Epoch: [0]  [ 8660/60301]  eta: 7:13:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3116 (5.6653)  loss_scale: 131072.0000 (102783.5566)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0598  max mem: 8161
Epoch: [0]  [ 8670/60301]  eta: 7:13:30  lr: 0.000031  min_lr: 0.000001  loss: 5.3506 (5.6650)  loss_scale: 131072.0000 (102816.1808)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0004  max mem: 8161
[2023-07-14 01:32:24,898] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:32:24,898] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 01:32:25,859] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4336
[2023-07-14 01:32:25,860] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:32:25,860] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 8680/60301]  eta: 7:13:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3978 (5.6649)  loss_scale: 131072.0000 (102878.9273)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.1881  max mem: 8161
Epoch: [0]  [ 8690/60301]  eta: 7:13:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3754 (5.6646)  loss_scale: 131072.0000 (102911.3667)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.2306  max mem: 8161
Epoch: [0]  [ 8700/60301]  eta: 7:13:46  lr: 0.000031  min_lr: 0.000001  loss: 5.3109 (5.6644)  loss_scale: 131072.0000 (102943.7315)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1179  max mem: 8161
Epoch: [0]  [ 8710/60301]  eta: 7:13:47  lr: 0.000031  min_lr: 0.000001  loss: 5.2948 (5.6640)  loss_scale: 131072.0000 (102976.0220)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1439  max mem: 8161
Epoch: [0]  [ 8720/60301]  eta: 7:13:44  lr: 0.000031  min_lr: 0.000001  loss: 5.3954 (5.6638)  loss_scale: 131072.0000 (103008.2385)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0970  max mem: 8161
Epoch: [0]  [ 8730/60301]  eta: 7:13:39  lr: 0.000031  min_lr: 0.000001  loss: 5.4148 (5.6636)  loss_scale: 131072.0000 (103040.3812)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0347  max mem: 8161
Epoch: [0]  [ 8740/60301]  eta: 7:13:32  lr: 0.000031  min_lr: 0.000001  loss: 5.5882 (5.6635)  loss_scale: 131072.0000 (103072.4503)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0082  max mem: 8161
Epoch: [0]  [ 8750/60301]  eta: 7:13:26  lr: 0.000031  min_lr: 0.000001  loss: 5.6713 (5.6633)  loss_scale: 131072.0000 (103104.4461)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0037  max mem: 8161
Epoch: [0]  [ 8760/60301]  eta: 7:13:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4519 (5.6632)  loss_scale: 131072.0000 (103136.3689)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0021  max mem: 8161
Epoch: [0]  [ 8770/60301]  eta: 7:13:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4724 (5.6630)  loss_scale: 131072.0000 (103168.2189)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0023  max mem: 8161
Epoch: [0]  [ 8780/60301]  eta: 7:12:48  lr: 0.000031  min_lr: 0.000001  loss: 5.5003 (5.6628)  loss_scale: 131072.0000 (103199.9964)  weight_decay: 0.0500 (0.0500)  time: 0.3149  data: 0.0188  max mem: 8161
Epoch: [0]  [ 8790/60301]  eta: 7:12:42  lr: 0.000031  min_lr: 0.000001  loss: 5.5003 (5.6626)  loss_scale: 131072.0000 (103231.7015)  weight_decay: 0.0500 (0.0500)  time: 0.3230  data: 0.2092  max mem: 8161
Epoch: [0]  [ 8800/60301]  eta: 7:13:01  lr: 0.000031  min_lr: 0.000001  loss: 5.3444 (5.6622)  loss_scale: 131072.0000 (103263.3346)  weight_decay: 0.0500 (0.0500)  time: 0.7017  data: 0.5885  max mem: 8161
Epoch: [0]  [ 8810/60301]  eta: 7:12:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2829 (5.6618)  loss_scale: 131072.0000 (103294.8959)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.4996  max mem: 8161
Epoch: [0]  [ 8820/60301]  eta: 7:12:49  lr: 0.000031  min_lr: 0.000001  loss: 5.4913 (5.6617)  loss_scale: 131072.0000 (103326.3857)  weight_decay: 0.0500 (0.0500)  time: 0.4929  data: 0.1161  max mem: 8161
Epoch: [0]  [ 8830/60301]  eta: 7:12:43  lr: 0.000031  min_lr: 0.000001  loss: 5.5672 (5.6615)  loss_scale: 131072.0000 (103357.8041)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0127  max mem: 8161
Epoch: [0]  [ 8840/60301]  eta: 7:12:37  lr: 0.000031  min_lr: 0.000001  loss: 5.5672 (5.6613)  loss_scale: 131072.0000 (103389.1515)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0014  max mem: 8161
Epoch: [0]  [ 8850/60301]  eta: 7:12:30  lr: 0.000031  min_lr: 0.000001  loss: 5.3992 (5.6608)  loss_scale: 131072.0000 (103420.4280)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0015  max mem: 8161
Epoch: [0]  [ 8860/60301]  eta: 7:12:24  lr: 0.000031  min_lr: 0.000001  loss: 5.2917 (5.6603)  loss_scale: 131072.0000 (103451.6339)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0012  max mem: 8161
Epoch: [0]  [ 8870/60301]  eta: 7:12:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2917 (5.6601)  loss_scale: 131072.0000 (103482.7695)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0012  max mem: 8161
Epoch: [0]  [ 8880/60301]  eta: 7:12:11  lr: 0.000031  min_lr: 0.000001  loss: 5.4694 (5.6598)  loss_scale: 131072.0000 (103513.8349)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0004  max mem: 8161
Epoch: [0]  [ 8890/60301]  eta: 7:12:15  lr: 0.000031  min_lr: 0.000001  loss: 5.4588 (5.6597)  loss_scale: 131072.0000 (103544.8305)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0896  max mem: 8161
Epoch: [0]  [ 8900/60301]  eta: 7:12:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3992 (5.6594)  loss_scale: 131072.0000 (103575.7564)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0896  max mem: 8161
Epoch: [0]  [ 8910/60301]  eta: 7:12:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4586 (5.6593)  loss_scale: 131072.0000 (103606.6130)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0012  max mem: 8161
Epoch: [0]  [ 8920/60301]  eta: 7:12:00  lr: 0.000031  min_lr: 0.000001  loss: 5.5151 (5.6589)  loss_scale: 131072.0000 (103637.4003)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0308  max mem: 8161
Epoch: [0]  [ 8930/60301]  eta: 7:11:53  lr: 0.000031  min_lr: 0.000001  loss: 5.4012 (5.6588)  loss_scale: 131072.0000 (103668.1187)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0300  max mem: 8161
[2023-07-14 01:34:42,073] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:34:42,073] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 8940/60301]  eta: 7:11:49  lr: 0.000031  min_lr: 0.000001  loss: 5.4384 (5.6585)  loss_scale: 131072.0000 (103845.3649)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0120  max mem: 8161
Epoch: [0]  [ 8950/60301]  eta: 7:11:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3384 (5.6582)  loss_scale: 262144.0000 (104022.2152)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0200  max mem: 8161
Epoch: [0]  [ 8960/60301]  eta: 7:11:37  lr: 0.000031  min_lr: 0.000001  loss: 5.2626 (5.6578)  loss_scale: 262144.0000 (104198.6707)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0085  max mem: 8161
Epoch: [0]  [ 8970/60301]  eta: 7:11:31  lr: 0.000031  min_lr: 0.000001  loss: 5.3618 (5.6578)  loss_scale: 262144.0000 (104374.7328)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.0089  max mem: 8161
[2023-07-14 01:35:04,505] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4488
[2023-07-14 01:35:04,505] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:35:04,505] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 8980/60301]  eta: 7:11:23  lr: 0.000031  min_lr: 0.000001  loss: 5.6509 (5.6577)  loss_scale: 262144.0000 (104492.0254)  weight_decay: 0.0500 (0.0500)  time: 0.4719  data: 0.0097  max mem: 8161
Epoch: [0]  [ 8990/60301]  eta: 7:11:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5646 (5.6574)  loss_scale: 131072.0000 (104521.5883)  weight_decay: 0.0500 (0.0500)  time: 0.4644  data: 0.0012  max mem: 8161
[2023-07-14 01:35:15,133] [INFO] [timer.py:181:stop] 0/9000, SamplesPerSec=9.356963772041576
Epoch: [0]  [ 9000/60301]  eta: 7:11:10  lr: 0.000031  min_lr: 0.000001  loss: 5.2314 (5.6570)  loss_scale: 131072.0000 (104551.0854)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9010/60301]  eta: 7:11:04  lr: 0.000031  min_lr: 0.000001  loss: 5.1748 (5.6566)  loss_scale: 131072.0000 (104580.5171)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0014  max mem: 8161
Epoch: [0]  [ 9020/60301]  eta: 7:11:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3697 (5.6564)  loss_scale: 131072.0000 (104609.8836)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1004  max mem: 8161
Epoch: [0]  [ 9030/60301]  eta: 7:11:03  lr: 0.000031  min_lr: 0.000001  loss: 5.5255 (5.6563)  loss_scale: 131072.0000 (104639.1850)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1004  max mem: 8161
Epoch: [0]  [ 9040/60301]  eta: 7:10:56  lr: 0.000031  min_lr: 0.000001  loss: 5.5707 (5.6562)  loss_scale: 131072.0000 (104668.4216)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9050/60301]  eta: 7:10:50  lr: 0.000031  min_lr: 0.000001  loss: 5.3587 (5.6558)  loss_scale: 131072.0000 (104697.5936)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0024  max mem: 8161
Epoch: [0]  [ 9060/60301]  eta: 7:10:44  lr: 0.000031  min_lr: 0.000001  loss: 5.3043 (5.6555)  loss_scale: 131072.0000 (104726.7012)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0024  max mem: 8161
Epoch: [0]  [ 9070/60301]  eta: 7:10:37  lr: 0.000031  min_lr: 0.000001  loss: 5.3306 (5.6551)  loss_scale: 131072.0000 (104755.7447)  weight_decay: 0.0500 (0.0500)  time: 0.4759  data: 0.0502  max mem: 8161
Epoch: [0]  [ 9080/60301]  eta: 7:10:20  lr: 0.000031  min_lr: 0.000001  loss: 5.3306 (5.6548)  loss_scale: 131072.0000 (104784.7241)  weight_decay: 0.0500 (0.0500)  time: 0.3776  data: 0.1381  max mem: 8161
Epoch: [0]  [ 9090/60301]  eta: 7:10:19  lr: 0.000031  min_lr: 0.000001  loss: 5.4852 (5.6546)  loss_scale: 131072.0000 (104813.6399)  weight_decay: 0.0500 (0.0500)  time: 0.4349  data: 0.3240  max mem: 8161
Epoch: [0]  [ 9100/60301]  eta: 7:10:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4852 (5.6544)  loss_scale: 131072.0000 (104842.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.4156  max mem: 8161
Epoch: [0]  [ 9110/60301]  eta: 7:09:59  lr: 0.000031  min_lr: 0.000001  loss: 5.5794 (5.6544)  loss_scale: 131072.0000 (104871.2809)  weight_decay: 0.0500 (0.0500)  time: 0.4196  data: 0.2719  max mem: 8161
Epoch: [0]  [ 9120/60301]  eta: 7:09:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4103 (5.6540)  loss_scale: 131072.0000 (104900.0066)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.1214  max mem: 8161
Epoch: [0]  [ 9130/60301]  eta: 7:10:12  lr: 0.000031  min_lr: 0.000001  loss: 5.3799 (5.6537)  loss_scale: 131072.0000 (104928.6694)  weight_decay: 0.0500 (0.0500)  time: 0.7048  data: 0.2274  max mem: 8161
Epoch: [0]  [ 9140/60301]  eta: 7:10:06  lr: 0.000031  min_lr: 0.000001  loss: 5.4278 (5.6535)  loss_scale: 131072.0000 (104957.2694)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.1984  max mem: 8161
Epoch: [0]  [ 9150/60301]  eta: 7:10:00  lr: 0.000031  min_lr: 0.000001  loss: 5.4126 (5.6532)  loss_scale: 131072.0000 (104985.8070)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0102  max mem: 8161
Epoch: [0]  [ 9160/60301]  eta: 7:09:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4185 (5.6529)  loss_scale: 131072.0000 (105014.2823)  weight_decay: 0.0500 (0.0500)  time: 0.4907  data: 0.0110  max mem: 8161
Epoch: [0]  [ 9170/60301]  eta: 7:09:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4320 (5.6527)  loss_scale: 131072.0000 (105042.6955)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9180/60301]  eta: 7:09:41  lr: 0.000031  min_lr: 0.000001  loss: 5.2442 (5.6523)  loss_scale: 131072.0000 (105071.0467)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0012  max mem: 8161
Epoch: [0]  [ 9190/60301]  eta: 7:09:35  lr: 0.000031  min_lr: 0.000001  loss: 5.3391 (5.6520)  loss_scale: 131072.0000 (105099.3363)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9200/60301]  eta: 7:09:28  lr: 0.000031  min_lr: 0.000001  loss: 5.4954 (5.6518)  loss_scale: 131072.0000 (105127.5644)  weight_decay: 0.0500 (0.0500)  time: 0.4794  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9210/60301]  eta: 7:09:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4110 (5.6516)  loss_scale: 131072.0000 (105155.7312)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9220/60301]  eta: 7:09:25  lr: 0.000031  min_lr: 0.000001  loss: 5.5063 (5.6516)  loss_scale: 131072.0000 (105183.8369)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0817  max mem: 8161
Epoch: [0]  [ 9230/60301]  eta: 7:09:18  lr: 0.000031  min_lr: 0.000001  loss: 5.5063 (5.6512)  loss_scale: 131072.0000 (105211.8817)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0809  max mem: 8161
[2023-07-14 01:37:15,408] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:37:15,408] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 9240/60301]  eta: 7:09:16  lr: 0.000031  min_lr: 0.000001  loss: 5.1976 (5.6509)  loss_scale: 131072.0000 (105324.9683)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0352  max mem: 8161
[2023-07-14 01:37:18,295] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4620
[2023-07-14 01:37:18,295] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:37:18,295] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 9250/60301]  eta: 7:09:01  lr: 0.000031  min_lr: 0.000001  loss: 5.2544 (5.6507)  loss_scale: 131072.0000 (105352.7999)  weight_decay: 0.0500 (0.0500)  time: 0.4393  data: 0.1058  max mem: 8161
Epoch: [0]  [ 9260/60301]  eta: 7:08:56  lr: 0.000031  min_lr: 0.000001  loss: 5.5856 (5.6505)  loss_scale: 131072.0000 (105380.5714)  weight_decay: 0.0500 (0.0500)  time: 0.4207  data: 0.2708  max mem: 8161
Epoch: [0]  [ 9270/60301]  eta: 7:08:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4966 (5.6501)  loss_scale: 131072.0000 (105408.2830)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.4397  max mem: 8161
Epoch: [0]  [ 9280/60301]  eta: 7:09:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4114 (5.6498)  loss_scale: 131072.0000 (105435.9349)  weight_decay: 0.0500 (0.0500)  time: 0.9021  data: 0.6069  max mem: 8161
Epoch: [0]  [ 9290/60301]  eta: 7:09:32  lr: 0.000031  min_lr: 0.000001  loss: 5.5281 (5.6497)  loss_scale: 131072.0000 (105463.5273)  weight_decay: 0.0500 (0.0500)  time: 0.9187  data: 0.4401  max mem: 8161
Epoch: [0]  [ 9300/60301]  eta: 7:09:25  lr: 0.000031  min_lr: 0.000001  loss: 5.4899 (5.6491)  loss_scale: 131072.0000 (105491.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0752  max mem: 8161
Epoch: [0]  [ 9310/60301]  eta: 7:09:19  lr: 0.000031  min_lr: 0.000001  loss: 5.4341 (5.6490)  loss_scale: 131072.0000 (105518.5342)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0035  max mem: 8161
Epoch: [0]  [ 9320/60301]  eta: 7:09:28  lr: 0.000031  min_lr: 0.000001  loss: 5.4341 (5.6485)  loss_scale: 131072.0000 (105545.9491)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1462  max mem: 8161
Epoch: [0]  [ 9330/60301]  eta: 7:09:22  lr: 0.000031  min_lr: 0.000001  loss: 5.2147 (5.6482)  loss_scale: 131072.0000 (105573.3053)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1488  max mem: 8161
Epoch: [0]  [ 9340/60301]  eta: 7:09:18  lr: 0.000031  min_lr: 0.000001  loss: 5.3100 (5.6479)  loss_scale: 131072.0000 (105600.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0217  max mem: 8161
Epoch: [0]  [ 9350/60301]  eta: 7:09:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3746 (5.6476)  loss_scale: 131072.0000 (105627.8422)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1028  max mem: 8161
Epoch: [0]  [ 9360/60301]  eta: 7:09:14  lr: 0.000031  min_lr: 0.000001  loss: 5.3746 (5.6474)  loss_scale: 131072.0000 (105655.0232)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0853  max mem: 8161
Epoch: [0]  [ 9370/60301]  eta: 7:09:23  lr: 0.000031  min_lr: 0.000001  loss: 5.3121 (5.6469)  loss_scale: 131072.0000 (105682.1462)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1399  max mem: 8161
Epoch: [0]  [ 9380/60301]  eta: 7:09:16  lr: 0.000031  min_lr: 0.000001  loss: 5.3250 (5.6466)  loss_scale: 131072.0000 (105709.2114)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1399  max mem: 8161
Epoch: [0]  [ 9390/60301]  eta: 7:09:10  lr: 0.000031  min_lr: 0.000001  loss: 5.4824 (5.6464)  loss_scale: 131072.0000 (105736.2189)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9400/60301]  eta: 7:09:04  lr: 0.000031  min_lr: 0.000001  loss: 5.5644 (5.6463)  loss_scale: 131072.0000 (105763.1690)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0012  max mem: 8161
Epoch: [0]  [ 9410/60301]  eta: 7:08:58  lr: 0.000031  min_lr: 0.000001  loss: 5.4165 (5.6460)  loss_scale: 131072.0000 (105790.0618)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0012  max mem: 8161
Epoch: [0]  [ 9420/60301]  eta: 7:08:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4434 (5.6459)  loss_scale: 131072.0000 (105816.8976)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0018  max mem: 8161
Epoch: [0]  [ 9430/60301]  eta: 7:08:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3959 (5.6455)  loss_scale: 131072.0000 (105843.6764)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0018  max mem: 8161
Epoch: [0]  [ 9440/60301]  eta: 7:08:39  lr: 0.000031  min_lr: 0.000001  loss: 5.2939 (5.6452)  loss_scale: 131072.0000 (105870.3985)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0014  max mem: 8161
Epoch: [0]  [ 9450/60301]  eta: 7:08:32  lr: 0.000031  min_lr: 0.000001  loss: 5.3464 (5.6450)  loss_scale: 131072.0000 (105897.0640)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0023  max mem: 8161
Epoch: [0]  [ 9460/60301]  eta: 7:08:26  lr: 0.000031  min_lr: 0.000001  loss: 5.3958 (5.6447)  loss_scale: 131072.0000 (105923.6732)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0014  max mem: 8161
Epoch: [0]  [ 9470/60301]  eta: 7:08:27  lr: 0.000031  min_lr: 0.000001  loss: 5.4911 (5.6446)  loss_scale: 131072.0000 (105950.2262)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0723  max mem: 8161
Epoch: [0]  [ 9480/60301]  eta: 7:08:21  lr: 0.000031  min_lr: 0.000001  loss: 5.4537 (5.6444)  loss_scale: 131072.0000 (105976.7231)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0723  max mem: 8161
Epoch: [0]  [ 9490/60301]  eta: 7:08:15  lr: 0.000031  min_lr: 0.000001  loss: 5.4327 (5.6441)  loss_scale: 131072.0000 (106003.1643)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0022  max mem: 8161
[2023-07-14 01:39:40,465] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:39:40,465] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [ 9500/60301]  eta: 7:08:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4646 (5.6440)  loss_scale: 131072.0000 (106057.1409)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0033  max mem: 8161
Epoch: [0]  [ 9510/60301]  eta: 7:08:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3817 (5.6435)  loss_scale: 262144.0000 (106221.2529)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0015  max mem: 8161
[2023-07-14 01:39:48,209] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4757
[2023-07-14 01:39:48,210] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:39:48,210] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [ 9520/60301]  eta: 7:07:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3005 (5.6433)  loss_scale: 262144.0000 (106302.4203)  weight_decay: 0.0500 (0.0500)  time: 0.4794  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9530/60301]  eta: 7:07:49  lr: 0.000031  min_lr: 0.000001  loss: 5.3005 (5.6429)  loss_scale: 131072.0000 (106328.4088)  weight_decay: 0.0500 (0.0500)  time: 0.4777  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9540/60301]  eta: 7:07:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3027 (5.6427)  loss_scale: 131072.0000 (106354.3427)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9550/60301]  eta: 7:07:37  lr: 0.000031  min_lr: 0.000001  loss: 5.2502 (5.6423)  loss_scale: 131072.0000 (106380.2224)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0022  max mem: 8161
Epoch: [0]  [ 9560/60301]  eta: 7:07:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4358 (5.6422)  loss_scale: 131072.0000 (106406.0479)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0014  max mem: 8161
Epoch: [0]  [ 9570/60301]  eta: 7:07:24  lr: 0.000031  min_lr: 0.000001  loss: 5.4846 (5.6419)  loss_scale: 131072.0000 (106431.8195)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9580/60301]  eta: 7:07:18  lr: 0.000031  min_lr: 0.000001  loss: 5.3278 (5.6416)  loss_scale: 131072.0000 (106457.5372)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9590/60301]  eta: 7:07:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3227 (5.6413)  loss_scale: 131072.0000 (106483.2013)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9600/60301]  eta: 7:07:05  lr: 0.000031  min_lr: 0.000001  loss: 5.2825 (5.6408)  loss_scale: 131072.0000 (106508.8120)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0013  max mem: 8161
Epoch: [0]  [ 9610/60301]  eta: 7:06:59  lr: 0.000031  min_lr: 0.000001  loss: 5.2825 (5.6405)  loss_scale: 131072.0000 (106534.3694)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0014  max mem: 8161
[2023-07-14 01:40:37,992] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4809
[2023-07-14 01:40:37,992] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-14 01:40:37,992] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [0]  [ 9620/60301]  eta: 7:06:50  lr: 0.000031  min_lr: 0.000001  loss: 5.3727 (5.6400)  loss_scale: 131072.0000 (106546.2501)  weight_decay: 0.0500 (0.0500)  time: 0.4620  data: 0.0015  max mem: 8161
Epoch: [0]  [ 9630/60301]  eta: 7:06:44  lr: 0.000031  min_lr: 0.000001  loss: 5.4163 (5.6399)  loss_scale: 65536.0000 (106503.6686)  weight_decay: 0.0500 (0.0500)  time: 0.4612  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9640/60301]  eta: 7:06:38  lr: 0.000031  min_lr: 0.000001  loss: 5.4768 (5.6399)  loss_scale: 65536.0000 (106461.1754)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9650/60301]  eta: 7:06:32  lr: 0.000031  min_lr: 0.000001  loss: 5.4706 (5.6397)  loss_scale: 65536.0000 (106418.7703)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9660/60301]  eta: 7:06:25  lr: 0.000031  min_lr: 0.000001  loss: 5.3837 (5.6393)  loss_scale: 65536.0000 (106376.4530)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9670/60301]  eta: 7:06:19  lr: 0.000031  min_lr: 0.000001  loss: 5.4318 (5.6393)  loss_scale: 65536.0000 (106334.2231)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9680/60301]  eta: 7:06:13  lr: 0.000031  min_lr: 0.000001  loss: 5.6256 (5.6391)  loss_scale: 65536.0000 (106292.0806)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9690/60301]  eta: 7:06:06  lr: 0.000031  min_lr: 0.000001  loss: 5.1948 (5.6387)  loss_scale: 65536.0000 (106250.0250)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9700/60301]  eta: 7:06:00  lr: 0.000031  min_lr: 0.000001  loss: 5.2343 (5.6385)  loss_scale: 65536.0000 (106208.0561)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0014  max mem: 8161
Epoch: [0]  [ 9710/60301]  eta: 7:06:10  lr: 0.000031  min_lr: 0.000001  loss: 5.3616 (5.6382)  loss_scale: 65536.0000 (106166.1736)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1523  max mem: 8161
Epoch: [0]  [ 9720/60301]  eta: 7:06:03  lr: 0.000031  min_lr: 0.000001  loss: 5.3695 (5.6379)  loss_scale: 65536.0000 (106124.3773)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1513  max mem: 8161
Epoch: [0]  [ 9730/60301]  eta: 7:06:07  lr: 0.000031  min_lr: 0.000001  loss: 5.2054 (5.6375)  loss_scale: 65536.0000 (106082.6669)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0959  max mem: 8161
Epoch: [0]  [ 9740/60301]  eta: 7:06:01  lr: 0.000031  min_lr: 0.000001  loss: 5.2216 (5.6372)  loss_scale: 65536.0000 (106041.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0994  max mem: 8161
Epoch: [0]  [ 9750/60301]  eta: 7:06:03  lr: 0.000031  min_lr: 0.000001  loss: 5.3365 (5.6369)  loss_scale: 65536.0000 (105999.5028)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0907  max mem: 8161
Epoch: [0]  [ 9760/60301]  eta: 7:05:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3365 (5.6366)  loss_scale: 65536.0000 (105958.0486)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0879  max mem: 8161
Epoch: [0]  [ 9770/60301]  eta: 7:05:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4723 (5.6364)  loss_scale: 65536.0000 (105916.6792)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0010  max mem: 8161
Epoch: [0]  [ 9780/60301]  eta: 7:05:44  lr: 0.000031  min_lr: 0.000001  loss: 5.4758 (5.6362)  loss_scale: 65536.0000 (105875.3943)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9790/60301]  eta: 7:05:38  lr: 0.000031  min_lr: 0.000001  loss: 5.5545 (5.6361)  loss_scale: 65536.0000 (105834.1939)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9800/60301]  eta: 7:05:32  lr: 0.000031  min_lr: 0.000001  loss: 5.5545 (5.6359)  loss_scale: 65536.0000 (105793.0774)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0019  max mem: 8161
Epoch: [0]  [ 9810/60301]  eta: 7:05:26  lr: 0.000031  min_lr: 0.000001  loss: 5.4561 (5.6355)  loss_scale: 65536.0000 (105752.0448)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0032  max mem: 8161
Epoch: [0]  [ 9820/60301]  eta: 7:05:19  lr: 0.000031  min_lr: 0.000001  loss: 5.2829 (5.6353)  loss_scale: 65536.0000 (105711.0958)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0026  max mem: 8161
Epoch: [0]  [ 9830/60301]  eta: 7:05:13  lr: 0.000031  min_lr: 0.000001  loss: 5.3172 (5.6351)  loss_scale: 65536.0000 (105670.2301)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0024  max mem: 8161
Epoch: [0]  [ 9840/60301]  eta: 7:05:07  lr: 0.000031  min_lr: 0.000001  loss: 5.3795 (5.6349)  loss_scale: 65536.0000 (105629.4474)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0015  max mem: 8161
Epoch: [0]  [ 9850/60301]  eta: 7:05:01  lr: 0.000031  min_lr: 0.000001  loss: 5.2883 (5.6345)  loss_scale: 65536.0000 (105588.7475)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9860/60301]  eta: 7:05:02  lr: 0.000031  min_lr: 0.000001  loss: 5.2067 (5.6342)  loss_scale: 65536.0000 (105548.1302)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0755  max mem: 8161
Epoch: [0]  [ 9870/60301]  eta: 7:04:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4059 (5.6338)  loss_scale: 65536.0000 (105507.5952)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0756  max mem: 8161
[2023-07-14 01:42:50,405] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:42:50,405] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [0]  [ 9880/60301]  eta: 7:04:49  lr: 0.000031  min_lr: 0.000001  loss: 5.4059 (5.6336)  loss_scale: 65536.0000 (105493.6723)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0005  max mem: 8161
Epoch: [0]  [ 9890/60301]  eta: 7:04:36  lr: 0.000031  min_lr: 0.000001  loss: 5.4080 (5.6333)  loss_scale: 131072.0000 (105519.5325)  weight_decay: 0.0500 (0.0500)  time: 0.4158  data: 0.0004  max mem: 8161
Epoch: [0]  [ 9900/60301]  eta: 7:04:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4284 (5.6332)  loss_scale: 131072.0000 (105545.3405)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.2532  max mem: 8161
Epoch: [0]  [ 9910/60301]  eta: 7:04:33  lr: 0.000031  min_lr: 0.000001  loss: 5.2626 (5.6327)  loss_scale: 131072.0000 (105571.0964)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.4603  max mem: 8161
Epoch: [0]  [ 9920/60301]  eta: 7:04:23  lr: 0.000031  min_lr: 0.000001  loss: 5.4826 (5.6326)  loss_scale: 131072.0000 (105596.8003)  weight_decay: 0.0500 (0.0500)  time: 0.4665  data: 0.3541  max mem: 8161
Epoch: [0]  [ 9930/60301]  eta: 7:04:19  lr: 0.000031  min_lr: 0.000001  loss: 5.4826 (5.6323)  loss_scale: 131072.0000 (105622.4525)  weight_decay: 0.0500 (0.0500)  time: 0.4617  data: 0.2238  max mem: 8161
Epoch: [0]  [ 9940/60301]  eta: 7:04:14  lr: 0.000031  min_lr: 0.000001  loss: 5.2884 (5.6320)  loss_scale: 131072.0000 (105648.0531)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0902  max mem: 8161
Epoch: [0]  [ 9950/60301]  eta: 7:04:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3764 (5.6318)  loss_scale: 131072.0000 (105673.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0233  max mem: 8161
Epoch: [0]  [ 9960/60301]  eta: 7:04:25  lr: 0.000031  min_lr: 0.000001  loss: 5.3468 (5.6315)  loss_scale: 131072.0000 (105699.1001)  weight_decay: 0.0500 (0.0500)  time: 0.7164  data: 0.2350  max mem: 8161
Epoch: [0]  [ 9970/60301]  eta: 7:04:19  lr: 0.000031  min_lr: 0.000001  loss: 5.2575 (5.6311)  loss_scale: 131072.0000 (105724.5468)  weight_decay: 0.0500 (0.0500)  time: 0.7066  data: 0.2257  max mem: 8161
Epoch: [0]  [ 9980/60301]  eta: 7:04:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4052 (5.6311)  loss_scale: 131072.0000 (105749.9425)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0009  max mem: 8161
Epoch: [0]  [ 9990/60301]  eta: 7:04:06  lr: 0.000031  min_lr: 0.000001  loss: 5.4052 (5.6308)  loss_scale: 131072.0000 (105775.2874)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0004  max mem: 8161
[2023-07-14 01:43:54,436] [INFO] [logging.py:69:log_dist] [Rank 0] step=5000, skipped=26, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 01:43:54,440] [INFO] [timer.py:181:stop] 0/10000, SamplesPerSec=9.34715981387857
Epoch: [0]  [10000/60301]  eta: 7:04:00  lr: 0.000031  min_lr: 0.000001  loss: 5.3588 (5.6306)  loss_scale: 131072.0000 (105800.5815)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0004  max mem: 8161
Epoch: [0]  [10010/60301]  eta: 7:03:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2793 (5.6304)  loss_scale: 131072.0000 (105825.8252)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0004  max mem: 8161
Epoch: [0]  [10020/60301]  eta: 7:03:48  lr: 0.000031  min_lr: 0.000001  loss: 5.5556 (5.6305)  loss_scale: 131072.0000 (105851.0185)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0004  max mem: 8161
Epoch: [0]  [10030/60301]  eta: 7:03:41  lr: 0.000031  min_lr: 0.000001  loss: 5.6708 (5.6302)  loss_scale: 131072.0000 (105876.1615)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0013  max mem: 8161
Epoch: [0]  [10040/60301]  eta: 7:03:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4329 (5.6301)  loss_scale: 131072.0000 (105901.2545)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0013  max mem: 8161
Epoch: [0]  [10050/60301]  eta: 7:03:29  lr: 0.000031  min_lr: 0.000001  loss: 5.3952 (5.6297)  loss_scale: 131072.0000 (105926.2975)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0013  max mem: 8161
Epoch: [0]  [10060/60301]  eta: 7:03:23  lr: 0.000031  min_lr: 0.000001  loss: 5.5169 (5.6297)  loss_scale: 131072.0000 (105951.2907)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0013  max mem: 8161
Epoch: [0]  [10070/60301]  eta: 7:03:17  lr: 0.000031  min_lr: 0.000001  loss: 5.6256 (5.6296)  loss_scale: 131072.0000 (105976.2343)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0005  max mem: 8161
Epoch: [0]  [10080/60301]  eta: 7:03:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4518 (5.6293)  loss_scale: 131072.0000 (106001.1285)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0140  max mem: 8161
Epoch: [0]  [10090/60301]  eta: 7:03:08  lr: 0.000031  min_lr: 0.000001  loss: 5.3408 (5.6290)  loss_scale: 131072.0000 (106025.9732)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0398  max mem: 8161
Epoch: [0]  [10100/60301]  eta: 7:03:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3408 (5.6287)  loss_scale: 131072.0000 (106050.7688)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0262  max mem: 8161
Epoch: [0]  [10110/60301]  eta: 7:02:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3560 (5.6284)  loss_scale: 131072.0000 (106075.5154)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0004  max mem: 8161
Epoch: [0]  [10120/60301]  eta: 7:02:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4069 (5.6283)  loss_scale: 131072.0000 (106100.2130)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0004  max mem: 8161
Epoch: [0]  [10130/60301]  eta: 7:02:44  lr: 0.000031  min_lr: 0.000001  loss: 5.4267 (5.6280)  loss_scale: 131072.0000 (106124.8619)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0005  max mem: 8161
[2023-07-14 01:44:59,946] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:44:59,946] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [10140/60301]  eta: 7:02:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4051 (5.6277)  loss_scale: 131072.0000 (106252.8618)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0013  max mem: 8161
[2023-07-14 01:45:03,824] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5070
[2023-07-14 01:45:03,824] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:45:03,824] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [10150/60301]  eta: 7:02:31  lr: 0.000031  min_lr: 0.000001  loss: 5.3203 (5.6274)  loss_scale: 131072.0000 (106277.3118)  weight_decay: 0.0500 (0.0500)  time: 0.4784  data: 0.0013  max mem: 8161
Epoch: [0]  [10160/60301]  eta: 7:02:25  lr: 0.000031  min_lr: 0.000001  loss: 5.2670 (5.6271)  loss_scale: 131072.0000 (106301.7136)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0014  max mem: 8161
Epoch: [0]  [10170/60301]  eta: 7:02:19  lr: 0.000031  min_lr: 0.000001  loss: 5.2851 (5.6269)  loss_scale: 131072.0000 (106326.0674)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0024  max mem: 8161
Epoch: [0]  [10180/60301]  eta: 7:02:12  lr: 0.000031  min_lr: 0.000001  loss: 5.4063 (5.6267)  loss_scale: 131072.0000 (106350.3734)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0021  max mem: 8161
Epoch: [0]  [10190/60301]  eta: 7:02:07  lr: 0.000031  min_lr: 0.000001  loss: 5.4514 (5.6265)  loss_scale: 131072.0000 (106374.6317)  weight_decay: 0.0500 (0.0500)  time: 0.4916  data: 0.0113  max mem: 8161
Epoch: [0]  [10200/60301]  eta: 7:02:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4934 (5.6262)  loss_scale: 131072.0000 (106398.8425)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0835  max mem: 8161
Epoch: [0]  [10210/60301]  eta: 7:02:02  lr: 0.000031  min_lr: 0.000001  loss: 5.4279 (5.6261)  loss_scale: 131072.0000 (106423.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0770  max mem: 8161
Epoch: [0]  [10220/60301]  eta: 7:01:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4056 (5.6259)  loss_scale: 131072.0000 (106447.1218)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0041  max mem: 8161
Epoch: [0]  [10230/60301]  eta: 7:01:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4357 (5.6256)  loss_scale: 131072.0000 (106471.1907)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0013  max mem: 8161
Epoch: [0]  [10240/60301]  eta: 7:01:33  lr: 0.000031  min_lr: 0.000001  loss: 5.3778 (5.6253)  loss_scale: 131072.0000 (106495.2126)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0707  max mem: 8161
Epoch: [0]  [10250/60301]  eta: 7:01:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3843 (5.6253)  loss_scale: 131072.0000 (106519.1876)  weight_decay: 0.0500 (0.0500)  time: 0.3128  data: 0.1910  max mem: 8161
Epoch: [0]  [10260/60301]  eta: 7:01:14  lr: 0.000031  min_lr: 0.000001  loss: 5.5501 (5.6251)  loss_scale: 131072.0000 (106543.1159)  weight_decay: 0.0500 (0.0500)  time: 0.4103  data: 0.2959  max mem: 8161
Epoch: [0]  [10270/60301]  eta: 7:01:06  lr: 0.000031  min_lr: 0.000001  loss: 5.4634 (5.6250)  loss_scale: 131072.0000 (106566.9976)  weight_decay: 0.0500 (0.0500)  time: 0.4558  data: 0.3404  max mem: 8161
Epoch: [0]  [10280/60301]  eta: 7:01:11  lr: 0.000031  min_lr: 0.000001  loss: 5.2792 (5.6245)  loss_scale: 131072.0000 (106590.8328)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.3087  max mem: 8161
Epoch: [0]  [10290/60301]  eta: 7:01:05  lr: 0.000031  min_lr: 0.000001  loss: 5.1059 (5.6241)  loss_scale: 131072.0000 (106614.6217)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1495  max mem: 8161
Epoch: [0]  [10300/60301]  eta: 7:00:59  lr: 0.000031  min_lr: 0.000001  loss: 5.1280 (5.6237)  loss_scale: 131072.0000 (106638.3644)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0077  max mem: 8161
Epoch: [0]  [10310/60301]  eta: 7:00:53  lr: 0.000031  min_lr: 0.000001  loss: 5.3352 (5.6234)  loss_scale: 131072.0000 (106662.0611)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0021  max mem: 8161
Epoch: [0]  [10320/60301]  eta: 7:00:46  lr: 0.000031  min_lr: 0.000001  loss: 5.3695 (5.6232)  loss_scale: 131072.0000 (106685.7118)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0012  max mem: 8161
Epoch: [0]  [10330/60301]  eta: 7:00:42  lr: 0.000031  min_lr: 0.000001  loss: 5.4092 (5.6231)  loss_scale: 131072.0000 (106709.3168)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0212  max mem: 8161
Epoch: [0]  [10340/60301]  eta: 7:00:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3824 (5.6228)  loss_scale: 131072.0000 (106732.8761)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0212  max mem: 8161
Epoch: [0]  [10350/60301]  eta: 7:00:29  lr: 0.000031  min_lr: 0.000001  loss: 5.3327 (5.6226)  loss_scale: 131072.0000 (106756.3899)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0005  max mem: 8161
Epoch: [0]  [10360/60301]  eta: 7:00:23  lr: 0.000031  min_lr: 0.000001  loss: 5.3821 (5.6223)  loss_scale: 131072.0000 (106779.8583)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0004  max mem: 8161
Epoch: [0]  [10370/60301]  eta: 7:00:17  lr: 0.000031  min_lr: 0.000001  loss: 5.3949 (5.6219)  loss_scale: 131072.0000 (106803.2815)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0005  max mem: 8161
Epoch: [0]  [10380/60301]  eta: 7:00:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3754 (5.6217)  loss_scale: 131072.0000 (106826.6595)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0005  max mem: 8161
Epoch: [0]  [10390/60301]  eta: 7:00:05  lr: 0.000031  min_lr: 0.000001  loss: 5.2505 (5.6215)  loss_scale: 131072.0000 (106849.9925)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0013  max mem: 8161
[2023-07-14 01:47:08,622] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:47:08,623] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [10400/60301]  eta: 6:59:58  lr: 0.000031  min_lr: 0.000001  loss: 5.2505 (5.6211)  loss_scale: 131072.0000 (106898.4844)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0018  max mem: 8161
Epoch: [0]  [10410/60301]  eta: 6:59:52  lr: 0.000031  min_lr: 0.000001  loss: 5.2557 (5.6208)  loss_scale: 262144.0000 (107047.6012)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.0010  max mem: 8161
[2023-07-14 01:47:15,991] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5207
[2023-07-14 01:47:15,991] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:47:15,991] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [10420/60301]  eta: 6:59:44  lr: 0.000031  min_lr: 0.000001  loss: 5.3063 (5.6205)  loss_scale: 262144.0000 (107120.9657)  weight_decay: 0.0500 (0.0500)  time: 0.4618  data: 0.0023  max mem: 8161
Epoch: [0]  [10430/60301]  eta: 6:59:38  lr: 0.000031  min_lr: 0.000001  loss: 5.3063 (5.6202)  loss_scale: 131072.0000 (107143.9271)  weight_decay: 0.0500 (0.0500)  time: 0.4627  data: 0.0031  max mem: 8161
Epoch: [0]  [10440/60301]  eta: 6:59:32  lr: 0.000031  min_lr: 0.000001  loss: 5.2474 (5.6199)  loss_scale: 131072.0000 (107166.8446)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0021  max mem: 8161
Epoch: [0]  [10450/60301]  eta: 6:59:15  lr: 0.000031  min_lr: 0.000001  loss: 5.2163 (5.6196)  loss_scale: 131072.0000 (107189.7181)  weight_decay: 0.0500 (0.0500)  time: 0.3723  data: 0.0714  max mem: 8161
Epoch: [0]  [10460/60301]  eta: 6:59:06  lr: 0.000031  min_lr: 0.000001  loss: 5.2012 (5.6192)  loss_scale: 131072.0000 (107212.5479)  weight_decay: 0.0500 (0.0500)  time: 0.3416  data: 0.2267  max mem: 8161
Epoch: [0]  [10470/60301]  eta: 6:58:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3653 (5.6191)  loss_scale: 131072.0000 (107235.3342)  weight_decay: 0.0500 (0.0500)  time: 0.4191  data: 0.3063  max mem: 8161
Epoch: [0]  [10480/60301]  eta: 6:59:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4743 (5.6189)  loss_scale: 131072.0000 (107258.0769)  weight_decay: 0.0500 (0.0500)  time: 0.6868  data: 0.5354  max mem: 8161
Epoch: [0]  [10490/60301]  eta: 6:59:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3944 (5.6188)  loss_scale: 131072.0000 (107280.7763)  weight_decay: 0.0500 (0.0500)  time: 0.7566  data: 0.4234  max mem: 8161
Epoch: [0]  [10500/60301]  eta: 6:59:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3356 (5.6186)  loss_scale: 131072.0000 (107303.4324)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0844  max mem: 8161
Epoch: [0]  [10510/60301]  eta: 6:59:03  lr: 0.000031  min_lr: 0.000001  loss: 5.3130 (5.6182)  loss_scale: 131072.0000 (107326.0455)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0482  max mem: 8161
Epoch: [0]  [10520/60301]  eta: 6:58:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3467 (5.6180)  loss_scale: 131072.0000 (107348.6155)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0030  max mem: 8161
Epoch: [0]  [10530/60301]  eta: 6:58:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4392 (5.6178)  loss_scale: 131072.0000 (107371.1427)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0023  max mem: 8161
Epoch: [0]  [10540/60301]  eta: 6:58:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4392 (5.6176)  loss_scale: 131072.0000 (107393.6272)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0323  max mem: 8161
Epoch: [0]  [10550/60301]  eta: 6:58:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3352 (5.6174)  loss_scale: 131072.0000 (107416.0690)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0462  max mem: 8161
Epoch: [0]  [10560/60301]  eta: 6:58:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3746 (5.6172)  loss_scale: 131072.0000 (107438.4683)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0163  max mem: 8161
Epoch: [0]  [10570/60301]  eta: 6:58:32  lr: 0.000031  min_lr: 0.000001  loss: 5.2510 (5.6167)  loss_scale: 131072.0000 (107460.8253)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0255  max mem: 8161
Epoch: [0]  [10580/60301]  eta: 6:58:33  lr: 0.000031  min_lr: 0.000001  loss: 5.1841 (5.6164)  loss_scale: 131072.0000 (107483.1400)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0994  max mem: 8161
Epoch: [0]  [10590/60301]  eta: 6:58:27  lr: 0.000031  min_lr: 0.000001  loss: 5.1841 (5.6161)  loss_scale: 131072.0000 (107505.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0754  max mem: 8161
Epoch: [0]  [10600/60301]  eta: 6:58:26  lr: 0.000031  min_lr: 0.000001  loss: 5.0770 (5.6157)  loss_scale: 131072.0000 (107527.6431)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0583  max mem: 8161
Epoch: [0]  [10610/60301]  eta: 6:58:20  lr: 0.000031  min_lr: 0.000001  loss: 5.1840 (5.6155)  loss_scale: 131072.0000 (107549.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0582  max mem: 8161
Epoch: [0]  [10620/60301]  eta: 6:58:14  lr: 0.000031  min_lr: 0.000001  loss: 5.2717 (5.6150)  loss_scale: 131072.0000 (107571.9785)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0005  max mem: 8161
Epoch: [0]  [10630/60301]  eta: 6:58:09  lr: 0.000031  min_lr: 0.000001  loss: 5.2349 (5.6147)  loss_scale: 131072.0000 (107594.0837)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0146  max mem: 8161
Epoch: [0]  [10640/60301]  eta: 6:58:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4157 (5.6145)  loss_scale: 131072.0000 (107616.1474)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0154  max mem: 8161
Epoch: [0]  [10650/60301]  eta: 6:57:57  lr: 0.000031  min_lr: 0.000001  loss: 5.3589 (5.6143)  loss_scale: 131072.0000 (107638.1696)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0013  max mem: 8161
Epoch: [0]  [10660/60301]  eta: 6:57:55  lr: 0.000031  min_lr: 0.000001  loss: 5.3589 (5.6141)  loss_scale: 131072.0000 (107660.1505)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0468  max mem: 8161
Epoch: [0]  [10670/60301]  eta: 6:57:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3613 (5.6139)  loss_scale: 131072.0000 (107682.0902)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1278  max mem: 8161
[2023-07-14 01:49:30,004] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:49:30,005] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [10680/60301]  eta: 6:57:50  lr: 0.000031  min_lr: 0.000001  loss: 5.3342 (5.6135)  loss_scale: 131072.0000 (107802.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0815  max mem: 8161
Epoch: [0]  [10690/60301]  eta: 6:57:44  lr: 0.000031  min_lr: 0.000001  loss: 5.3993 (5.6134)  loss_scale: 262144.0000 (107946.5270)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0004  max mem: 8161
Epoch: [0]  [10700/60301]  eta: 6:57:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4325 (5.6133)  loss_scale: 262144.0000 (108090.6233)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0005  max mem: 8161
Epoch: [0]  [10710/60301]  eta: 6:57:31  lr: 0.000031  min_lr: 0.000001  loss: 5.4325 (5.6132)  loss_scale: 262144.0000 (108234.4506)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0005  max mem: 8161
[2023-07-14 01:49:50,032] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5358
[2023-07-14 01:49:50,032] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:49:50,032] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [10720/60301]  eta: 6:57:15  lr: 0.000031  min_lr: 0.000001  loss: 5.3132 (5.6128)  loss_scale: 262144.0000 (108329.1066)  weight_decay: 0.0500 (0.0500)  time: 0.3759  data: 0.0032  max mem: 8161
Epoch: [0]  [10730/60301]  eta: 6:57:08  lr: 0.000031  min_lr: 0.000001  loss: 5.1248 (5.6123)  loss_scale: 131072.0000 (108350.3003)  weight_decay: 0.0500 (0.0500)  time: 0.3588  data: 0.1697  max mem: 8161
Epoch: [0]  [10740/60301]  eta: 6:56:59  lr: 0.000031  min_lr: 0.000001  loss: 5.1248 (5.6120)  loss_scale: 131072.0000 (108371.4544)  weight_decay: 0.0500 (0.0500)  time: 0.4333  data: 0.3208  max mem: 8161
Epoch: [0]  [10750/60301]  eta: 6:57:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4613 (5.6120)  loss_scale: 131072.0000 (108392.5692)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.4490  max mem: 8161
Epoch: [0]  [10760/60301]  eta: 6:56:59  lr: 0.000031  min_lr: 0.000001  loss: 5.4613 (5.6117)  loss_scale: 131072.0000 (108413.6448)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.3851  max mem: 8161
Epoch: [0]  [10770/60301]  eta: 6:56:53  lr: 0.000031  min_lr: 0.000001  loss: 5.1853 (5.6113)  loss_scale: 131072.0000 (108434.6813)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0921  max mem: 8161
Epoch: [0]  [10780/60301]  eta: 6:56:48  lr: 0.000031  min_lr: 0.000001  loss: 5.1862 (5.6110)  loss_scale: 131072.0000 (108455.6787)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0190  max mem: 8161
Epoch: [0]  [10790/60301]  eta: 6:56:42  lr: 0.000031  min_lr: 0.000001  loss: 5.2598 (5.6109)  loss_scale: 131072.0000 (108476.6372)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0189  max mem: 8161
Epoch: [0]  [10800/60301]  eta: 6:56:36  lr: 0.000031  min_lr: 0.000001  loss: 5.4577 (5.6107)  loss_scale: 131072.0000 (108497.5569)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0039  max mem: 8161
Epoch: [0]  [10810/60301]  eta: 6:56:31  lr: 0.000031  min_lr: 0.000001  loss: 5.3835 (5.6104)  loss_scale: 131072.0000 (108518.4379)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0135  max mem: 8161
Epoch: [0]  [10820/60301]  eta: 6:56:26  lr: 0.000031  min_lr: 0.000001  loss: 5.2480 (5.6101)  loss_scale: 131072.0000 (108539.2803)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0261  max mem: 8161
Epoch: [0]  [10830/60301]  eta: 6:56:20  lr: 0.000031  min_lr: 0.000001  loss: 5.2076 (5.6096)  loss_scale: 131072.0000 (108560.0842)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0160  max mem: 8161
Epoch: [0]  [10840/60301]  eta: 6:56:14  lr: 0.000031  min_lr: 0.000001  loss: 5.3126 (5.6095)  loss_scale: 131072.0000 (108580.8497)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0015  max mem: 8161
Epoch: [0]  [10850/60301]  eta: 6:56:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3995 (5.6094)  loss_scale: 131072.0000 (108601.5770)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0365  max mem: 8161
Epoch: [0]  [10860/60301]  eta: 6:56:05  lr: 0.000031  min_lr: 0.000001  loss: 5.4565 (5.6092)  loss_scale: 131072.0000 (108622.2661)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0428  max mem: 8161
Epoch: [0]  [10870/60301]  eta: 6:56:00  lr: 0.000031  min_lr: 0.000001  loss: 5.4565 (5.6091)  loss_scale: 131072.0000 (108642.9171)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0124  max mem: 8161
Epoch: [0]  [10880/60301]  eta: 6:55:53  lr: 0.000031  min_lr: 0.000001  loss: 5.4636 (5.6089)  loss_scale: 131072.0000 (108663.5302)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0061  max mem: 8161
Epoch: [0]  [10890/60301]  eta: 6:55:51  lr: 0.000031  min_lr: 0.000001  loss: 5.4651 (5.6087)  loss_scale: 131072.0000 (108684.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0404  max mem: 8161
Epoch: [0]  [10900/60301]  eta: 6:55:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3510 (5.6084)  loss_scale: 131072.0000 (108704.6429)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0403  max mem: 8161
Epoch: [0]  [10910/60301]  eta: 6:55:39  lr: 0.000031  min_lr: 0.000001  loss: 5.3582 (5.6083)  loss_scale: 131072.0000 (108725.1427)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0004  max mem: 8161
Epoch: [0]  [10920/60301]  eta: 6:55:32  lr: 0.000031  min_lr: 0.000001  loss: 5.2395 (5.6079)  loss_scale: 131072.0000 (108745.6050)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0005  max mem: 8161
Epoch: [0]  [10930/60301]  eta: 6:55:26  lr: 0.000031  min_lr: 0.000001  loss: 5.2015 (5.6076)  loss_scale: 131072.0000 (108766.0298)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0005  max mem: 8161
Epoch: [0]  [10940/60301]  eta: 6:55:20  lr: 0.000031  min_lr: 0.000001  loss: 5.1776 (5.6073)  loss_scale: 131072.0000 (108786.4173)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0023  max mem: 8161
Epoch: [0]  [10950/60301]  eta: 6:55:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4219 (5.6073)  loss_scale: 131072.0000 (108806.7676)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0023  max mem: 8161
Epoch: [0]  [10960/60301]  eta: 6:55:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4457 (5.6070)  loss_scale: 131072.0000 (108827.0807)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0014  max mem: 8161
Epoch: [0]  [10970/60301]  eta: 6:55:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3428 (5.6068)  loss_scale: 131072.0000 (108847.3568)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0024  max mem: 8161
[2023-07-14 01:51:57,667] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:51:57,667] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [10980/60301]  eta: 6:54:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4886 (5.6067)  loss_scale: 131072.0000 (108939.2136)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0023  max mem: 8161
Epoch: [0]  [10990/60301]  eta: 6:54:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4125 (5.6063)  loss_scale: 262144.0000 (109078.6047)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0014  max mem: 8161
[2023-07-14 01:52:09,244] [INFO] [timer.py:181:stop] 0/11000, SamplesPerSec=9.348538766167552
Epoch: [0]  [11000/60301]  eta: 6:54:43  lr: 0.000031  min_lr: 0.000001  loss: 5.2676 (5.6061)  loss_scale: 262144.0000 (109217.7424)  weight_decay: 0.0500 (0.0500)  time: 0.4788  data: 0.0005  max mem: 8161
Epoch: [0]  [11010/60301]  eta: 6:54:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4653 (5.6061)  loss_scale: 262144.0000 (109356.6274)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0005  max mem: 8161
[2023-07-14 01:52:18,608] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5509
[2023-07-14 01:52:18,609] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:52:18,609] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [11020/60301]  eta: 6:54:30  lr: 0.000031  min_lr: 0.000001  loss: 5.5512 (5.6059)  loss_scale: 262144.0000 (109471.4745)  weight_decay: 0.0500 (0.0500)  time: 0.4678  data: 0.0005  max mem: 8161
Epoch: [0]  [11030/60301]  eta: 6:54:24  lr: 0.000031  min_lr: 0.000001  loss: 5.3710 (5.6056)  loss_scale: 131072.0000 (109491.0561)  weight_decay: 0.0500 (0.0500)  time: 0.4662  data: 0.0014  max mem: 8161
Epoch: [0]  [11040/60301]  eta: 6:54:18  lr: 0.000031  min_lr: 0.000001  loss: 5.3013 (5.6055)  loss_scale: 131072.0000 (109510.6023)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0014  max mem: 8161
Epoch: [0]  [11050/60301]  eta: 6:54:12  lr: 0.000031  min_lr: 0.000001  loss: 5.3381 (5.6052)  loss_scale: 131072.0000 (109530.1131)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0014  max mem: 8161
Epoch: [0]  [11060/60301]  eta: 6:53:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3381 (5.6051)  loss_scale: 131072.0000 (109549.5886)  weight_decay: 0.0500 (0.0500)  time: 0.3734  data: 0.0585  max mem: 8161
Epoch: [0]  [11070/60301]  eta: 6:53:49  lr: 0.000031  min_lr: 0.000001  loss: 5.5290 (5.6050)  loss_scale: 131072.0000 (109569.0290)  weight_decay: 0.0500 (0.0500)  time: 0.3589  data: 0.2282  max mem: 8161
Epoch: [0]  [11080/60301]  eta: 6:53:43  lr: 0.000031  min_lr: 0.000001  loss: 5.3435 (5.6046)  loss_scale: 131072.0000 (109588.4343)  weight_decay: 0.0500 (0.0500)  time: 0.4747  data: 0.3618  max mem: 8161
Epoch: [0]  [11090/60301]  eta: 6:53:35  lr: 0.000031  min_lr: 0.000001  loss: 5.1853 (5.6043)  loss_scale: 131072.0000 (109607.8045)  weight_decay: 0.0500 (0.0500)  time: 0.4645  data: 0.3515  max mem: 8161
Epoch: [0]  [11100/60301]  eta: 6:53:44  lr: 0.000031  min_lr: 0.000001  loss: 5.1311 (5.6040)  loss_scale: 131072.0000 (109627.1399)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.3284  max mem: 8161
Epoch: [0]  [11110/60301]  eta: 6:53:38  lr: 0.000031  min_lr: 0.000001  loss: 5.1671 (5.6037)  loss_scale: 131072.0000 (109646.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1782  max mem: 8161
Epoch: [0]  [11120/60301]  eta: 6:53:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3321 (5.6034)  loss_scale: 131072.0000 (109665.7063)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0483  max mem: 8161
Epoch: [0]  [11130/60301]  eta: 6:53:29  lr: 0.000031  min_lr: 0.000001  loss: 5.3729 (5.6032)  loss_scale: 131072.0000 (109684.9376)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0390  max mem: 8161
Epoch: [0]  [11140/60301]  eta: 6:53:23  lr: 0.000031  min_lr: 0.000001  loss: 5.4453 (5.6030)  loss_scale: 131072.0000 (109704.1343)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0024  max mem: 8161
Epoch: [0]  [11150/60301]  eta: 6:53:26  lr: 0.000031  min_lr: 0.000001  loss: 5.4326 (5.6028)  loss_scale: 131072.0000 (109723.2966)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1013  max mem: 8161
Epoch: [0]  [11160/60301]  eta: 6:53:20  lr: 0.000031  min_lr: 0.000001  loss: 5.4398 (5.6026)  loss_scale: 131072.0000 (109742.4245)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1026  max mem: 8161
Epoch: [0]  [11170/60301]  eta: 6:53:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4303 (5.6024)  loss_scale: 131072.0000 (109761.5182)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0030  max mem: 8161
Epoch: [0]  [11180/60301]  eta: 6:53:08  lr: 0.000031  min_lr: 0.000001  loss: 5.4536 (5.6023)  loss_scale: 131072.0000 (109780.5778)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.0005  max mem: 8161
Epoch: [0]  [11190/60301]  eta: 6:53:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3524 (5.6020)  loss_scale: 131072.0000 (109799.6033)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0004  max mem: 8161
Epoch: [0]  [11200/60301]  eta: 6:52:55  lr: 0.000031  min_lr: 0.000001  loss: 5.3524 (5.6018)  loss_scale: 131072.0000 (109818.5948)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0004  max mem: 8161
Epoch: [0]  [11210/60301]  eta: 6:52:49  lr: 0.000031  min_lr: 0.000001  loss: 5.2125 (5.6015)  loss_scale: 131072.0000 (109837.5524)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.0004  max mem: 8161
Epoch: [0]  [11220/60301]  eta: 6:52:43  lr: 0.000031  min_lr: 0.000001  loss: 5.2060 (5.6013)  loss_scale: 131072.0000 (109856.4762)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0004  max mem: 8161
Epoch: [0]  [11230/60301]  eta: 6:52:37  lr: 0.000031  min_lr: 0.000001  loss: 5.4071 (5.6012)  loss_scale: 131072.0000 (109875.3664)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0004  max mem: 8161
Epoch: [0]  [11240/60301]  eta: 6:52:31  lr: 0.000031  min_lr: 0.000001  loss: 5.4456 (5.6009)  loss_scale: 131072.0000 (109894.2229)  weight_decay: 0.0500 (0.0500)  time: 0.4788  data: 0.0004  max mem: 8161
Epoch: [0]  [11250/60301]  eta: 6:52:25  lr: 0.000031  min_lr: 0.000001  loss: 5.1642 (5.6006)  loss_scale: 131072.0000 (109913.0460)  weight_decay: 0.0500 (0.0500)  time: 0.4791  data: 0.0004  max mem: 8161
Epoch: [0]  [11260/60301]  eta: 6:52:19  lr: 0.000031  min_lr: 0.000001  loss: 5.3981 (5.6005)  loss_scale: 131072.0000 (109931.8355)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0014  max mem: 8161
Epoch: [0]  [11270/60301]  eta: 6:52:36  lr: 0.000031  min_lr: 0.000001  loss: 5.4030 (5.6001)  loss_scale: 131072.0000 (109950.5918)  weight_decay: 0.0500 (0.0500)  time: 0.7451  data: 0.2666  max mem: 8161
[2023-07-14 01:54:31,989] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:54:31,989] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [11280/60301]  eta: 6:52:32  lr: 0.000031  min_lr: 0.000001  loss: 5.2347 (5.5999)  loss_scale: 131072.0000 (110015.7901)  weight_decay: 0.0500 (0.0500)  time: 0.7769  data: 0.2992  max mem: 8161
[2023-07-14 01:54:37,023] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5643
[2023-07-14 01:54:37,023] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 01:54:37,024] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [11290/60301]  eta: 6:52:29  lr: 0.000031  min_lr: 0.000001  loss: 5.2468 (5.5997)  loss_scale: 131072.0000 (110104.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0706  max mem: 8161
Epoch: [0]  [11300/60301]  eta: 6:52:26  lr: 0.000031  min_lr: 0.000001  loss: 5.2375 (5.5993)  loss_scale: 131072.0000 (110122.6440)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0718  max mem: 8161
Epoch: [0]  [11310/60301]  eta: 6:52:20  lr: 0.000031  min_lr: 0.000001  loss: 5.2929 (5.5990)  loss_scale: 131072.0000 (110141.1652)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0352  max mem: 8161
Epoch: [0]  [11320/60301]  eta: 6:52:14  lr: 0.000031  min_lr: 0.000001  loss: 5.2657 (5.5988)  loss_scale: 131072.0000 (110159.6537)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0013  max mem: 8161
Epoch: [0]  [11330/60301]  eta: 6:52:00  lr: 0.000031  min_lr: 0.000001  loss: 5.2292 (5.5986)  loss_scale: 131072.0000 (110178.1096)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.0013  max mem: 8161
Epoch: [0]  [11340/60301]  eta: 6:51:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2388 (5.5984)  loss_scale: 131072.0000 (110196.5329)  weight_decay: 0.0500 (0.0500)  time: 0.3943  data: 0.1907  max mem: 8161
Epoch: [0]  [11350/60301]  eta: 6:51:46  lr: 0.000031  min_lr: 0.000001  loss: 5.4929 (5.5984)  loss_scale: 131072.0000 (110214.9238)  weight_decay: 0.0500 (0.0500)  time: 0.4651  data: 0.3519  max mem: 8161
Epoch: [0]  [11360/60301]  eta: 6:51:41  lr: 0.000031  min_lr: 0.000001  loss: 5.5244 (5.5983)  loss_scale: 131072.0000 (110233.2823)  weight_decay: 0.0500 (0.0500)  time: 0.4666  data: 0.3528  max mem: 8161
Epoch: [0]  [11370/60301]  eta: 6:51:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4739 (5.5980)  loss_scale: 131072.0000 (110251.6085)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.3103  max mem: 8161
Epoch: [0]  [11380/60301]  eta: 6:51:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4661 (5.5979)  loss_scale: 131072.0000 (110269.9025)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1817  max mem: 8161
Epoch: [0]  [11390/60301]  eta: 6:51:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4336 (5.5978)  loss_scale: 131072.0000 (110288.1643)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0781  max mem: 8161
Epoch: [0]  [11400/60301]  eta: 6:51:25  lr: 0.000031  min_lr: 0.000001  loss: 5.2662 (5.5975)  loss_scale: 131072.0000 (110306.3942)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0313  max mem: 8161
Epoch: [0]  [11410/60301]  eta: 6:51:20  lr: 0.000031  min_lr: 0.000001  loss: 5.2662 (5.5973)  loss_scale: 131072.0000 (110324.5921)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0229  max mem: 8161
Epoch: [0]  [11420/60301]  eta: 6:51:14  lr: 0.000031  min_lr: 0.000001  loss: 5.3663 (5.5971)  loss_scale: 131072.0000 (110342.7581)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0081  max mem: 8161
Epoch: [0]  [11430/60301]  eta: 6:51:08  lr: 0.000031  min_lr: 0.000001  loss: 5.3663 (5.5970)  loss_scale: 131072.0000 (110360.8923)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0015  max mem: 8161
Epoch: [0]  [11440/60301]  eta: 6:51:02  lr: 0.000031  min_lr: 0.000001  loss: 5.2505 (5.5966)  loss_scale: 131072.0000 (110378.9948)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0005  max mem: 8161
Epoch: [0]  [11450/60301]  eta: 6:50:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3592 (5.5967)  loss_scale: 131072.0000 (110397.0658)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0005  max mem: 8161
Epoch: [0]  [11460/60301]  eta: 6:50:50  lr: 0.000031  min_lr: 0.000001  loss: 5.4171 (5.5964)  loss_scale: 131072.0000 (110415.1051)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0005  max mem: 8161
Epoch: [0]  [11470/60301]  eta: 6:50:44  lr: 0.000031  min_lr: 0.000001  loss: 5.3709 (5.5961)  loss_scale: 131072.0000 (110433.1131)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0014  max mem: 8161
Epoch: [0]  [11480/60301]  eta: 6:50:28  lr: 0.000031  min_lr: 0.000001  loss: 5.5135 (5.5962)  loss_scale: 131072.0000 (110451.0896)  weight_decay: 0.0500 (0.0500)  time: 0.3713  data: 0.0723  max mem: 8161
Epoch: [0]  [11490/60301]  eta: 6:50:23  lr: 0.000031  min_lr: 0.000001  loss: 5.6136 (5.5961)  loss_scale: 131072.0000 (110469.0349)  weight_decay: 0.0500 (0.0500)  time: 0.3710  data: 0.2571  max mem: 8161
Epoch: [0]  [11500/60301]  eta: 6:50:16  lr: 0.000031  min_lr: 0.000001  loss: 5.4987 (5.5959)  loss_scale: 131072.0000 (110486.9490)  weight_decay: 0.0500 (0.0500)  time: 0.4765  data: 0.3634  max mem: 8161
Epoch: [0]  [11510/60301]  eta: 6:50:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4202 (5.5958)  loss_scale: 131072.0000 (110504.8319)  weight_decay: 0.0500 (0.0500)  time: 0.4651  data: 0.3517  max mem: 8161
Epoch: [0]  [11520/60301]  eta: 6:50:07  lr: 0.000031  min_lr: 0.000001  loss: 5.3584 (5.5955)  loss_scale: 131072.0000 (110522.6838)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.2505  max mem: 8161
[2023-07-14 01:56:33,583] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5764
[2023-07-14 01:56:33,584] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-14 01:56:33,584] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [0]  [11530/60301]  eta: 6:50:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3395 (5.5953)  loss_scale: 131072.0000 (110529.1378)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0874  max mem: 8161
Epoch: [0]  [11540/60301]  eta: 6:50:07  lr: 0.000031  min_lr: 0.000001  loss: 5.3395 (5.5951)  loss_scale: 65536.0000 (110490.1523)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1485  max mem: 8161
Epoch: [0]  [11550/60301]  eta: 6:50:01  lr: 0.000031  min_lr: 0.000001  loss: 5.3490 (5.5949)  loss_scale: 65536.0000 (110451.2344)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1387  max mem: 8161
Epoch: [0]  [11560/60301]  eta: 6:49:55  lr: 0.000031  min_lr: 0.000001  loss: 5.2580 (5.5947)  loss_scale: 65536.0000 (110412.3837)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0015  max mem: 8161
Epoch: [0]  [11570/60301]  eta: 6:49:46  lr: 0.000031  min_lr: 0.000001  loss: 5.2997 (5.5945)  loss_scale: 65536.0000 (110373.6002)  weight_decay: 0.0500 (0.0500)  time: 0.4462  data: 0.1106  max mem: 8161
Epoch: [0]  [11580/60301]  eta: 6:49:40  lr: 0.000031  min_lr: 0.000001  loss: 5.3082 (5.5942)  loss_scale: 65536.0000 (110334.8837)  weight_decay: 0.0500 (0.0500)  time: 0.4431  data: 0.2917  max mem: 8161
Epoch: [0]  [11590/60301]  eta: 6:49:35  lr: 0.000031  min_lr: 0.000001  loss: 5.3082 (5.5940)  loss_scale: 65536.0000 (110296.2340)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.3814  max mem: 8161
Epoch: [0]  [11600/60301]  eta: 6:49:33  lr: 0.000031  min_lr: 0.000001  loss: 5.4154 (5.5938)  loss_scale: 65536.0000 (110257.6509)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.3503  max mem: 8161
Epoch: [0]  [11610/60301]  eta: 6:49:35  lr: 0.000031  min_lr: 0.000001  loss: 5.3499 (5.5936)  loss_scale: 65536.0000 (110219.1343)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.2453  max mem: 8161
Epoch: [0]  [11620/60301]  eta: 6:49:30  lr: 0.000031  min_lr: 0.000001  loss: 5.3182 (5.5933)  loss_scale: 65536.0000 (110180.6839)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1079  max mem: 8161
Epoch: [0]  [11630/60301]  eta: 6:49:24  lr: 0.000031  min_lr: 0.000001  loss: 5.2244 (5.5930)  loss_scale: 65536.0000 (110142.2997)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0138  max mem: 8161
Epoch: [0]  [11640/60301]  eta: 6:49:18  lr: 0.000031  min_lr: 0.000001  loss: 5.3873 (5.5930)  loss_scale: 65536.0000 (110103.9814)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0012  max mem: 8161
Epoch: [0]  [11650/60301]  eta: 6:49:24  lr: 0.000031  min_lr: 0.000001  loss: 5.3675 (5.5926)  loss_scale: 65536.0000 (110065.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1454  max mem: 8161
Epoch: [0]  [11660/60301]  eta: 6:49:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2168 (5.5925)  loss_scale: 65536.0000 (110027.5421)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1454  max mem: 8161
Epoch: [0]  [11670/60301]  eta: 6:49:12  lr: 0.000031  min_lr: 0.000001  loss: 5.3417 (5.5923)  loss_scale: 65536.0000 (109989.4206)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0009  max mem: 8161
Epoch: [0]  [11680/60301]  eta: 6:49:06  lr: 0.000031  min_lr: 0.000001  loss: 5.3417 (5.5922)  loss_scale: 65536.0000 (109951.3644)  weight_decay: 0.0500 (0.0500)  time: 0.4845  data: 0.0019  max mem: 8161
Epoch: [0]  [11690/60301]  eta: 6:49:00  lr: 0.000031  min_lr: 0.000001  loss: 5.3128 (5.5919)  loss_scale: 65536.0000 (109913.3734)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0014  max mem: 8161
Epoch: [0]  [11700/60301]  eta: 6:48:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2917 (5.5917)  loss_scale: 65536.0000 (109875.4472)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0004  max mem: 8161
Epoch: [0]  [11710/60301]  eta: 6:48:48  lr: 0.000031  min_lr: 0.000001  loss: 5.2917 (5.5915)  loss_scale: 65536.0000 (109837.5859)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0004  max mem: 8161
Epoch: [0]  [11720/60301]  eta: 6:48:42  lr: 0.000031  min_lr: 0.000001  loss: 5.1331 (5.5910)  loss_scale: 65536.0000 (109799.7891)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0004  max mem: 8161
Epoch: [0]  [11730/60301]  eta: 6:48:36  lr: 0.000031  min_lr: 0.000001  loss: 5.0280 (5.5906)  loss_scale: 65536.0000 (109762.0568)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0005  max mem: 8161
Epoch: [0]  [11740/60301]  eta: 6:48:21  lr: 0.000031  min_lr: 0.000001  loss: 5.0255 (5.5901)  loss_scale: 65536.0000 (109724.3887)  weight_decay: 0.0500 (0.0500)  time: 0.3675  data: 0.0692  max mem: 8161
Epoch: [0]  [11750/60301]  eta: 6:48:13  lr: 0.000031  min_lr: 0.000001  loss: 5.1833 (5.5899)  loss_scale: 65536.0000 (109686.7848)  weight_decay: 0.0500 (0.0500)  time: 0.3496  data: 0.2364  max mem: 8161
Epoch: [0]  [11760/60301]  eta: 6:48:27  lr: 0.000031  min_lr: 0.000001  loss: 5.2673 (5.5897)  loss_scale: 65536.0000 (109649.2448)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.5822  max mem: 8161
Epoch: [0]  [11770/60301]  eta: 6:48:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3650 (5.5895)  loss_scale: 65536.0000 (109611.7686)  weight_decay: 0.0500 (0.0500)  time: 0.7172  data: 0.4150  max mem: 8161
Epoch: [0]  [11780/60301]  eta: 6:48:15  lr: 0.000031  min_lr: 0.000001  loss: 5.4200 (5.5893)  loss_scale: 65536.0000 (109574.3560)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0041  max mem: 8161
[2023-07-14 01:58:48,367] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 01:58:48,367] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [0]  [11790/60301]  eta: 6:48:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4200 (5.5891)  loss_scale: 65536.0000 (109559.2394)  weight_decay: 0.0500 (0.0500)  time: 0.4866  data: 0.0051  max mem: 8161
Epoch: [0]  [11800/60301]  eta: 6:48:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4092 (5.5888)  loss_scale: 131072.0000 (109577.4690)  weight_decay: 0.0500 (0.0500)  time: 0.4824  data: 0.0014  max mem: 8161
Epoch: [0]  [11810/60301]  eta: 6:47:57  lr: 0.000031  min_lr: 0.000001  loss: 5.4743 (5.5887)  loss_scale: 131072.0000 (109595.6678)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0004  max mem: 8161
Epoch: [0]  [11820/60301]  eta: 6:47:51  lr: 0.000031  min_lr: 0.000001  loss: 5.3665 (5.5885)  loss_scale: 131072.0000 (109613.8357)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0004  max mem: 8161
Epoch: [0]  [11830/60301]  eta: 6:47:46  lr: 0.000031  min_lr: 0.000001  loss: 5.1860 (5.5882)  loss_scale: 131072.0000 (109631.9730)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0136  max mem: 8161
Epoch: [0]  [11840/60301]  eta: 6:47:42  lr: 0.000031  min_lr: 0.000001  loss: 5.2416 (5.5880)  loss_scale: 131072.0000 (109650.0796)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0382  max mem: 8161
Epoch: [0]  [11850/60301]  eta: 6:47:36  lr: 0.000031  min_lr: 0.000001  loss: 5.2693 (5.5878)  loss_scale: 131072.0000 (109668.1556)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0251  max mem: 8161
Epoch: [0]  [11860/60301]  eta: 6:47:30  lr: 0.000031  min_lr: 0.000001  loss: 5.3652 (5.5876)  loss_scale: 131072.0000 (109686.2012)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
Epoch: [0]  [11870/60301]  eta: 6:47:24  lr: 0.000031  min_lr: 0.000001  loss: 5.3940 (5.5874)  loss_scale: 131072.0000 (109704.2163)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0024  max mem: 8161
Epoch: [0]  [11880/60301]  eta: 6:47:20  lr: 0.000031  min_lr: 0.000001  loss: 5.2558 (5.5870)  loss_scale: 131072.0000 (109722.2012)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0224  max mem: 8161
Epoch: [0]  [11890/60301]  eta: 6:47:26  lr: 0.000031  min_lr: 0.000001  loss: 5.3234 (5.5868)  loss_scale: 131072.0000 (109740.1557)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.1730  max mem: 8161
Epoch: [0]  [11900/60301]  eta: 6:47:22  lr: 0.000031  min_lr: 0.000001  loss: 5.4158 (5.5866)  loss_scale: 131072.0000 (109758.0802)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.1723  max mem: 8161
Epoch: [0]  [11910/60301]  eta: 6:47:16  lr: 0.000031  min_lr: 0.000001  loss: 5.3751 (5.5863)  loss_scale: 131072.0000 (109775.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0207  max mem: 8161
Epoch: [0]  [11920/60301]  eta: 6:47:11  lr: 0.000031  min_lr: 0.000001  loss: 5.1286 (5.5859)  loss_scale: 131072.0000 (109793.8388)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0183  max mem: 8161
Epoch: [0]  [11930/60301]  eta: 6:47:05  lr: 0.000031  min_lr: 0.000001  loss: 5.1938 (5.5857)  loss_scale: 131072.0000 (109811.6731)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0173  max mem: 8161
Epoch: [0]  [11940/60301]  eta: 6:46:59  lr: 0.000031  min_lr: 0.000001  loss: 5.3471 (5.5855)  loss_scale: 131072.0000 (109829.4776)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0004  max mem: 8161
Epoch: [0]  [11950/60301]  eta: 6:46:53  lr: 0.000031  min_lr: 0.000001  loss: 5.3726 (5.5854)  loss_scale: 131072.0000 (109847.2523)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0013  max mem: 8161
Epoch: [0]  [11960/60301]  eta: 6:46:47  lr: 0.000031  min_lr: 0.000001  loss: 5.3811 (5.5851)  loss_scale: 131072.0000 (109864.9972)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0022  max mem: 8161
Epoch: [0]  [11970/60301]  eta: 6:46:41  lr: 0.000031  min_lr: 0.000001  loss: 5.4368 (5.5850)  loss_scale: 131072.0000 (109882.7126)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0013  max mem: 8161
Epoch: [0]  [11980/60301]  eta: 6:46:35  lr: 0.000031  min_lr: 0.000001  loss: 5.3472 (5.5849)  loss_scale: 131072.0000 (109900.3983)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
Epoch: [0]  [11990/60301]  eta: 6:46:29  lr: 0.000031  min_lr: 0.000001  loss: 5.3351 (5.5846)  loss_scale: 131072.0000 (109918.0545)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0005  max mem: 8161
[2023-07-14 02:00:35,483] [INFO] [logging.py:69:log_dist] [Rank 0] step=6000, skipped=32, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 02:00:35,487] [INFO] [timer.py:181:stop] 0/12000, SamplesPerSec=9.3939687925733
Epoch: [0]  [12000/60301]  eta: 6:46:23  lr: 0.000031  min_lr: 0.000001  loss: 5.3355 (5.5844)  loss_scale: 131072.0000 (109935.6814)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0055  max mem: 8161
Epoch: [0]  [12010/60301]  eta: 6:46:17  lr: 0.000031  min_lr: 0.000001  loss: 5.3686 (5.5844)  loss_scale: 131072.0000 (109953.2788)  weight_decay: 0.0500 (0.0500)  time: 0.4858  data: 0.0055  max mem: 8161
Epoch: [0]  [12020/60301]  eta: 6:46:11  lr: 0.000031  min_lr: 0.000001  loss: 5.3652 (5.5842)  loss_scale: 131072.0000 (109970.8470)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0021  max mem: 8161
Epoch: [0]  [12030/60301]  eta: 6:46:17  lr: 0.000031  min_lr: 0.000001  loss: 5.2945 (5.5840)  loss_scale: 131072.0000 (109988.3860)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1472  max mem: 8161
Epoch: [0]  [12040/60301]  eta: 6:46:12  lr: 0.000031  min_lr: 0.000001  loss: 5.3259 (5.5838)  loss_scale: 131072.0000 (110005.8959)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1628  max mem: 8161
[2023-07-14 02:00:59,775] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:00:59,775] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 02:01:00,721] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6022
[2023-07-14 02:01:00,722] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:01:00,722] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [12050/60301]  eta: 6:46:06  lr: 0.000031  min_lr: 0.000001  loss: 5.3608 (5.5836)  loss_scale: 131072.0000 (110045.1295)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0188  max mem: 8161
Epoch: [0]  [12060/60301]  eta: 6:46:00  lr: 0.000031  min_lr: 0.000001  loss: 5.4223 (5.5834)  loss_scale: 131072.0000 (110062.5633)  weight_decay: 0.0500 (0.0500)  time: 0.4780  data: 0.0016  max mem: 8161
Epoch: [0]  [12070/60301]  eta: 6:45:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4282 (5.5832)  loss_scale: 131072.0000 (110079.9682)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0004  max mem: 8161
Epoch: [0]  [12080/60301]  eta: 6:45:48  lr: 0.000031  min_lr: 0.000001  loss: 5.4124 (5.5832)  loss_scale: 131072.0000 (110097.3443)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0028  max mem: 8161
Epoch: [0]  [12090/60301]  eta: 6:45:42  lr: 0.000031  min_lr: 0.000001  loss: 5.4499 (5.5830)  loss_scale: 131072.0000 (110114.6916)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0028  max mem: 8161
Epoch: [0]  [12100/60301]  eta: 6:45:36  lr: 0.000031  min_lr: 0.000001  loss: 5.2851 (5.5826)  loss_scale: 131072.0000 (110132.0102)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0005  max mem: 8161
Epoch: [0]  [12110/60301]  eta: 6:45:30  lr: 0.000031  min_lr: 0.000001  loss: 5.0673 (5.5823)  loss_scale: 131072.0000 (110149.3003)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0004  max mem: 8161
Epoch: [0]  [12120/60301]  eta: 6:45:24  lr: 0.000031  min_lr: 0.000001  loss: 5.1425 (5.5820)  loss_scale: 131072.0000 (110166.5618)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0014  max mem: 8161
Epoch: [0]  [12130/60301]  eta: 6:45:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2478 (5.5818)  loss_scale: 131072.0000 (110183.7949)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0015  max mem: 8161
Epoch: [0]  [12140/60301]  eta: 6:45:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2349 (5.5814)  loss_scale: 131072.0000 (110200.9996)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0005  max mem: 8161
Epoch: [0]  [12150/60301]  eta: 6:45:06  lr: 0.000031  min_lr: 0.000001  loss: 5.1798 (5.5811)  loss_scale: 131072.0000 (110218.1760)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0020  max mem: 8161
Epoch: [0]  [12160/60301]  eta: 6:45:00  lr: 0.000031  min_lr: 0.000001  loss: 5.4008 (5.5809)  loss_scale: 131072.0000 (110235.3241)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0019  max mem: 8161
Epoch: [0]  [12170/60301]  eta: 6:44:54  lr: 0.000031  min_lr: 0.000001  loss: 5.2654 (5.5806)  loss_scale: 131072.0000 (110252.4440)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0005  max mem: 8161
Epoch: [0]  [12180/60301]  eta: 6:44:48  lr: 0.000031  min_lr: 0.000001  loss: 5.2453 (5.5803)  loss_scale: 131072.0000 (110269.5358)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 8161
Epoch: [0]  [12190/60301]  eta: 6:44:42  lr: 0.000031  min_lr: 0.000001  loss: 5.2485 (5.5801)  loss_scale: 131072.0000 (110286.5996)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0011  max mem: 8161
Epoch: [0]  [12200/60301]  eta: 6:44:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3304 (5.5799)  loss_scale: 131072.0000 (110303.6354)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0019  max mem: 8161
Epoch: [0]  [12210/60301]  eta: 6:44:30  lr: 0.000031  min_lr: 0.000001  loss: 5.4321 (5.5797)  loss_scale: 131072.0000 (110320.6434)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0022  max mem: 8161
Epoch: [0]  [12220/60301]  eta: 6:44:23  lr: 0.000031  min_lr: 0.000001  loss: 5.2625 (5.5794)  loss_scale: 131072.0000 (110337.6234)  weight_decay: 0.0500 (0.0500)  time: 0.4686  data: 0.0912  max mem: 8161
Epoch: [0]  [12230/60301]  eta: 6:44:21  lr: 0.000031  min_lr: 0.000001  loss: 5.2173 (5.5791)  loss_scale: 131072.0000 (110354.5758)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.3141  max mem: 8161
Epoch: [0]  [12240/60301]  eta: 6:44:14  lr: 0.000031  min_lr: 0.000001  loss: 5.3294 (5.5789)  loss_scale: 131072.0000 (110371.5004)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.3930  max mem: 8161
Epoch: [0]  [12250/60301]  eta: 6:44:03  lr: 0.000031  min_lr: 0.000001  loss: 5.2670 (5.5786)  loss_scale: 131072.0000 (110388.3974)  weight_decay: 0.0500 (0.0500)  time: 0.4049  data: 0.2917  max mem: 8161
Epoch: [0]  [12260/60301]  eta: 6:44:04  lr: 0.000031  min_lr: 0.000001  loss: 5.2432 (5.5784)  loss_scale: 131072.0000 (110405.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.3322  max mem: 8161
Epoch: [0]  [12270/60301]  eta: 6:43:59  lr: 0.000031  min_lr: 0.000001  loss: 5.2150 (5.5781)  loss_scale: 131072.0000 (110422.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.2105  max mem: 8161
Epoch: [0]  [12280/60301]  eta: 6:43:53  lr: 0.000031  min_lr: 0.000001  loss: 5.2792 (5.5780)  loss_scale: 131072.0000 (110438.9232)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0013  max mem: 8161
Epoch: [0]  [12290/60301]  eta: 6:43:47  lr: 0.000031  min_lr: 0.000001  loss: 5.4899 (5.5780)  loss_scale: 131072.0000 (110455.7104)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0004  max mem: 8161
Epoch: [0]  [12300/60301]  eta: 6:43:41  lr: 0.000031  min_lr: 0.000001  loss: 5.3654 (5.5778)  loss_scale: 131072.0000 (110472.4702)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0004  max mem: 8161
[2023-07-14 02:03:06,096] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:03:06,096] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [12310/60301]  eta: 6:43:35  lr: 0.000031  min_lr: 0.000001  loss: 5.2979 (5.5775)  loss_scale: 131072.0000 (110574.3767)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.0004  max mem: 8161
Epoch: [0]  [12320/60301]  eta: 6:43:30  lr: 0.000031  min_lr: 0.000001  loss: 5.1386 (5.5770)  loss_scale: 262144.0000 (110697.3940)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0010  max mem: 8161
[2023-07-14 02:03:17,421] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6163
[2023-07-14 02:03:17,421] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:03:17,421] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [12330/60301]  eta: 6:43:22  lr: 0.000031  min_lr: 0.000001  loss: 5.2761 (5.5769)  loss_scale: 262144.0000 (110777.6939)  weight_decay: 0.0500 (0.0500)  time: 0.4639  data: 0.0010  max mem: 8161
Epoch: [0]  [12340/60301]  eta: 6:43:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5148 (5.5767)  loss_scale: 131072.0000 (110794.1386)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0150  max mem: 8161
Epoch: [0]  [12350/60301]  eta: 6:43:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2632 (5.5765)  loss_scale: 131072.0000 (110810.5566)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0150  max mem: 8161
Epoch: [0]  [12360/60301]  eta: 6:43:06  lr: 0.000031  min_lr: 0.000001  loss: 5.2003 (5.5763)  loss_scale: 131072.0000 (110826.9480)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0004  max mem: 8161
Epoch: [0]  [12370/60301]  eta: 6:43:00  lr: 0.000031  min_lr: 0.000001  loss: 5.1737 (5.5760)  loss_scale: 131072.0000 (110843.3129)  weight_decay: 0.0500 (0.0500)  time: 0.4850  data: 0.0004  max mem: 8161
Epoch: [0]  [12380/60301]  eta: 6:42:54  lr: 0.000031  min_lr: 0.000001  loss: 5.1970 (5.5759)  loss_scale: 131072.0000 (110859.6514)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0004  max mem: 8161
Epoch: [0]  [12390/60301]  eta: 6:42:55  lr: 0.000031  min_lr: 0.000001  loss: 5.2755 (5.5756)  loss_scale: 131072.0000 (110875.9635)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0864  max mem: 8161
Epoch: [0]  [12400/60301]  eta: 6:42:49  lr: 0.000031  min_lr: 0.000001  loss: 5.2226 (5.5754)  loss_scale: 131072.0000 (110892.2493)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0902  max mem: 8161
Epoch: [0]  [12410/60301]  eta: 6:42:44  lr: 0.000031  min_lr: 0.000001  loss: 5.4240 (5.5752)  loss_scale: 131072.0000 (110908.5089)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0042  max mem: 8161
Epoch: [0]  [12420/60301]  eta: 6:42:46  lr: 0.000031  min_lr: 0.000001  loss: 5.3166 (5.5749)  loss_scale: 131072.0000 (110924.7423)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1112  max mem: 8161
Epoch: [0]  [12430/60301]  eta: 6:42:40  lr: 0.000031  min_lr: 0.000001  loss: 5.1081 (5.5746)  loss_scale: 131072.0000 (110940.9496)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1124  max mem: 8161
Epoch: [0]  [12440/60301]  eta: 6:42:35  lr: 0.000031  min_lr: 0.000001  loss: 5.1842 (5.5742)  loss_scale: 131072.0000 (110957.1308)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0035  max mem: 8161
Epoch: [0]  [12450/60301]  eta: 6:42:29  lr: 0.000031  min_lr: 0.000001  loss: 5.2621 (5.5740)  loss_scale: 131072.0000 (110973.2860)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0032  max mem: 8161
Epoch: [0]  [12460/60301]  eta: 6:42:23  lr: 0.000031  min_lr: 0.000001  loss: 5.2610 (5.5737)  loss_scale: 131072.0000 (110989.4153)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0013  max mem: 8161
Epoch: [0]  [12470/60301]  eta: 6:42:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2010 (5.5736)  loss_scale: 131072.0000 (111005.5187)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0005  max mem: 8161
Epoch: [0]  [12480/60301]  eta: 6:42:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2548 (5.5734)  loss_scale: 131072.0000 (111021.5963)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0005  max mem: 8161
Epoch: [0]  [12490/60301]  eta: 6:42:06  lr: 0.000031  min_lr: 0.000001  loss: 5.2631 (5.5730)  loss_scale: 131072.0000 (111037.6482)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0004  max mem: 8161
Epoch: [0]  [12500/60301]  eta: 6:42:00  lr: 0.000031  min_lr: 0.000001  loss: 5.3279 (5.5729)  loss_scale: 131072.0000 (111053.6744)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0004  max mem: 8161
Epoch: [0]  [12510/60301]  eta: 6:41:54  lr: 0.000031  min_lr: 0.000001  loss: 5.4182 (5.5728)  loss_scale: 131072.0000 (111069.6750)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0010  max mem: 8161
Epoch: [0]  [12520/60301]  eta: 6:41:48  lr: 0.000031  min_lr: 0.000001  loss: 5.4333 (5.5727)  loss_scale: 131072.0000 (111085.6500)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0011  max mem: 8161
Epoch: [0]  [12530/60301]  eta: 6:41:43  lr: 0.000031  min_lr: 0.000001  loss: 5.4664 (5.5725)  loss_scale: 131072.0000 (111101.5996)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0013  max mem: 8161
Epoch: [0]  [12540/60301]  eta: 6:41:37  lr: 0.000031  min_lr: 0.000001  loss: 5.1191 (5.5721)  loss_scale: 131072.0000 (111117.5236)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0021  max mem: 8161
Epoch: [0]  [12550/60301]  eta: 6:41:31  lr: 0.000031  min_lr: 0.000001  loss: 5.2668 (5.5719)  loss_scale: 131072.0000 (111133.4224)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0014  max mem: 8161
Epoch: [0]  [12560/60301]  eta: 6:41:25  lr: 0.000031  min_lr: 0.000001  loss: 5.3919 (5.5716)  loss_scale: 131072.0000 (111149.2958)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0004  max mem: 8161
Epoch: [0]  [12570/60301]  eta: 6:41:13  lr: 0.000031  min_lr: 0.000001  loss: 5.2224 (5.5714)  loss_scale: 131072.0000 (111165.1439)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0605  max mem: 8161
Epoch: [0]  [12580/60301]  eta: 6:41:02  lr: 0.000031  min_lr: 0.000001  loss: 5.0876 (5.5711)  loss_scale: 131072.0000 (111180.9669)  weight_decay: 0.0500 (0.0500)  time: 0.3247  data: 0.1747  max mem: 8161
[2023-07-14 02:05:23,818] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:05:23,819] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [12590/60301]  eta: 6:40:59  lr: 0.000031  min_lr: 0.000001  loss: 5.3228 (5.5709)  loss_scale: 131072.0000 (111259.2245)  weight_decay: 0.0500 (0.0500)  time: 0.4506  data: 0.3367  max mem: 8161
[2023-07-14 02:05:28,267] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6296
[2023-07-14 02:05:28,268] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:05:28,268] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [12600/60301]  eta: 6:40:52  lr: 0.000031  min_lr: 0.000001  loss: 5.3908 (5.5708)  loss_scale: 131072.0000 (111295.7511)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.3962  max mem: 8161
Epoch: [0]  [12610/60301]  eta: 6:40:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3468 (5.5705)  loss_scale: 131072.0000 (111311.4329)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.3564  max mem: 8161
Epoch: [0]  [12620/60301]  eta: 6:40:51  lr: 0.000031  min_lr: 0.000001  loss: 5.3468 (5.5704)  loss_scale: 131072.0000 (111327.0898)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1827  max mem: 8161
Epoch: [0]  [12630/60301]  eta: 6:40:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3641 (5.5701)  loss_scale: 131072.0000 (111342.7219)  weight_decay: 0.0500 (0.0500)  time: 0.4807  data: 0.0005  max mem: 8161
Epoch: [0]  [12640/60301]  eta: 6:40:39  lr: 0.000031  min_lr: 0.000001  loss: 5.2368 (5.5698)  loss_scale: 131072.0000 (111358.3292)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
Epoch: [0]  [12650/60301]  eta: 6:40:40  lr: 0.000031  min_lr: 0.000001  loss: 5.1751 (5.5696)  loss_scale: 131072.0000 (111373.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1051  max mem: 8161
Epoch: [0]  [12660/60301]  eta: 6:40:34  lr: 0.000031  min_lr: 0.000001  loss: 5.2476 (5.5694)  loss_scale: 131072.0000 (111389.4700)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1051  max mem: 8161
Epoch: [0]  [12670/60301]  eta: 6:40:28  lr: 0.000031  min_lr: 0.000001  loss: 5.3403 (5.5694)  loss_scale: 131072.0000 (111405.0036)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0005  max mem: 8161
Epoch: [0]  [12680/60301]  eta: 6:40:22  lr: 0.000031  min_lr: 0.000001  loss: 5.5519 (5.5693)  loss_scale: 131072.0000 (111420.5126)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0005  max mem: 8161
Epoch: [0]  [12690/60301]  eta: 6:40:16  lr: 0.000031  min_lr: 0.000001  loss: 5.4547 (5.5691)  loss_scale: 131072.0000 (111435.9972)  weight_decay: 0.0500 (0.0500)  time: 0.4774  data: 0.0005  max mem: 8161
Epoch: [0]  [12700/60301]  eta: 6:40:15  lr: 0.000031  min_lr: 0.000001  loss: 5.3683 (5.5690)  loss_scale: 131072.0000 (111451.4574)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0689  max mem: 8161
Epoch: [0]  [12710/60301]  eta: 6:40:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4232 (5.5688)  loss_scale: 131072.0000 (111466.8932)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0706  max mem: 8161
Epoch: [0]  [12720/60301]  eta: 6:40:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4178 (5.5687)  loss_scale: 131072.0000 (111482.3049)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0022  max mem: 8161
Epoch: [0]  [12730/60301]  eta: 6:39:58  lr: 0.000031  min_lr: 0.000001  loss: 5.4515 (5.5687)  loss_scale: 131072.0000 (111497.6922)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0097  max mem: 8161
Epoch: [0]  [12740/60301]  eta: 6:39:52  lr: 0.000031  min_lr: 0.000001  loss: 5.4515 (5.5685)  loss_scale: 131072.0000 (111513.0555)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0107  max mem: 8161
Epoch: [0]  [12750/60301]  eta: 6:39:46  lr: 0.000031  min_lr: 0.000001  loss: 5.3774 (5.5684)  loss_scale: 131072.0000 (111528.3946)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0014  max mem: 8161
Epoch: [0]  [12760/60301]  eta: 6:39:40  lr: 0.000031  min_lr: 0.000001  loss: 5.3415 (5.5682)  loss_scale: 131072.0000 (111543.7097)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0031  max mem: 8161
Epoch: [0]  [12770/60301]  eta: 6:39:34  lr: 0.000031  min_lr: 0.000001  loss: 5.4779 (5.5681)  loss_scale: 131072.0000 (111559.0009)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0049  max mem: 8161
Epoch: [0]  [12780/60301]  eta: 6:39:29  lr: 0.000031  min_lr: 0.000001  loss: 5.4864 (5.5680)  loss_scale: 131072.0000 (111574.2681)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0022  max mem: 8161
Epoch: [0]  [12790/60301]  eta: 6:39:23  lr: 0.000031  min_lr: 0.000001  loss: 5.1940 (5.5678)  loss_scale: 131072.0000 (111589.5114)  weight_decay: 0.0500 (0.0500)  time: 0.4789  data: 0.0005  max mem: 8161
Epoch: [0]  [12800/60301]  eta: 6:39:17  lr: 0.000031  min_lr: 0.000001  loss: 5.1929 (5.5677)  loss_scale: 131072.0000 (111604.7309)  weight_decay: 0.0500 (0.0500)  time: 0.4785  data: 0.0012  max mem: 8161
Epoch: [0]  [12810/60301]  eta: 6:39:11  lr: 0.000031  min_lr: 0.000001  loss: 5.2099 (5.5673)  loss_scale: 131072.0000 (111619.9266)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0012  max mem: 8161
Epoch: [0]  [12820/60301]  eta: 6:39:05  lr: 0.000031  min_lr: 0.000001  loss: 5.2990 (5.5672)  loss_scale: 131072.0000 (111635.0987)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0021  max mem: 8161
Epoch: [0]  [12830/60301]  eta: 6:38:59  lr: 0.000031  min_lr: 0.000001  loss: 5.4031 (5.5671)  loss_scale: 131072.0000 (111650.2471)  weight_decay: 0.0500 (0.0500)  time: 0.4818  data: 0.0030  max mem: 8161
Epoch: [0]  [12840/60301]  eta: 6:38:53  lr: 0.000031  min_lr: 0.000001  loss: 5.3477 (5.5669)  loss_scale: 131072.0000 (111665.3719)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0015  max mem: 8161
Epoch: [0]  [12850/60301]  eta: 6:38:35  lr: 0.000031  min_lr: 0.000001  loss: 5.2914 (5.5667)  loss_scale: 131072.0000 (111680.4731)  weight_decay: 0.0500 (0.0500)  time: 0.3242  data: 0.0019  max mem: 8161
[2023-07-14 02:07:34,849] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:07:34,849] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 02:07:37,736] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6428
[2023-07-14 02:07:37,736] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:07:37,736] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [12860/60301]  eta: 6:38:28  lr: 0.000031  min_lr: 0.000001  loss: 5.2469 (5.5664)  loss_scale: 131072.0000 (111756.6995)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.1686  max mem: 8161
Epoch: [0]  [12870/60301]  eta: 6:38:19  lr: 0.000031  min_lr: 0.000001  loss: 5.3020 (5.5663)  loss_scale: 131072.0000 (111771.7063)  weight_decay: 0.0500 (0.0500)  time: 0.4180  data: 0.3024  max mem: 8161
Epoch: [0]  [12880/60301]  eta: 6:38:17  lr: 0.000031  min_lr: 0.000001  loss: 5.5306 (5.5662)  loss_scale: 131072.0000 (111786.6899)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.3704  max mem: 8161
Epoch: [0]  [12890/60301]  eta: 6:38:15  lr: 0.000031  min_lr: 0.000001  loss: 5.5790 (5.5660)  loss_scale: 131072.0000 (111801.6501)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.3655  max mem: 8161
Epoch: [0]  [12900/60301]  eta: 6:38:09  lr: 0.000031  min_lr: 0.000001  loss: 5.2593 (5.5658)  loss_scale: 131072.0000 (111816.5872)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1306  max mem: 8161
Epoch: [0]  [12910/60301]  eta: 6:38:03  lr: 0.000031  min_lr: 0.000001  loss: 5.2610 (5.5656)  loss_scale: 131072.0000 (111831.5012)  weight_decay: 0.0500 (0.0500)  time: 0.4807  data: 0.0005  max mem: 8161
Epoch: [0]  [12920/60301]  eta: 6:37:57  lr: 0.000031  min_lr: 0.000001  loss: 5.2960 (5.5654)  loss_scale: 131072.0000 (111846.3921)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0005  max mem: 8161
Epoch: [0]  [12930/60301]  eta: 6:37:52  lr: 0.000031  min_lr: 0.000001  loss: 5.3740 (5.5653)  loss_scale: 131072.0000 (111861.2599)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0021  max mem: 8161
Epoch: [0]  [12940/60301]  eta: 6:37:46  lr: 0.000031  min_lr: 0.000001  loss: 5.3047 (5.5650)  loss_scale: 131072.0000 (111876.1048)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0021  max mem: 8161
Epoch: [0]  [12950/60301]  eta: 6:37:45  lr: 0.000031  min_lr: 0.000001  loss: 5.2022 (5.5647)  loss_scale: 131072.0000 (111890.9267)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0703  max mem: 8161
Epoch: [0]  [12960/60301]  eta: 6:37:40  lr: 0.000031  min_lr: 0.000001  loss: 5.4759 (5.5647)  loss_scale: 131072.0000 (111905.7258)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0749  max mem: 8161
Epoch: [0]  [12970/60301]  eta: 6:37:36  lr: 0.000031  min_lr: 0.000001  loss: 5.2930 (5.5643)  loss_scale: 131072.0000 (111920.5020)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0326  max mem: 8161
Epoch: [0]  [12980/60301]  eta: 6:37:30  lr: 0.000031  min_lr: 0.000001  loss: 5.1235 (5.5640)  loss_scale: 131072.0000 (111935.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0289  max mem: 8161
Epoch: [0]  [12990/60301]  eta: 6:37:24  lr: 0.000031  min_lr: 0.000001  loss: 5.1235 (5.5637)  loss_scale: 131072.0000 (111949.9863)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0014  max mem: 8161
[2023-07-14 02:08:48,305] [INFO] [timer.py:181:stop] 0/13000, SamplesPerSec=9.399305954831215
Epoch: [0]  [13000/60301]  eta: 6:37:13  lr: 0.000031  min_lr: 0.000001  loss: 5.1008 (5.5633)  loss_scale: 131072.0000 (111964.6944)  weight_decay: 0.0500 (0.0500)  time: 0.4185  data: 0.0915  max mem: 8161
Epoch: [0]  [13010/60301]  eta: 6:37:07  lr: 0.000031  min_lr: 0.000001  loss: 5.1813 (5.5630)  loss_scale: 131072.0000 (111979.3799)  weight_decay: 0.0500 (0.0500)  time: 0.4076  data: 0.2655  max mem: 8161
Epoch: [0]  [13020/60301]  eta: 6:36:56  lr: 0.000031  min_lr: 0.000001  loss: 5.4400 (5.5630)  loss_scale: 131072.0000 (111994.0429)  weight_decay: 0.0500 (0.0500)  time: 0.4088  data: 0.2966  max mem: 8161
Epoch: [0]  [13030/60301]  eta: 6:36:55  lr: 0.000031  min_lr: 0.000001  loss: 5.5148 (5.5630)  loss_scale: 131072.0000 (112008.6833)  weight_decay: 0.0500 (0.0500)  time: 0.4781  data: 0.3633  max mem: 8161
Epoch: [0]  [13040/60301]  eta: 6:37:06  lr: 0.000031  min_lr: 0.000001  loss: 5.4916 (5.5628)  loss_scale: 131072.0000 (112023.3013)  weight_decay: 0.0500 (0.0500)  time: 0.7775  data: 0.5151  max mem: 8161
Epoch: [0]  [13050/60301]  eta: 6:37:02  lr: 0.000031  min_lr: 0.000001  loss: 5.3203 (5.5626)  loss_scale: 131072.0000 (112037.8969)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.2982  max mem: 8161
Epoch: [0]  [13060/60301]  eta: 6:36:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3683 (5.5626)  loss_scale: 131072.0000 (112052.4701)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0242  max mem: 8161
Epoch: [0]  [13070/60301]  eta: 6:36:50  lr: 0.000031  min_lr: 0.000001  loss: 5.2341 (5.5623)  loss_scale: 131072.0000 (112067.0210)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0004  max mem: 8161
Epoch: [0]  [13080/60301]  eta: 6:36:48  lr: 0.000031  min_lr: 0.000001  loss: 5.1735 (5.5620)  loss_scale: 131072.0000 (112081.5497)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0455  max mem: 8161
Epoch: [0]  [13090/60301]  eta: 6:36:42  lr: 0.000031  min_lr: 0.000001  loss: 5.0790 (5.5616)  loss_scale: 131072.0000 (112096.0562)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0481  max mem: 8161
Epoch: [0]  [13100/60301]  eta: 6:36:36  lr: 0.000031  min_lr: 0.000001  loss: 5.0213 (5.5613)  loss_scale: 131072.0000 (112110.5406)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0040  max mem: 8161
Epoch: [0]  [13110/60301]  eta: 6:36:30  lr: 0.000031  min_lr: 0.000001  loss: 5.1417 (5.5611)  loss_scale: 131072.0000 (112125.0028)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0014  max mem: 8161
[2023-07-14 02:09:49,871] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:09:49,871] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2023-07-14 02:09:50,839] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6558
[2023-07-14 02:09:50,839] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:09:50,839] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [13120/60301]  eta: 6:36:24  lr: 0.000031  min_lr: 0.000001  loss: 5.2434 (5.5607)  loss_scale: 131072.0000 (112159.4220)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0005  max mem: 8161
Epoch: [0]  [13130/60301]  eta: 6:36:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2333 (5.5606)  loss_scale: 131072.0000 (112173.8250)  weight_decay: 0.0500 (0.0500)  time: 0.4794  data: 0.0012  max mem: 8161
Epoch: [0]  [13140/60301]  eta: 6:36:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2702 (5.5604)  loss_scale: 131072.0000 (112188.2061)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0012  max mem: 8161
Epoch: [0]  [13150/60301]  eta: 6:36:15  lr: 0.000031  min_lr: 0.000001  loss: 5.3339 (5.5603)  loss_scale: 131072.0000 (112202.5653)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1232  max mem: 8161
Epoch: [0]  [13160/60301]  eta: 6:36:10  lr: 0.000031  min_lr: 0.000001  loss: 5.4050 (5.5601)  loss_scale: 131072.0000 (112216.9027)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1353  max mem: 8161
Epoch: [0]  [13170/60301]  eta: 6:36:05  lr: 0.000031  min_lr: 0.000001  loss: 5.3568 (5.5599)  loss_scale: 131072.0000 (112231.2183)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0125  max mem: 8161
Epoch: [0]  [13180/60301]  eta: 6:35:59  lr: 0.000031  min_lr: 0.000001  loss: 5.3086 (5.5598)  loss_scale: 131072.0000 (112245.5122)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0013  max mem: 8161
Epoch: [0]  [13190/60301]  eta: 6:35:53  lr: 0.000031  min_lr: 0.000001  loss: 5.2955 (5.5595)  loss_scale: 131072.0000 (112259.7844)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0013  max mem: 8161
Epoch: [0]  [13200/60301]  eta: 6:35:48  lr: 0.000031  min_lr: 0.000001  loss: 5.3185 (5.5595)  loss_scale: 131072.0000 (112274.0350)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0151  max mem: 8161
Epoch: [0]  [13210/60301]  eta: 6:35:42  lr: 0.000031  min_lr: 0.000001  loss: 5.3857 (5.5593)  loss_scale: 131072.0000 (112288.2640)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0152  max mem: 8161
Epoch: [0]  [13220/60301]  eta: 6:35:36  lr: 0.000031  min_lr: 0.000001  loss: 5.3049 (5.5591)  loss_scale: 131072.0000 (112302.4715)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0005  max mem: 8161
Epoch: [0]  [13230/60301]  eta: 6:35:30  lr: 0.000031  min_lr: 0.000001  loss: 5.2589 (5.5588)  loss_scale: 131072.0000 (112316.6575)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0005  max mem: 8161
Epoch: [0]  [13240/60301]  eta: 6:35:25  lr: 0.000031  min_lr: 0.000001  loss: 5.2589 (5.5585)  loss_scale: 131072.0000 (112330.8221)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0004  max mem: 8161
Epoch: [0]  [13250/60301]  eta: 6:35:19  lr: 0.000031  min_lr: 0.000001  loss: 5.4070 (5.5584)  loss_scale: 131072.0000 (112344.9654)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0020  max mem: 8161
Epoch: [0]  [13260/60301]  eta: 6:35:13  lr: 0.000031  min_lr: 0.000001  loss: 5.3387 (5.5582)  loss_scale: 131072.0000 (112359.0872)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0029  max mem: 8161
Epoch: [0]  [13270/60301]  eta: 6:35:07  lr: 0.000031  min_lr: 0.000001  loss: 5.1044 (5.5578)  loss_scale: 131072.0000 (112373.1879)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0025  max mem: 8161
Epoch: [0]  [13280/60301]  eta: 6:35:02  lr: 0.000031  min_lr: 0.000001  loss: 5.1044 (5.5576)  loss_scale: 131072.0000 (112387.2672)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0015  max mem: 8161
Epoch: [0]  [13290/60301]  eta: 6:34:56  lr: 0.000031  min_lr: 0.000001  loss: 5.2007 (5.5573)  loss_scale: 131072.0000 (112401.3254)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0005  max mem: 8161
Epoch: [0]  [13300/60301]  eta: 6:34:50  lr: 0.000031  min_lr: 0.000001  loss: 5.1541 (5.5571)  loss_scale: 131072.0000 (112415.3625)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0004  max mem: 8161
Epoch: [0]  [13310/60301]  eta: 6:34:44  lr: 0.000031  min_lr: 0.000001  loss: 5.1468 (5.5568)  loss_scale: 131072.0000 (112429.3784)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0010  max mem: 8161
Epoch: [0]  [13320/60301]  eta: 6:34:38  lr: 0.000031  min_lr: 0.000001  loss: 5.1083 (5.5566)  loss_scale: 131072.0000 (112443.3733)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0020  max mem: 8161
Epoch: [0]  [13330/60301]  eta: 6:34:33  lr: 0.000031  min_lr: 0.000001  loss: 5.1083 (5.5562)  loss_scale: 131072.0000 (112457.3472)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0014  max mem: 8161
Epoch: [0]  [13340/60301]  eta: 6:34:27  lr: 0.000031  min_lr: 0.000001  loss: 5.1038 (5.5558)  loss_scale: 131072.0000 (112471.3002)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0014  max mem: 8161
Epoch: [0]  [13350/60301]  eta: 6:34:21  lr: 0.000031  min_lr: 0.000001  loss: 5.2108 (5.5557)  loss_scale: 131072.0000 (112485.2323)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0014  max mem: 8161
Epoch: [0]  [13360/60301]  eta: 6:34:15  lr: 0.000031  min_lr: 0.000001  loss: 5.3371 (5.5555)  loss_scale: 131072.0000 (112499.1435)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0005  max mem: 8161
Epoch: [0]  [13370/60301]  eta: 6:34:09  lr: 0.000031  min_lr: 0.000001  loss: 5.4579 (5.5554)  loss_scale: 131072.0000 (112513.0339)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0005  max mem: 8161
[2023-07-14 02:11:58,221] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:11:58,222] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [13380/60301]  eta: 6:34:03  lr: 0.000031  min_lr: 0.000001  loss: 5.4427 (5.5553)  loss_scale: 131072.0000 (112585.6758)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0013  max mem: 8161
Epoch: [0]  [13390/60301]  eta: 6:33:58  lr: 0.000031  min_lr: 0.000001  loss: 5.2718 (5.5551)  loss_scale: 262144.0000 (112697.3615)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0014  max mem: 8161
[2023-07-14 02:12:05,647] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6695
[2023-07-14 02:12:05,648] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:12:05,648] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [13400/60301]  eta: 6:33:51  lr: 0.000031  min_lr: 0.000001  loss: 5.1468 (5.5548)  loss_scale: 131072.0000 (112711.0729)  weight_decay: 0.0500 (0.0500)  time: 0.4626  data: 0.0005  max mem: 8161
Epoch: [0]  [13410/60301]  eta: 6:33:45  lr: 0.000031  min_lr: 0.000001  loss: 5.0427 (5.5545)  loss_scale: 131072.0000 (112724.7639)  weight_decay: 0.0500 (0.0500)  time: 0.4623  data: 0.0005  max mem: 8161
Epoch: [0]  [13420/60301]  eta: 6:33:39  lr: 0.000031  min_lr: 0.000001  loss: 5.2985 (5.5544)  loss_scale: 131072.0000 (112738.4344)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0005  max mem: 8161
Epoch: [0]  [13430/60301]  eta: 6:33:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4144 (5.5542)  loss_scale: 131072.0000 (112752.0846)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0279  max mem: 8161
Epoch: [0]  [13440/60301]  eta: 6:33:29  lr: 0.000031  min_lr: 0.000001  loss: 5.2240 (5.5540)  loss_scale: 131072.0000 (112765.7145)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0280  max mem: 8161
Epoch: [0]  [13450/60301]  eta: 6:33:24  lr: 0.000031  min_lr: 0.000001  loss: 5.2990 (5.5540)  loss_scale: 131072.0000 (112779.3241)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0014  max mem: 8161
Epoch: [0]  [13460/60301]  eta: 6:33:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2475 (5.5536)  loss_scale: 131072.0000 (112792.9135)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0024  max mem: 8161
Epoch: [0]  [13470/60301]  eta: 6:33:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2001 (5.5535)  loss_scale: 131072.0000 (112806.4827)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0024  max mem: 8161
Epoch: [0]  [13480/60301]  eta: 6:33:07  lr: 0.000031  min_lr: 0.000001  loss: 5.1972 (5.5532)  loss_scale: 131072.0000 (112820.0317)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0014  max mem: 8161
Epoch: [0]  [13490/60301]  eta: 6:33:01  lr: 0.000031  min_lr: 0.000001  loss: 5.1667 (5.5530)  loss_scale: 131072.0000 (112833.5607)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0014  max mem: 8161
Epoch: [0]  [13500/60301]  eta: 6:32:55  lr: 0.000031  min_lr: 0.000001  loss: 5.1667 (5.5528)  loss_scale: 131072.0000 (112847.0697)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0014  max mem: 8161
Epoch: [0]  [13510/60301]  eta: 6:32:49  lr: 0.000031  min_lr: 0.000001  loss: 5.0987 (5.5526)  loss_scale: 131072.0000 (112860.5587)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0013  max mem: 8161
Epoch: [0]  [13520/60301]  eta: 6:32:44  lr: 0.000031  min_lr: 0.000001  loss: 5.0538 (5.5523)  loss_scale: 131072.0000 (112874.0277)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0031  max mem: 8161
Epoch: [0]  [13530/60301]  eta: 6:32:38  lr: 0.000031  min_lr: 0.000001  loss: 5.3619 (5.5522)  loss_scale: 131072.0000 (112887.4768)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0027  max mem: 8161
Epoch: [0]  [13540/60301]  eta: 6:32:32  lr: 0.000031  min_lr: 0.000001  loss: 5.1852 (5.5518)  loss_scale: 131072.0000 (112900.9060)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0010  max mem: 8161
Epoch: [0]  [13550/60301]  eta: 6:32:27  lr: 0.000031  min_lr: 0.000001  loss: 5.1050 (5.5515)  loss_scale: 131072.0000 (112914.3154)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0031  max mem: 8161
Epoch: [0]  [13560/60301]  eta: 6:32:15  lr: 0.000031  min_lr: 0.000001  loss: 5.3331 (5.5514)  loss_scale: 131072.0000 (112927.7050)  weight_decay: 0.0500 (0.0500)  time: 0.4074  data: 0.0244  max mem: 8161
Epoch: [0]  [13570/60301]  eta: 6:32:14  lr: 0.000031  min_lr: 0.000001  loss: 5.4533 (5.5511)  loss_scale: 131072.0000 (112941.0749)  weight_decay: 0.0500 (0.0500)  time: 0.4635  data: 0.2648  max mem: 8161
Epoch: [0]  [13580/60301]  eta: 6:32:15  lr: 0.000031  min_lr: 0.000001  loss: 5.2525 (5.5510)  loss_scale: 131072.0000 (112954.4252)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.5318  max mem: 8161
Epoch: [0]  [13590/60301]  eta: 6:32:03  lr: 0.000031  min_lr: 0.000001  loss: 5.2525 (5.5506)  loss_scale: 131072.0000 (112967.7557)  weight_decay: 0.0500 (0.0500)  time: 0.4909  data: 0.3784  max mem: 8161
Epoch: [0]  [13600/60301]  eta: 6:32:00  lr: 0.000031  min_lr: 0.000001  loss: 5.0942 (5.5504)  loss_scale: 131072.0000 (112981.0667)  weight_decay: 0.0500 (0.0500)  time: 0.4315  data: 0.1666  max mem: 8161
Epoch: [0]  [13610/60301]  eta: 6:31:54  lr: 0.000031  min_lr: 0.000001  loss: 5.0386 (5.5501)  loss_scale: 131072.0000 (112994.3581)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0777  max mem: 8161
Epoch: [0]  [13620/60301]  eta: 6:31:49  lr: 0.000031  min_lr: 0.000001  loss: 5.0825 (5.5498)  loss_scale: 131072.0000 (113007.6300)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0090  max mem: 8161
Epoch: [0]  [13630/60301]  eta: 6:31:43  lr: 0.000031  min_lr: 0.000001  loss: 5.2452 (5.5497)  loss_scale: 131072.0000 (113020.8824)  weight_decay: 0.0500 (0.0500)  time: 0.4866  data: 0.0101  max mem: 8161
Epoch: [0]  [13640/60301]  eta: 6:31:37  lr: 0.000031  min_lr: 0.000001  loss: 5.1879 (5.5493)  loss_scale: 131072.0000 (113034.1154)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0023  max mem: 8161
[2023-07-14 02:14:11,581] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:14:11,582] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [13650/60301]  eta: 6:31:31  lr: 0.000031  min_lr: 0.000001  loss: 5.0139 (5.5489)  loss_scale: 131072.0000 (113066.5323)  weight_decay: 0.0500 (0.0500)  time: 0.4789  data: 0.0014  max mem: 8161
Epoch: [0]  [13660/60301]  eta: 6:31:25  lr: 0.000031  min_lr: 0.000001  loss: 5.1845 (5.5486)  loss_scale: 262144.0000 (113175.6586)  weight_decay: 0.0500 (0.0500)  time: 0.4792  data: 0.0024  max mem: 8161
Epoch: [0]  [13670/60301]  eta: 6:31:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3697 (5.5486)  loss_scale: 262144.0000 (113284.6253)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0278  max mem: 8161
Epoch: [0]  [13680/60301]  eta: 6:31:16  lr: 0.000031  min_lr: 0.000001  loss: 5.4789 (5.5485)  loss_scale: 262144.0000 (113393.4326)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0347  max mem: 8161
[2023-07-14 02:14:31,153] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6844
[2023-07-14 02:14:31,153] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:14:31,153] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [13690/60301]  eta: 6:31:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3416 (5.5482)  loss_scale: 262144.0000 (113482.9339)  weight_decay: 0.0500 (0.0500)  time: 0.4679  data: 0.0082  max mem: 8161
Epoch: [0]  [13700/60301]  eta: 6:31:03  lr: 0.000031  min_lr: 0.000001  loss: 5.3528 (5.5482)  loss_scale: 131072.0000 (113495.7717)  weight_decay: 0.0500 (0.0500)  time: 0.4598  data: 0.0004  max mem: 8161
Epoch: [0]  [13710/60301]  eta: 6:30:58  lr: 0.000031  min_lr: 0.000001  loss: 5.4117 (5.5480)  loss_scale: 131072.0000 (113508.5908)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0143  max mem: 8161
Epoch: [0]  [13720/60301]  eta: 6:30:52  lr: 0.000031  min_lr: 0.000001  loss: 5.2664 (5.5478)  loss_scale: 131072.0000 (113521.3912)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0142  max mem: 8161
Epoch: [0]  [13730/60301]  eta: 6:30:46  lr: 0.000031  min_lr: 0.000001  loss: 5.2762 (5.5476)  loss_scale: 131072.0000 (113534.1729)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0004  max mem: 8161
Epoch: [0]  [13740/60301]  eta: 6:30:40  lr: 0.000031  min_lr: 0.000001  loss: 5.4379 (5.5474)  loss_scale: 131072.0000 (113546.9360)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0004  max mem: 8161
Epoch: [0]  [13750/60301]  eta: 6:30:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4379 (5.5473)  loss_scale: 131072.0000 (113559.6806)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0006  max mem: 8161
Epoch: [0]  [13760/60301]  eta: 6:30:33  lr: 0.000031  min_lr: 0.000001  loss: 5.4003 (5.5472)  loss_scale: 131072.0000 (113572.4067)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0668  max mem: 8161
Epoch: [0]  [13770/60301]  eta: 6:30:27  lr: 0.000031  min_lr: 0.000001  loss: 5.1904 (5.5469)  loss_scale: 131072.0000 (113585.1142)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0667  max mem: 8161
Epoch: [0]  [13780/60301]  eta: 6:30:24  lr: 0.000031  min_lr: 0.000001  loss: 5.1996 (5.5468)  loss_scale: 131072.0000 (113597.8034)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0335  max mem: 8161
Epoch: [0]  [13790/60301]  eta: 6:30:18  lr: 0.000031  min_lr: 0.000001  loss: 5.2750 (5.5466)  loss_scale: 131072.0000 (113610.4741)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0336  max mem: 8161
Epoch: [0]  [13800/60301]  eta: 6:30:12  lr: 0.000031  min_lr: 0.000001  loss: 5.2730 (5.5463)  loss_scale: 131072.0000 (113623.1264)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0006  max mem: 8161
Epoch: [0]  [13810/60301]  eta: 6:30:06  lr: 0.000031  min_lr: 0.000001  loss: 5.1775 (5.5461)  loss_scale: 131072.0000 (113635.7605)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0015  max mem: 8161
Epoch: [0]  [13820/60301]  eta: 6:30:01  lr: 0.000031  min_lr: 0.000001  loss: 4.9639 (5.5456)  loss_scale: 131072.0000 (113648.3762)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0023  max mem: 8161
Epoch: [0]  [13830/60301]  eta: 6:29:56  lr: 0.000031  min_lr: 0.000001  loss: 5.2266 (5.5454)  loss_scale: 131072.0000 (113660.9738)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0120  max mem: 8161
Epoch: [0]  [13840/60301]  eta: 6:29:56  lr: 0.000031  min_lr: 0.000001  loss: 5.2913 (5.5453)  loss_scale: 131072.0000 (113673.5531)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1061  max mem: 8161
Epoch: [0]  [13850/60301]  eta: 6:29:52  lr: 0.000031  min_lr: 0.000001  loss: 5.1407 (5.5449)  loss_scale: 131072.0000 (113686.1142)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1131  max mem: 8161
Epoch: [0]  [13860/60301]  eta: 6:29:46  lr: 0.000031  min_lr: 0.000001  loss: 5.1115 (5.5448)  loss_scale: 131072.0000 (113698.6572)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0273  max mem: 8161
Epoch: [0]  [13870/60301]  eta: 6:29:40  lr: 0.000031  min_lr: 0.000001  loss: 5.4126 (5.5446)  loss_scale: 131072.0000 (113711.1822)  weight_decay: 0.0500 (0.0500)  time: 0.4872  data: 0.0096  max mem: 8161
Epoch: [0]  [13880/60301]  eta: 6:29:35  lr: 0.000031  min_lr: 0.000001  loss: 5.4126 (5.5446)  loss_scale: 131072.0000 (113723.6891)  weight_decay: 0.0500 (0.0500)  time: 0.4800  data: 0.0005  max mem: 8161
Epoch: [0]  [13890/60301]  eta: 6:29:29  lr: 0.000031  min_lr: 0.000001  loss: 5.3746 (5.5444)  loss_scale: 131072.0000 (113736.1780)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0005  max mem: 8161
Epoch: [0]  [13900/60301]  eta: 6:29:32  lr: 0.000031  min_lr: 0.000001  loss: 5.0562 (5.5442)  loss_scale: 131072.0000 (113748.6489)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.2447  max mem: 8161
Epoch: [0]  [13910/60301]  eta: 6:29:29  lr: 0.000031  min_lr: 0.000001  loss: 5.1434 (5.5439)  loss_scale: 131072.0000 (113761.1019)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.4692  max mem: 8161
Epoch: [0]  [13920/60301]  eta: 6:29:21  lr: 0.000031  min_lr: 0.000001  loss: 5.3066 (5.5438)  loss_scale: 131072.0000 (113773.5370)  weight_decay: 0.0500 (0.0500)  time: 0.4875  data: 0.3766  max mem: 8161
Epoch: [0]  [13930/60301]  eta: 6:29:24  lr: 0.000031  min_lr: 0.000001  loss: 5.3214 (5.5436)  loss_scale: 131072.0000 (113785.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.3954  max mem: 8161
Epoch: [0]  [13940/60301]  eta: 6:29:20  lr: 0.000031  min_lr: 0.000001  loss: 5.2460 (5.5433)  loss_scale: 131072.0000 (113798.3536)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.2729  max mem: 8161
[2023-07-14 02:16:46,132] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:16:46,141] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [13950/60301]  eta: 6:29:14  lr: 0.000031  min_lr: 0.000001  loss: 5.0791 (5.5430)  loss_scale: 131072.0000 (113848.3160)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0316  max mem: 8161
[2023-07-14 02:16:48,699] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6976
[2023-07-14 02:16:48,700] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:16:48,700] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [13960/60301]  eta: 6:29:07  lr: 0.000031  min_lr: 0.000001  loss: 5.1636 (5.5430)  loss_scale: 131072.0000 (113879.4298)  weight_decay: 0.0500 (0.0500)  time: 0.4611  data: 0.0024  max mem: 8161
Epoch: [0]  [13970/60301]  eta: 6:29:01  lr: 0.000031  min_lr: 0.000001  loss: 5.4646 (5.5429)  loss_scale: 131072.0000 (113891.7357)  weight_decay: 0.0500 (0.0500)  time: 0.4610  data: 0.0014  max mem: 8161
Epoch: [0]  [13980/60301]  eta: 6:28:56  lr: 0.000031  min_lr: 0.000001  loss: 5.2130 (5.5426)  loss_scale: 131072.0000 (113904.0240)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0019  max mem: 8161
Epoch: [0]  [13990/60301]  eta: 6:28:50  lr: 0.000031  min_lr: 0.000001  loss: 5.2130 (5.5425)  loss_scale: 131072.0000 (113916.2948)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0010  max mem: 8161
[2023-07-14 02:17:14,352] [INFO] [logging.py:69:log_dist] [Rank 0] step=7000, skipped=40, lr=[7.424145005643368e-07, 7.424145005643368e-07, 9.89886000752449e-07, 9.89886000752449e-07, 1.3198480010032653e-06, 1.3198480010032653e-06, 1.7597973346710205e-06, 1.7597973346710205e-06, 2.3463964462280273e-06, 2.3463964462280273e-06, 3.128528594970703e-06, 3.128528594970703e-06, 4.171371459960937e-06, 4.171371459960937e-06, 5.56182861328125e-06, 5.56182861328125e-06, 7.415771484375e-06, 7.415771484375e-06, 9.8876953125e-06, 9.8876953125e-06, 1.318359375e-05, 1.318359375e-05, 1.7578125000000002e-05, 1.7578125000000002e-05, 2.34375e-05, 2.34375e-05, 3.125e-05, 3.125e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-14 02:17:14,355] [INFO] [timer.py:181:stop] 0/14000, SamplesPerSec=9.387766073952424
Epoch: [0]  [14000/60301]  eta: 6:28:56  lr: 0.000031  min_lr: 0.000001  loss: 5.3489 (5.5424)  loss_scale: 131072.0000 (113928.5480)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.1763  max mem: 8161
Epoch: [0]  [14010/60301]  eta: 6:28:53  lr: 0.000031  min_lr: 0.000001  loss: 5.2743 (5.5421)  loss_scale: 131072.0000 (113940.7837)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.2205  max mem: 8161
Epoch: [0]  [14020/60301]  eta: 6:28:48  lr: 0.000031  min_lr: 0.000001  loss: 5.0018 (5.5417)  loss_scale: 131072.0000 (113953.0019)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0535  max mem: 8161
Epoch: [0]  [14030/60301]  eta: 6:28:42  lr: 0.000031  min_lr: 0.000001  loss: 5.2152 (5.5416)  loss_scale: 131072.0000 (113965.2028)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0162  max mem: 8161
Epoch: [0]  [14040/60301]  eta: 6:28:36  lr: 0.000031  min_lr: 0.000001  loss: 5.4147 (5.5414)  loss_scale: 131072.0000 (113977.3862)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0074  max mem: 8161
Epoch: [0]  [14050/60301]  eta: 6:28:31  lr: 0.000031  min_lr: 0.000001  loss: 5.4799 (5.5414)  loss_scale: 131072.0000 (113989.5523)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0015  max mem: 8161
Epoch: [0]  [14060/60301]  eta: 6:28:25  lr: 0.000031  min_lr: 0.000001  loss: 5.1106 (5.5410)  loss_scale: 131072.0000 (114001.7012)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0015  max mem: 8161
Epoch: [0]  [14070/60301]  eta: 6:28:19  lr: 0.000031  min_lr: 0.000001  loss: 5.2207 (5.5409)  loss_scale: 131072.0000 (114013.8327)  weight_decay: 0.0500 (0.0500)  time: 0.4817  data: 0.0005  max mem: 8161
Epoch: [0]  [14080/60301]  eta: 6:28:13  lr: 0.000031  min_lr: 0.000001  loss: 5.2785 (5.5408)  loss_scale: 131072.0000 (114025.9470)  weight_decay: 0.0500 (0.0500)  time: 0.4809  data: 0.0005  max mem: 8161
Epoch: [0]  [14090/60301]  eta: 6:28:08  lr: 0.000031  min_lr: 0.000001  loss: 5.2187 (5.5407)  loss_scale: 131072.0000 (114038.0441)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0012  max mem: 8161
Epoch: [0]  [14100/60301]  eta: 6:27:55  lr: 0.000031  min_lr: 0.000001  loss: 5.1600 (5.5404)  loss_scale: 131072.0000 (114050.1241)  weight_decay: 0.0500 (0.0500)  time: 0.3707  data: 0.0712  max mem: 8161
Epoch: [0]  [14110/60301]  eta: 6:27:49  lr: 0.000031  min_lr: 0.000001  loss: 4.9973 (5.5400)  loss_scale: 131072.0000 (114062.1869)  weight_decay: 0.0500 (0.0500)  time: 0.3698  data: 0.2554  max mem: 8161
Epoch: [0]  [14120/60301]  eta: 6:27:41  lr: 0.000031  min_lr: 0.000001  loss: 5.0477 (5.5397)  loss_scale: 131072.0000 (114074.2327)  weight_decay: 0.0500 (0.0500)  time: 0.4502  data: 0.3372  max mem: 8161
Epoch: [0]  [14130/60301]  eta: 6:27:56  lr: 0.000031  min_lr: 0.000001  loss: 5.2115 (5.5394)  loss_scale: 131072.0000 (114086.2614)  weight_decay: 0.0500 (0.0500)  time: 0.7624  data: 0.5762  max mem: 8161
Epoch: [0]  [14140/60301]  eta: 6:27:53  lr: 0.000031  min_lr: 0.000001  loss: 5.2372 (5.5392)  loss_scale: 131072.0000 (114098.2731)  weight_decay: 0.0500 (0.0500)  time: 0.8365  data: 0.4675  max mem: 8161
Epoch: [0]  [14150/60301]  eta: 6:27:48  lr: 0.000031  min_lr: 0.000001  loss: 5.2430 (5.5390)  loss_scale: 131072.0000 (114110.2678)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0658  max mem: 8161
Epoch: [0]  [14160/60301]  eta: 6:27:42  lr: 0.000031  min_lr: 0.000001  loss: 5.2596 (5.5388)  loss_scale: 131072.0000 (114122.2456)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0226  max mem: 8161
Epoch: [0]  [14170/60301]  eta: 6:27:37  lr: 0.000031  min_lr: 0.000001  loss: 5.3086 (5.5386)  loss_scale: 131072.0000 (114134.2065)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0005  max mem: 8161
Epoch: [0]  [14180/60301]  eta: 6:27:31  lr: 0.000031  min_lr: 0.000001  loss: 5.3268 (5.5385)  loss_scale: 131072.0000 (114146.1505)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0013  max mem: 8161
Epoch: [0]  [14190/60301]  eta: 6:27:38  lr: 0.000031  min_lr: 0.000001  loss: 5.3633 (5.5384)  loss_scale: 131072.0000 (114158.0777)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.1913  max mem: 8161
Epoch: [0]  [14200/60301]  eta: 6:27:32  lr: 0.000031  min_lr: 0.000001  loss: 5.4365 (5.5383)  loss_scale: 131072.0000 (114169.9880)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.1905  max mem: 8161
Epoch: [0]  [14210/60301]  eta: 6:27:26  lr: 0.000031  min_lr: 0.000001  loss: 5.3784 (5.5381)  loss_scale: 131072.0000 (114181.8816)  weight_decay: 0.0500 (0.0500)  time: 0.4821  data: 0.0005  max mem: 8161
[2023-07-14 02:19:06,110] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-14 02:19:06,110] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [14220/60301]  eta: 6:27:20  lr: 0.000031  min_lr: 0.000001  loss: 5.0299 (5.5377)  loss_scale: 131072.0000 (114285.9264)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0015  max mem: 8161
Epoch: [0]  [14230/60301]  eta: 6:27:14  lr: 0.000031  min_lr: 0.000001  loss: 5.3032 (5.5376)  loss_scale: 262144.0000 (114389.8250)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0015  max mem: 8161
Epoch: [0]  [14240/60301]  eta: 6:27:09  lr: 0.000031  min_lr: 0.000001  loss: 5.3083 (5.5373)  loss_scale: 262144.0000 (114493.5777)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0004  max mem: 8161
[2023-07-14 02:19:20,282] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7120
[2023-07-14 02:19:20,282] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2023-07-14 02:19:20,283] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [14250/60301]  eta: 6:27:02  lr: 0.000031  min_lr: 0.000001  loss: 5.2006 (5.5371)  loss_scale: 131072.0000 (114505.2109)  weight_decay: 0.0500 (0.0500)  time: 0.4615  data: 0.0004  max mem: 8161
Epoch: [0]  [14260/60301]  eta: 6:26:56  lr: 0.000031  min_lr: 0.000001  loss: 5.1886 (5.5369)  loss_scale: 131072.0000 (114516.8277)  weight_decay: 0.0500 (0.0500)  time: 0.4688  data: 0.0069  max mem: 8161
Epoch: [0]  [14270/60301]  eta: 6:26:50  lr: 0.000031  min_lr: 0.000001  loss: 5.2658 (5.5368)  loss_scale: 131072.0000 (114528.4283)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0069  max mem: 8161
Epoch: [0]  [14280/60301]  eta: 6:26:45  lr: 0.000031  min_lr: 0.000001  loss: 5.3308 (5.5366)  loss_scale: 131072.0000 (114540.0126)  weight_decay: 0.0500 (0.0500)  time: 0.4820  data: 0.0012  max mem: 8161
Epoch: [0]  [14290/60301]  eta: 6:26:39  lr: 0.000031  min_lr: 0.000001  loss: 5.3560 (5.5364)  loss_scale: 131072.0000 (114551.5807)  weight_decay: 0.0500 (0.0500)  time: 0.4829  data: 0.0020  max mem: 8161
Epoch: [0]  [14300/60301]  eta: 6:26:33  lr: 0.000031  min_lr: 0.000001  loss: 5.3560 (5.5362)  loss_scale: 131072.0000 (114563.1326)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0012  max mem: 8161
Epoch: [0]  [14310/60301]  eta: 6:26:27  lr: 0.000031  min_lr: 0.000001  loss: 5.1044 (5.5359)  loss_scale: 131072.0000 (114574.6684)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0004  max mem: 8161
Epoch: [0]  [14320/60301]  eta: 6:26:22  lr: 0.000031  min_lr: 0.000001  loss: 5.1178 (5.5357)  loss_scale: 131072.0000 (114586.1881)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0013  max mem: 8161
Epoch: [0]  [14330/60301]  eta: 6:26:16  lr: 0.000031  min_lr: 0.000001  loss: 5.3357 (5.5355)  loss_scale: 131072.0000 (114597.6917)  weight_decay: 0.0500 (0.0500)  time: 0.4849  data: 0.0013  max mem: 8161
Epoch: [0]  [14340/60301]  eta: 6:26:10  lr: 0.000031  min_lr: 0.000001  loss: 5.3171 (5.5353)  loss_scale: 131072.0000 (114609.1793)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0005  max mem: 8161
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "run_class_finetuning.py", line 587, in <module>
    main(opts, ds_init)
  File "run_class_finetuning.py", line 522, in main
    train_stats = train_one_epoch(
  File "/models/mvd/engine_for_finetuning.py", line 104, in train_one_epoch
    model.step()
  File "/root/myenv/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1913, in step
    self.tput_timer.stop(report_progress)
  File "/root/myenv/lib/python3.8/site-packages/deepspeed/utils/timer.py", line 175, in stop
    torch.cuda.synchronize()
  File "/root/myenv/lib/python3.8/site-packages/torch/cuda/__init__.py", line 493, in synchronize
    return torch._C._cuda_synchronize()
  File "/root/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2562146) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2561846) of binary: /root/myenv/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_class_finetuning.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-14_02:20:15
  host      : facfbba3a31b
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2561846)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
