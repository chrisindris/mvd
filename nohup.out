| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=3, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/data/i5O/kinetics-dataset/annotations', data_root='/data/i5O/kinetics400/train/', data_set='Kinetics-400', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=30, eval=False, eval_data_path=None, eval_log_name='log_eval', fc_drop_rate=0.0, finetune='/data/i5O/finetuned/july24/checkpoint-24/mp_rank_00_model_states.pt', gpu=0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', lr=0.001, merge_test=False, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_small_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=400, no_save_best_ckpt=False, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', pin_mem=True, rank=0, recount=1, remode='pixel', remove_pos_emb=False, reprob=0.25, resplit=False, resume='', resume_best=False, sampling_rate=4, save_ckpt=True, save_ckpt_freq=1, seed=0, short_side_size=224, smoothing=0.1, start_epoch=25, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=2, use_checkpoint=False, use_cls_token=False, use_mean_pooling=True, warmup_epochs=0, warmup_lr=0.001, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=2)
Patch size = (16, 16)
                                                        0    1
0       /data/i5O/kinetics400/train/-3B32lodo2M_000059...    0
1       /data/i5O/kinetics400/train/-7kbO0v4hag_000107...    0
2       /data/i5O/kinetics400/train/-bwYZwnwb8E_000013...    0
3       /data/i5O/kinetics400/train/-Cv3NwxG_8g_000087...    0
4       /data/i5O/kinetics400/train/-hLv_HL6UhY_000151...    0
...                                                   ...  ...
241202  /data/i5O/kinetics400/train/_GRX1r0JV30_000039...  399
241203  /data/i5O/kinetics400/train/_JuIL8GGX0A_000150...  399
241204  /data/i5O/kinetics400/train/_Jun6C7ICps_000037...  399
241205  /data/i5O/kinetics400/train/_SyAhrLns4k_000200...  399
241206  /data/i5O/kinetics400/train/_URmIF4eHg4_000014...  399

[241207 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7c68164e20>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Load ckpt from /data/i5O/finetuned/july24/checkpoint-24/mp_rank_00_model_states.pt
Load state_dict by model_key = module
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=400, bias=True)
)
number of params: 22033936
LR = 0.00004688
Batch size = 12
Update frequent = 2
Number of training examples = 241207
Number of training training per epoch = 20100
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-07-24 15:14:50,906] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.8, git-hash=unknown, git-branch=unknown
[2023-07-24 15:14:50,919] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups
[2023-07-24 15:14:50,919] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1
[2023-07-24 15:14:52,079] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1
[2023-07-24 15:14:52,079] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0, 1]
[2023-07-24 15:14:52,090] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]
[2023-07-24 15:14:52,090] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [1]
[2023-07-24 15:14:52,379] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.4 does not match the version torch was compiled with 11.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py38_cu111/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.590907335281372 seconds
[2023-07-24 15:14:55,552] [INFO] [engine.py:1108:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-07-24 15:14:55,559] [INFO] [engine.py:1116:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2023-07-24 15:14:55,559] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-07-24 15:14:55,659] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-07-24 15:14:55,660] [INFO] [engine.py:808:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-07-24 15:14:55,660] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-07-24 15:14:55,660] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 15:14:55,660] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2023-07-24 15:14:55,661] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-24 15:14:55,661] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-24 15:14:55,661] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2023-07-24 15:14:55,661] [INFO] [config.py:1063:print]   amp_params ................... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   dump_state ................... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'min_scale': 1}
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2023-07-24 15:14:55,662] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   global_rank .................. 0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 2
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 128
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   optimizer_name ............... adam
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   pld_params ................... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   steps_per_print .............. 1000
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   train_batch_size ............. 12
[2023-07-24 15:14:55,663] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  3
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   world_size ................... 2
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 0, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   zero_enabled ................. False
[2023-07-24 15:14:55,664] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0
[2023-07-24 15:14:55,664] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 3, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.6095004081726074 seconds
model.gradient_accumulation_steps() = 2
Use step level LR scheduler!
Set warmup steps = 0
Set warmup steps = 0
len(lr_schedule_values) = 603000
np.unique(lr_schedule_values) = [4.68750003e-08 4.68750013e-08 4.68750029e-08 ... 4.68750000e-05
 4.68750000e-05 4.68750000e-05]
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 30 epochs
Epoch: [25]  [    0/40201]  eta: 2 days, 7:05:21  lr: 0.000003  min_lr: 0.000000  loss: 3.9815 (3.9815)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 4.9332  data: 4.5450  max mem: 6151
Epoch: [25]  [   10/40201]  eta: 14:00:29  lr: 0.000003  min_lr: 0.000000  loss: 3.6472 (3.6143)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2547  data: 0.4139  max mem: 6185
Epoch: [25]  [   20/40201]  eta: 11:29:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2005 (3.5433)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8347  data: 0.0012  max mem: 6186
Epoch: [25]  [   30/40201]  eta: 10:32:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6075 (3.7021)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7748  data: 0.0012  max mem: 6186
Epoch: [25]  [   40/40201]  eta: 10:04:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4032 (3.5916)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7713  data: 0.0007  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/SZtj2TEWiHc_000195_000205.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [   50/40201]  eta: 9:41:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2080 (3.5188)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.0006  max mem: 6186
Epoch: [25]  [   60/40201]  eta: 9:25:44  lr: 0.000003  min_lr: 0.000000  loss: 2.8320 (3.4240)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7265  data: 0.0014  max mem: 6186
Epoch: [25]  [   70/40201]  eta: 9:13:03  lr: 0.000003  min_lr: 0.000000  loss: 2.8320 (3.3889)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7206  data: 0.0019  max mem: 6186
Epoch: [25]  [   80/40201]  eta: 9:09:08  lr: 0.000003  min_lr: 0.000000  loss: 3.1907 (3.4301)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7468  data: 0.0089  max mem: 6186
Epoch: [25]  [   90/40201]  eta: 9:03:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7149 (3.4382)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7637  data: 0.0082  max mem: 6186
Epoch: [25]  [  100/40201]  eta: 8:56:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4950 (3.4387)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7265  data: 0.0005  max mem: 6186
Epoch: [25]  [  110/40201]  eta: 8:52:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3671 (3.4760)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7230  data: 0.0006  max mem: 6186
Epoch: [25]  [  120/40201]  eta: 8:46:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3671 (3.4574)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.0011  max mem: 6186
Epoch: [25]  [  130/40201]  eta: 8:43:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3639 (3.4698)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.0015  max mem: 6186
Epoch: [25]  [  140/40201]  eta: 8:39:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2299 (3.4555)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.0020  max mem: 6186
Epoch: [25]  [  150/40201]  eta: 8:39:02  lr: 0.000003  min_lr: 0.000000  loss: 3.1038 (3.4485)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7341  data: 0.0017  max mem: 6186
Epoch: [25]  [  160/40201]  eta: 8:36:48  lr: 0.000003  min_lr: 0.000000  loss: 3.3295 (3.4740)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7483  data: 0.0008  max mem: 6186
Epoch: [25]  [  170/40201]  eta: 8:35:00  lr: 0.000003  min_lr: 0.000000  loss: 3.3295 (3.4692)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7290  data: 0.0007  max mem: 6186
Epoch: [25]  [  180/40201]  eta: 8:33:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1544 (3.4611)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7359  data: 0.0007  max mem: 6186
Epoch: [25]  [  190/40201]  eta: 8:28:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4563 (3.4746)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6889  data: 0.0010  max mem: 6186
Epoch: [25]  [  200/40201]  eta: 8:26:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5413 (3.4712)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.0016  max mem: 6186
Epoch: [25]  [  210/40201]  eta: 8:23:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6576 (3.4853)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6816  data: 0.0013  max mem: 6186
Epoch: [25]  [  220/40201]  eta: 8:20:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4043 (3.4665)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0019  max mem: 6186
Epoch: [25]  [  230/40201]  eta: 8:17:36  lr: 0.000003  min_lr: 0.000000  loss: 2.8792 (3.4414)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0017  max mem: 6186
Epoch: [25]  [  240/40201]  eta: 8:14:04  lr: 0.000003  min_lr: 0.000000  loss: 2.8480 (3.4375)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0009  max mem: 6186
Epoch: [25]  [  250/40201]  eta: 8:09:34  lr: 0.000003  min_lr: 0.000000  loss: 3.3456 (3.4252)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0012  max mem: 6186
[2023-07-24 15:18:09,120] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:18:09,120] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-07-24 15:18:09,120] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:18:09,121] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [25]  [  260/40201]  eta: 8:03:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2636 (3.4256)  loss_scale: 128.0000 (129.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0010  max mem: 6186
Epoch: [25]  [  270/40201]  eta: 7:58:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2636 (3.4270)  loss_scale: 256.0000 (134.6125)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0006  max mem: 6186
Epoch: [25]  [  280/40201]  eta: 7:53:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4740 (3.4335)  loss_scale: 256.0000 (138.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0005  max mem: 6186
Epoch: [25]  [  290/40201]  eta: 7:48:38  lr: 0.000003  min_lr: 0.000000  loss: 3.7099 (3.4466)  loss_scale: 256.0000 (142.9553)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0005  max mem: 6186
Epoch: [25]  [  300/40201]  eta: 7:43:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6047 (3.4425)  loss_scale: 256.0000 (146.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0004  max mem: 6186
Epoch: [25]  [  310/40201]  eta: 7:39:31  lr: 0.000003  min_lr: 0.000000  loss: 3.5283 (3.4388)  loss_scale: 256.0000 (150.2251)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0004  max mem: 6186
Epoch: [25]  [  320/40201]  eta: 7:35:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5859 (3.4540)  loss_scale: 256.0000 (153.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0004  max mem: 6186
Epoch: [25]  [  330/40201]  eta: 7:32:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4406 (3.4488)  loss_scale: 256.0000 (156.6163)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0004  max mem: 6186
Epoch: [25]  [  340/40201]  eta: 7:28:28  lr: 0.000003  min_lr: 0.000000  loss: 3.6151 (3.4579)  loss_scale: 256.0000 (159.5308)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0004  max mem: 6186
Epoch: [25]  [  350/40201]  eta: 7:24:45  lr: 0.000003  min_lr: 0.000000  loss: 3.8497 (3.4671)  loss_scale: 256.0000 (162.2792)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0004  max mem: 6186
Epoch: [25]  [  360/40201]  eta: 7:15:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5492 (3.4739)  loss_scale: 256.0000 (164.8753)  weight_decay: 0.0500 (0.0500)  time: 0.3304  data: 0.0004  max mem: 6186
Epoch: [25]  [  370/40201]  eta: 7:07:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6449 (3.4795)  loss_scale: 256.0000 (167.3315)  weight_decay: 0.0500 (0.0500)  time: 0.1884  data: 0.0071  max mem: 6186
Epoch: [25]  [  380/40201]  eta: 7:01:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7479 (3.4952)  loss_scale: 256.0000 (169.6588)  weight_decay: 0.0500 (0.0500)  time: 0.2474  data: 0.0391  max mem: 6186
Epoch: [25]  [  390/40201]  eta: 6:56:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4520 (3.4831)  loss_scale: 256.0000 (171.8670)  weight_decay: 0.0500 (0.0500)  time: 0.3358  data: 0.0380  max mem: 6186
Epoch: [25]  [  400/40201]  eta: 6:51:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1977 (3.4870)  loss_scale: 256.0000 (173.9651)  weight_decay: 0.0500 (0.0500)  time: 0.3575  data: 0.0081  max mem: 6186
Epoch: [25]  [  410/40201]  eta: 6:45:52  lr: 0.000003  min_lr: 0.000000  loss: 3.4408 (3.4889)  loss_scale: 256.0000 (175.9611)  weight_decay: 0.0500 (0.0500)  time: 0.3009  data: 0.0026  max mem: 6186
Epoch: [25]  [  420/40201]  eta: 6:41:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4412 (3.4970)  loss_scale: 256.0000 (177.8622)  weight_decay: 0.0500 (0.0500)  time: 0.3012  data: 0.0276  max mem: 6186
Epoch: [25]  [  430/40201]  eta: 6:39:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5088 (3.4954)  loss_scale: 256.0000 (179.6752)  weight_decay: 0.0500 (0.0500)  time: 0.4240  data: 0.0284  max mem: 6186
Epoch: [25]  [  440/40201]  eta: 6:38:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3064 (3.4944)  loss_scale: 256.0000 (181.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0013  max mem: 6186
Epoch: [25]  [  450/40201]  eta: 6:36:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3064 (3.4915)  loss_scale: 256.0000 (183.0599)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0004  max mem: 6186
Epoch: [25]  [  460/40201]  eta: 6:35:02  lr: 0.000003  min_lr: 0.000000  loss: 3.3736 (3.4845)  loss_scale: 256.0000 (184.6421)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0004  max mem: 6186
Epoch: [25]  [  470/40201]  eta: 6:33:34  lr: 0.000003  min_lr: 0.000000  loss: 3.1806 (3.4791)  loss_scale: 256.0000 (186.1571)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0011  max mem: 6186
Epoch: [25]  [  480/40201]  eta: 6:32:01  lr: 0.000003  min_lr: 0.000000  loss: 3.1806 (3.4737)  loss_scale: 256.0000 (187.6091)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0011  max mem: 6186
Epoch: [25]  [  490/40201]  eta: 6:30:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4115 (3.4812)  loss_scale: 256.0000 (189.0020)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0004  max mem: 6186
Epoch: [25]  [  500/40201]  eta: 6:29:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4320 (3.4792)  loss_scale: 256.0000 (190.3393)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
Epoch: [25]  [  510/40201]  eta: 6:28:14  lr: 0.000003  min_lr: 0.000000  loss: 3.1507 (3.4825)  loss_scale: 256.0000 (191.6243)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0004  max mem: 6186
[2023-07-24 15:20:02,387] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:20:02,387] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-07-24 15:20:02,408] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:20:02,408] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [25]  [  520/40201]  eta: 6:26:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4652 (3.4852)  loss_scale: 256.0000 (196.7908)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0004  max mem: 6186
Epoch: [25]  [  530/40201]  eta: 6:25:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2013 (3.4753)  loss_scale: 512.0000 (202.7269)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0005  max mem: 6186
Epoch: [25]  [  540/40201]  eta: 6:24:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0486 (3.4717)  loss_scale: 512.0000 (208.4436)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0005  max mem: 6186
Epoch: [25]  [  550/40201]  eta: 6:23:31  lr: 0.000003  min_lr: 0.000000  loss: 3.2828 (3.4818)  loss_scale: 512.0000 (213.9528)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0005  max mem: 6186
Epoch: [25]  [  560/40201]  eta: 6:22:25  lr: 0.000003  min_lr: 0.000000  loss: 3.7389 (3.4870)  loss_scale: 512.0000 (219.2656)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0004  max mem: 6186
Epoch: [25]  [  570/40201]  eta: 6:21:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7278 (3.4922)  loss_scale: 512.0000 (224.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0008  max mem: 6186
Epoch: [25]  [  580/40201]  eta: 6:20:34  lr: 0.000003  min_lr: 0.000000  loss: 3.3726 (3.4848)  loss_scale: 512.0000 (229.3425)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0008  max mem: 6186
Epoch: [25]  [  590/40201]  eta: 6:19:34  lr: 0.000003  min_lr: 0.000000  loss: 3.0932 (3.4812)  loss_scale: 512.0000 (234.1252)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0005  max mem: 6186
Epoch: [25]  [  600/40201]  eta: 6:18:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2225 (3.4829)  loss_scale: 512.0000 (238.7488)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0005  max mem: 6186
Epoch: [25]  [  610/40201]  eta: 6:17:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2198 (3.4789)  loss_scale: 512.0000 (243.2209)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0005  max mem: 6186
Epoch: [25]  [  620/40201]  eta: 6:17:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1308 (3.4767)  loss_scale: 512.0000 (247.5491)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0004  max mem: 6186
Epoch: [25]  [  630/40201]  eta: 6:16:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2119 (3.4746)  loss_scale: 512.0000 (251.7401)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0005  max mem: 6186
Epoch: [25]  [  640/40201]  eta: 6:16:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3637 (3.4751)  loss_scale: 512.0000 (255.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0009  max mem: 6186
Epoch: [25]  [  650/40201]  eta: 6:16:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3740 (3.4744)  loss_scale: 512.0000 (259.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0010  max mem: 6186
Epoch: [25]  [  660/40201]  eta: 6:16:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5242 (3.4733)  loss_scale: 512.0000 (263.5522)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0006  max mem: 6186
Epoch: [25]  [  670/40201]  eta: 6:17:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2585 (3.4686)  loss_scale: 512.0000 (267.2548)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0628  max mem: 6186
Epoch: [25]  [  680/40201]  eta: 6:17:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0339 (3.4624)  loss_scale: 512.0000 (270.8488)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.0630  max mem: 6186
Epoch: [25]  [  690/40201]  eta: 6:17:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2115 (3.4620)  loss_scale: 512.0000 (274.3386)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0007  max mem: 6186
Epoch: [25]  [  700/40201]  eta: 6:18:36  lr: 0.000003  min_lr: 0.000000  loss: 3.3389 (3.4638)  loss_scale: 512.0000 (277.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.0008  max mem: 6186
Epoch: [25]  [  710/40201]  eta: 6:18:32  lr: 0.000003  min_lr: 0.000000  loss: 3.7320 (3.4675)  loss_scale: 512.0000 (281.0239)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0010  max mem: 6186
Epoch: [25]  [  720/40201]  eta: 6:17:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3316 (3.4663)  loss_scale: 512.0000 (284.2275)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0010  max mem: 6186
Epoch: [25]  [  730/40201]  eta: 6:14:29  lr: 0.000003  min_lr: 0.000000  loss: 3.2537 (3.4674)  loss_scale: 512.0000 (287.3434)  weight_decay: 0.0500 (0.0500)  time: 0.3610  data: 0.0008  max mem: 6186
Epoch: [25]  [  740/40201]  eta: 6:12:36  lr: 0.000003  min_lr: 0.000000  loss: 3.5021 (3.4708)  loss_scale: 512.0000 (290.3752)  weight_decay: 0.0500 (0.0500)  time: 0.2868  data: 0.0699  max mem: 6186
Epoch: [25]  [  750/40201]  eta: 6:11:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5021 (3.4690)  loss_scale: 512.0000 (293.3262)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.1729  max mem: 6186
Epoch: [25]  [  760/40201]  eta: 6:10:02  lr: 0.000003  min_lr: 0.000000  loss: 3.5825 (3.4687)  loss_scale: 512.0000 (296.1997)  weight_decay: 0.0500 (0.0500)  time: 0.4290  data: 0.2242  max mem: 6186
[2023-07-24 15:22:13,336] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:22:13,336] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-07-24 15:22:13,341] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:22:13,341] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [25]  [  770/40201]  eta: 6:08:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2780 (3.4677)  loss_scale: 512.0000 (300.3268)  weight_decay: 0.0500 (0.0500)  time: 0.4409  data: 0.2299  max mem: 6186
Epoch: [25]  [  780/40201]  eta: 6:07:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2437 (3.4679)  loss_scale: 1024.0000 (309.5928)  weight_decay: 0.0500 (0.0500)  time: 0.3994  data: 0.1861  max mem: 6186
Epoch: [25]  [  790/40201]  eta: 6:13:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2437 (3.4716)  loss_scale: 1024.0000 (318.6245)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.4850  max mem: 6186
Epoch: [25]  [  800/40201]  eta: 6:17:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6212 (3.4725)  loss_scale: 1024.0000 (327.4307)  weight_decay: 0.0500 (0.0500)  time: 1.1756  data: 0.6249  max mem: 6186
Epoch: [25]  [  810/40201]  eta: 6:17:14  lr: 0.000003  min_lr: 0.000000  loss: 3.1667 (3.4614)  loss_scale: 1024.0000 (336.0197)  weight_decay: 0.0500 (0.0500)  time: 0.8163  data: 0.2174  max mem: 6186
Epoch: [25]  [  820/40201]  eta: 6:17:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2585 (3.4681)  loss_scale: 1024.0000 (344.3995)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0009  max mem: 6186
Epoch: [25]  [  830/40201]  eta: 6:17:29  lr: 0.000003  min_lr: 0.000000  loss: 3.6680 (3.4650)  loss_scale: 1024.0000 (352.5776)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0012  max mem: 6186
Epoch: [25]  [  840/40201]  eta: 6:18:02  lr: 0.000003  min_lr: 0.000000  loss: 3.0893 (3.4582)  loss_scale: 1024.0000 (360.5612)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0305  max mem: 6186
Epoch: [25]  [  850/40201]  eta: 6:18:11  lr: 0.000003  min_lr: 0.000000  loss: 2.9086 (3.4542)  loss_scale: 1024.0000 (368.3572)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0303  max mem: 6186
Epoch: [25]  [  860/40201]  eta: 6:17:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3390 (3.4578)  loss_scale: 1024.0000 (375.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0006  max mem: 6186
Epoch: [25]  [  870/40201]  eta: 6:18:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6409 (3.4568)  loss_scale: 1024.0000 (383.4122)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0113  max mem: 6186
Epoch: [25]  [  880/40201]  eta: 6:18:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5220 (3.4540)  loss_scale: 1024.0000 (390.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0114  max mem: 6186
Epoch: [25]  [  890/40201]  eta: 6:17:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2206 (3.4495)  loss_scale: 1024.0000 (397.7912)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0006  max mem: 6186
Epoch: [25]  [  900/40201]  eta: 6:14:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1936 (3.4492)  loss_scale: 1024.0000 (404.7414)  weight_decay: 0.0500 (0.0500)  time: 0.3567  data: 0.0006  max mem: 6186
Epoch: [25]  [  910/40201]  eta: 6:13:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2064 (3.4497)  loss_scale: 1024.0000 (411.5390)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.1034  max mem: 6186
Epoch: [25]  [  920/40201]  eta: 6:12:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2539 (3.4457)  loss_scale: 1024.0000 (418.1889)  weight_decay: 0.0500 (0.0500)  time: 0.4362  data: 0.2258  max mem: 6186
Epoch: [25]  [  930/40201]  eta: 6:13:51  lr: 0.000003  min_lr: 0.000000  loss: 3.4287 (3.4494)  loss_scale: 1024.0000 (424.6960)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.4082  max mem: 6186
Epoch: [25]  [  940/40201]  eta: 6:13:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7587 (3.4510)  loss_scale: 1024.0000 (431.0648)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.4217  max mem: 6186
Epoch: [25]  [  950/40201]  eta: 6:13:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5710 (3.4507)  loss_scale: 1024.0000 (437.2997)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.1367  max mem: 6186
Epoch: [25]  [  960/40201]  eta: 6:12:59  lr: 0.000003  min_lr: 0.000000  loss: 3.6920 (3.4532)  loss_scale: 1024.0000 (443.4048)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0017  max mem: 6186
Epoch: [25]  [  970/40201]  eta: 6:12:53  lr: 0.000003  min_lr: 0.000000  loss: 3.0883 (3.4500)  loss_scale: 1024.0000 (449.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0016  max mem: 6186
Epoch: [25]  [  980/40201]  eta: 6:12:40  lr: 0.000003  min_lr: 0.000000  loss: 3.0486 (3.4505)  loss_scale: 1024.0000 (455.2416)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0004  max mem: 6186
Epoch: [25]  [  990/40201]  eta: 6:12:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2312 (3.4468)  loss_scale: 1024.0000 (460.9808)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0008  max mem: 6186
[2023-07-24 15:24:32,647] [INFO] [timer.py:181:stop] 0/1000, SamplesPerSec=13.224122635574409
Epoch: [25]  [ 1000/40201]  eta: 6:13:19  lr: 0.000003  min_lr: 0.000000  loss: 3.3317 (3.4485)  loss_scale: 1024.0000 (466.6054)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.0013  max mem: 6186
Epoch: [25]  [ 1010/40201]  eta: 6:13:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6181 (3.4503)  loss_scale: 1024.0000 (472.1187)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1020/40201]  eta: 6:13:26  lr: 0.000003  min_lr: 0.000000  loss: 3.4497 (3.4491)  loss_scale: 1024.0000 (477.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0005  max mem: 6186
[2023-07-24 15:24:47,900] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:24:47,900] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-07-24 15:24:47,904] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:24:47,904] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [25]  [ 1030/40201]  eta: 6:13:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1517 (3.4454)  loss_scale: 1024.0000 (488.7837)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1040/40201]  eta: 6:13:19  lr: 0.000003  min_lr: 0.000000  loss: 3.2554 (3.4474)  loss_scale: 2048.0000 (503.7618)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1050/40201]  eta: 6:13:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2554 (3.4432)  loss_scale: 2048.0000 (518.4548)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0014  max mem: 6186
Epoch: [25]  [ 1060/40201]  eta: 6:13:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1400 (3.4451)  loss_scale: 2048.0000 (532.8709)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1070/40201]  eta: 6:14:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5768 (3.4491)  loss_scale: 2048.0000 (547.0177)  weight_decay: 0.0500 (0.0500)  time: 0.7144  data: 0.1023  max mem: 6186
Epoch: [25]  [ 1080/40201]  eta: 6:14:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4286 (3.4476)  loss_scale: 2048.0000 (560.9029)  weight_decay: 0.0500 (0.0500)  time: 0.7232  data: 0.1023  max mem: 6186
Epoch: [25]  [ 1090/40201]  eta: 6:15:03  lr: 0.000003  min_lr: 0.000000  loss: 3.0979 (3.4452)  loss_scale: 2048.0000 (574.5335)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1100/40201]  eta: 6:14:53  lr: 0.000003  min_lr: 0.000000  loss: 3.0502 (3.4430)  loss_scale: 2048.0000 (587.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1110/40201]  eta: 6:15:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3420 (3.4453)  loss_scale: 2048.0000 (601.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1120/40201]  eta: 6:14:58  lr: 0.000003  min_lr: 0.000000  loss: 3.7086 (3.4511)  loss_scale: 2048.0000 (613.9661)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1130/40201]  eta: 6:16:27  lr: 0.000003  min_lr: 0.000000  loss: 3.6249 (3.4517)  loss_scale: 2048.0000 (626.6454)  weight_decay: 0.0500 (0.0500)  time: 0.7079  data: 0.1316  max mem: 6186
Epoch: [25]  [ 1140/40201]  eta: 6:16:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4423 (3.4523)  loss_scale: 2048.0000 (639.1025)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.1313  max mem: 6186
Epoch: [25]  [ 1150/40201]  eta: 6:16:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3084 (3.4520)  loss_scale: 2048.0000 (651.3432)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0020  max mem: 6186
Epoch: [25]  [ 1160/40201]  eta: 6:16:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3040 (3.4529)  loss_scale: 2048.0000 (663.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0022  max mem: 6186
Epoch: [25]  [ 1170/40201]  eta: 6:16:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2494 (3.4507)  loss_scale: 2048.0000 (675.1973)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1180/40201]  eta: 6:16:31  lr: 0.000003  min_lr: 0.000000  loss: 3.2827 (3.4545)  loss_scale: 2048.0000 (686.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1190/40201]  eta: 6:16:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4914 (3.4556)  loss_scale: 2048.0000 (698.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1200/40201]  eta: 6:16:32  lr: 0.000003  min_lr: 0.000000  loss: 3.4991 (3.4569)  loss_scale: 2048.0000 (709.4888)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1210/40201]  eta: 6:16:19  lr: 0.000003  min_lr: 0.000000  loss: 3.2540 (3.4541)  loss_scale: 2048.0000 (720.5417)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1220/40201]  eta: 6:16:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1033 (3.4518)  loss_scale: 2048.0000 (731.4136)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1230/40201]  eta: 6:16:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1033 (3.4503)  loss_scale: 2048.0000 (742.1089)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1240/40201]  eta: 6:16:02  lr: 0.000003  min_lr: 0.000000  loss: 3.6486 (3.4523)  loss_scale: 2048.0000 (752.6317)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0010  max mem: 6186
Epoch: [25]  [ 1250/40201]  eta: 6:15:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5686 (3.4522)  loss_scale: 2048.0000 (762.9864)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0016  max mem: 6186
Epoch: [25]  [ 1260/40201]  eta: 6:16:02  lr: 0.000003  min_lr: 0.000000  loss: 2.9741 (3.4488)  loss_scale: 2048.0000 (773.1768)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0012  max mem: 6186
Epoch: [25]  [ 1270/40201]  eta: 6:15:54  lr: 0.000003  min_lr: 0.000000  loss: 3.0961 (3.4449)  loss_scale: 2048.0000 (783.2069)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1280/40201]  eta: 6:15:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1375 (3.4460)  loss_scale: 2048.0000 (793.0804)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0006  max mem: 6186
[2023-07-24 15:27:23,712] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:27:23,712] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-07-24 15:27:23,718] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:27:23,719] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [25]  [ 1290/40201]  eta: 6:15:45  lr: 0.000003  min_lr: 0.000000  loss: 3.7979 (3.4476)  loss_scale: 2048.0000 (818.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1300/40201]  eta: 6:15:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3882 (3.4484)  loss_scale: 4096.0000 (843.8555)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1310/40201]  eta: 6:15:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1250 (3.4477)  loss_scale: 4096.0000 (868.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0021  max mem: 6186
Epoch: [25]  [ 1320/40201]  eta: 6:15:22  lr: 0.000003  min_lr: 0.000000  loss: 3.3969 (3.4492)  loss_scale: 4096.0000 (893.0931)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0017  max mem: 6186
Epoch: [25]  [ 1330/40201]  eta: 6:16:39  lr: 0.000003  min_lr: 0.000000  loss: 3.4327 (3.4478)  loss_scale: 4096.0000 (917.1570)  weight_decay: 0.0500 (0.0500)  time: 0.7082  data: 0.1191  max mem: 6186
Epoch: [25]  [ 1340/40201]  eta: 6:16:31  lr: 0.000003  min_lr: 0.000000  loss: 3.0413 (3.4474)  loss_scale: 4096.0000 (940.8620)  weight_decay: 0.0500 (0.0500)  time: 0.7192  data: 0.1191  max mem: 6186
Epoch: [25]  [ 1350/40201]  eta: 6:16:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3858 (3.4476)  loss_scale: 4096.0000 (964.2161)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0215  max mem: 6186
Epoch: [25]  [ 1360/40201]  eta: 6:16:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5431 (3.4479)  loss_scale: 4096.0000 (987.2270)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0216  max mem: 6186
Epoch: [25]  [ 1370/40201]  eta: 6:16:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7268 (3.4519)  loss_scale: 4096.0000 (1009.9023)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1380/40201]  eta: 6:16:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7268 (3.4512)  loss_scale: 4096.0000 (1032.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1390/40201]  eta: 6:17:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3133 (3.4530)  loss_scale: 4096.0000 (1054.2746)  weight_decay: 0.0500 (0.0500)  time: 0.7539  data: 0.1486  max mem: 6186
Epoch: [25]  [ 1400/40201]  eta: 6:18:05  lr: 0.000003  min_lr: 0.000000  loss: 3.3412 (3.4516)  loss_scale: 4096.0000 (1075.9857)  weight_decay: 0.0500 (0.0500)  time: 0.7884  data: 0.1560  max mem: 6186
Epoch: [25]  [ 1410/40201]  eta: 6:17:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5254 (3.4529)  loss_scale: 4096.0000 (1097.3891)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0084  max mem: 6186
Epoch: [25]  [ 1420/40201]  eta: 6:18:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3088 (3.4498)  loss_scale: 4096.0000 (1118.4912)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1430/40201]  eta: 6:16:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2622 (3.4493)  loss_scale: 4096.0000 (1139.2984)  weight_decay: 0.0500 (0.0500)  time: 0.4233  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1440/40201]  eta: 6:15:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3346 (3.4483)  loss_scale: 4096.0000 (1159.8168)  weight_decay: 0.0500 (0.0500)  time: 0.2947  data: 0.0295  max mem: 6186
Epoch: [25]  [ 1450/40201]  eta: 6:14:39  lr: 0.000003  min_lr: 0.000000  loss: 2.9982 (3.4471)  loss_scale: 4096.0000 (1180.0524)  weight_decay: 0.0500 (0.0500)  time: 0.4154  data: 0.1414  max mem: 6186
Epoch: [25]  [ 1460/40201]  eta: 6:12:59  lr: 0.000003  min_lr: 0.000000  loss: 3.3510 (3.4465)  loss_scale: 4096.0000 (1200.0110)  weight_decay: 0.0500 (0.0500)  time: 0.3428  data: 0.1125  max mem: 6186
Epoch: [25]  [ 1470/40201]  eta: 6:14:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2758 (3.4454)  loss_scale: 4096.0000 (1219.6982)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.3113  max mem: 6186
Epoch: [25]  [ 1480/40201]  eta: 6:12:43  lr: 0.000003  min_lr: 0.000000  loss: 3.1016 (3.4454)  loss_scale: 4096.0000 (1239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.3428  max mem: 6186
Epoch: [25]  [ 1490/40201]  eta: 6:13:44  lr: 0.000003  min_lr: 0.000000  loss: 2.9958 (3.4440)  loss_scale: 4096.0000 (1258.2803)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1077  max mem: 6186
Epoch: [25]  [ 1500/40201]  eta: 6:13:46  lr: 0.000003  min_lr: 0.000000  loss: 2.7372 (3.4393)  loss_scale: 4096.0000 (1277.1859)  weight_decay: 0.0500 (0.0500)  time: 0.7208  data: 0.0762  max mem: 6186
Epoch: [25]  [ 1510/40201]  eta: 6:13:40  lr: 0.000003  min_lr: 0.000000  loss: 2.9801 (3.4402)  loss_scale: 4096.0000 (1295.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1520/40201]  eta: 6:13:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6168 (3.4417)  loss_scale: 4096.0000 (1314.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1530/40201]  eta: 6:13:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4092 (3.4398)  loss_scale: 4096.0000 (1332.4206)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0009  max mem: 6186
[2023-07-24 15:29:53,831] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:29:53,831] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-07-24 15:29:53,832] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:29:53,832] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [25]  [ 1540/40201]  eta: 6:13:55  lr: 0.000003  min_lr: 0.000000  loss: 2.9884 (3.4383)  loss_scale: 4096.0000 (1360.9864)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0866  max mem: 6186
Epoch: [25]  [ 1550/40201]  eta: 6:13:54  lr: 0.000003  min_lr: 0.000000  loss: 3.3908 (3.4390)  loss_scale: 8192.0000 (1405.0290)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.0865  max mem: 6186
Epoch: [25]  [ 1560/40201]  eta: 6:13:52  lr: 0.000003  min_lr: 0.000000  loss: 3.1521 (3.4373)  loss_scale: 8192.0000 (1448.5074)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0017  max mem: 6186
Epoch: [25]  [ 1570/40201]  eta: 6:13:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1150 (3.4368)  loss_scale: 8192.0000 (1491.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0017  max mem: 6186
Epoch: [25]  [ 1580/40201]  eta: 6:13:43  lr: 0.000003  min_lr: 0.000000  loss: 3.1314 (3.4360)  loss_scale: 8192.0000 (1533.8140)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1590/40201]  eta: 6:14:00  lr: 0.000003  min_lr: 0.000000  loss: 3.2190 (3.4357)  loss_scale: 8192.0000 (1575.6631)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1600/40201]  eta: 6:14:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2190 (3.4364)  loss_scale: 8192.0000 (1616.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1610/40201]  eta: 6:14:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5805 (3.4390)  loss_scale: 8192.0000 (1657.8026)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1620/40201]  eta: 6:14:08  lr: 0.000003  min_lr: 0.000000  loss: 3.8247 (3.4403)  loss_scale: 8192.0000 (1698.1123)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1630/40201]  eta: 6:14:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2320 (3.4396)  loss_scale: 8192.0000 (1737.9277)  weight_decay: 0.0500 (0.0500)  time: 0.6836  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1640/40201]  eta: 6:15:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4947 (3.4431)  loss_scale: 8192.0000 (1777.2578)  weight_decay: 0.0500 (0.0500)  time: 0.7266  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1650/40201]  eta: 6:15:14  lr: 0.000003  min_lr: 0.000000  loss: 3.4947 (3.4408)  loss_scale: 8192.0000 (1816.1114)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1660/40201]  eta: 6:15:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1990 (3.4397)  loss_scale: 8192.0000 (1854.4973)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0014  max mem: 6186
Epoch: [25]  [ 1670/40201]  eta: 6:15:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1990 (3.4378)  loss_scale: 8192.0000 (1892.4237)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0012  max mem: 6186
Epoch: [25]  [ 1680/40201]  eta: 6:14:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1394 (3.4352)  loss_scale: 8192.0000 (1929.8989)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1690/40201]  eta: 6:14:52  lr: 0.000003  min_lr: 0.000000  loss: 3.2814 (3.4379)  loss_scale: 8192.0000 (1966.9308)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1700/40201]  eta: 6:16:04  lr: 0.000003  min_lr: 0.000000  loss: 3.6137 (3.4393)  loss_scale: 8192.0000 (2003.5273)  weight_decay: 0.0500 (0.0500)  time: 0.7618  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1710/40201]  eta: 6:15:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3649 (3.4388)  loss_scale: 8192.0000 (2039.6961)  weight_decay: 0.0500 (0.0500)  time: 0.7537  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1720/40201]  eta: 6:15:55  lr: 0.000003  min_lr: 0.000000  loss: 3.0345 (3.4349)  loss_scale: 8192.0000 (2075.4445)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1730/40201]  eta: 6:15:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2554 (3.4371)  loss_scale: 8192.0000 (2110.7799)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1740/40201]  eta: 6:15:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4794 (3.4364)  loss_scale: 8192.0000 (2145.7094)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1750/40201]  eta: 6:15:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4794 (3.4386)  loss_scale: 8192.0000 (2180.2399)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1760/40201]  eta: 6:15:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5984 (3.4402)  loss_scale: 8192.0000 (2214.3782)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1770/40201]  eta: 6:15:52  lr: 0.000003  min_lr: 0.000000  loss: 3.4766 (3.4419)  loss_scale: 8192.0000 (2248.1310)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0010  max mem: 6186
Epoch: [25]  [ 1780/40201]  eta: 6:15:51  lr: 0.000003  min_lr: 0.000000  loss: 3.4508 (3.4417)  loss_scale: 8192.0000 (2281.5048)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1790/40201]  eta: 6:15:56  lr: 0.000003  min_lr: 0.000000  loss: 3.2466 (3.4422)  loss_scale: 8192.0000 (2314.5059)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0005  max mem: 6186
[2023-07-24 15:32:34,648] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:32:34,648] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-07-24 15:32:34,660] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:32:34,660] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [25]  [ 1800/40201]  eta: 6:17:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2466 (3.4415)  loss_scale: 8192.0000 (2383.5292)  weight_decay: 0.0500 (0.0500)  time: 0.8524  data: 0.2401  max mem: 6186
Epoch: [25]  [ 1810/40201]  eta: 6:17:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4978 (3.4425)  loss_scale: 16384.0000 (2460.8371)  weight_decay: 0.0500 (0.0500)  time: 0.8192  data: 0.2400  max mem: 6186
Epoch: [25]  [ 1820/40201]  eta: 6:17:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3486 (3.4413)  loss_scale: 16384.0000 (2537.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1830/40201]  eta: 6:17:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3139 (3.4421)  loss_scale: 16384.0000 (2612.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1840/40201]  eta: 6:17:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3986 (3.4413)  loss_scale: 16384.0000 (2687.7219)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1850/40201]  eta: 6:17:14  lr: 0.000003  min_lr: 0.000000  loss: 3.3824 (3.4404)  loss_scale: 16384.0000 (2761.7158)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1860/40201]  eta: 6:17:13  lr: 0.000003  min_lr: 0.000000  loss: 3.3824 (3.4409)  loss_scale: 16384.0000 (2834.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1870/40201]  eta: 6:17:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2402 (3.4396)  loss_scale: 16384.0000 (2907.3308)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0003  max mem: 6186
Epoch: [25]  [ 1880/40201]  eta: 6:17:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3594 (3.4423)  loss_scale: 16384.0000 (2978.9771)  weight_decay: 0.0500 (0.0500)  time: 0.7044  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1890/40201]  eta: 6:18:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2809 (3.4415)  loss_scale: 16384.0000 (3049.8657)  weight_decay: 0.0500 (0.0500)  time: 0.7513  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1900/40201]  eta: 6:18:20  lr: 0.000003  min_lr: 0.000000  loss: 3.0538 (3.4407)  loss_scale: 16384.0000 (3120.0084)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.0013  max mem: 6186
Epoch: [25]  [ 1910/40201]  eta: 6:18:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1073 (3.4403)  loss_scale: 16384.0000 (3189.4171)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1920/40201]  eta: 6:18:15  lr: 0.000003  min_lr: 0.000000  loss: 3.0871 (3.4380)  loss_scale: 16384.0000 (3258.1031)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1930/40201]  eta: 6:18:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0871 (3.4383)  loss_scale: 16384.0000 (3326.0777)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1940/40201]  eta: 6:17:53  lr: 0.000003  min_lr: 0.000000  loss: 3.1042 (3.4376)  loss_scale: 16384.0000 (3393.3519)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0013  max mem: 6186
Epoch: [25]  [ 1950/40201]  eta: 6:17:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3038 (3.4378)  loss_scale: 16384.0000 (3459.9364)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0014  max mem: 6186
Epoch: [25]  [ 1960/40201]  eta: 6:17:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1950 (3.4358)  loss_scale: 16384.0000 (3525.8419)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0010  max mem: 6186
Epoch: [25]  [ 1970/40201]  eta: 6:17:29  lr: 0.000003  min_lr: 0.000000  loss: 2.8251 (3.4348)  loss_scale: 16384.0000 (3591.0786)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1980/40201]  eta: 6:17:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5334 (3.4344)  loss_scale: 16384.0000 (3655.6567)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1990/40201]  eta: 6:15:48  lr: 0.000003  min_lr: 0.000000  loss: 2.8941 (3.4332)  loss_scale: 16384.0000 (3719.5861)  weight_decay: 0.0500 (0.0500)  time: 0.3611  data: 0.0006  max mem: 6186
[2023-07-24 15:34:42,331] [INFO] [logging.py:69:log_dist] [Rank 0] step=1000, skipped=0, lr=[7.419649121582776e-08, 7.419649121582776e-08, 9.892865495443702e-08, 9.892865495443702e-08, 1.3190487327258267e-07, 1.3190487327258267e-07, 1.758731643634436e-07, 1.758731643634436e-07, 2.3449755248459144e-07, 2.3449755248459144e-07, 3.1266340331278857e-07, 3.1266340331278857e-07, 4.168845377503848e-07, 4.168845377503848e-07, 5.558460503338464e-07, 5.558460503338464e-07, 7.411280671117952e-07, 7.411280671117952e-07, 9.881707561490601e-07, 9.881707561490601e-07, 1.3175610081987469e-06, 1.3175610081987469e-06, 1.7567480109316627e-06, 1.7567480109316627e-06, 2.342330681242217e-06, 2.342330681242217e-06, 3.1231075749896224e-06, 3.1231075749896224e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 15:34:42,331] [INFO] [timer.py:181:stop] 0/2000, SamplesPerSec=12.597799086521224
Epoch: [25]  [ 2000/40201]  eta: 6:15:53  lr: 0.000003  min_lr: 0.000000  loss: 2.9879 (3.4334)  loss_scale: 16384.0000 (3782.8766)  weight_decay: 0.0500 (0.0500)  time: 0.4469  data: 0.0108  max mem: 6186
Epoch: [25]  [ 2010/40201]  eta: 6:15:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6964 (3.4352)  loss_scale: 16384.0000 (3845.5375)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0120  max mem: 6186
Epoch: [25]  [ 2020/40201]  eta: 6:14:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5485 (3.4358)  loss_scale: 16384.0000 (3907.5784)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0020  max mem: 6186
Epoch: [25]  [ 2030/40201]  eta: 6:13:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3844 (3.4373)  loss_scale: 16384.0000 (3969.0084)  weight_decay: 0.0500 (0.0500)  time: 0.3448  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2040/40201]  eta: 6:13:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6236 (3.4393)  loss_scale: 16384.0000 (4029.8364)  weight_decay: 0.0500 (0.0500)  time: 0.4755  data: 0.0019  max mem: 6186
[2023-07-24 15:35:08,770] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:35:08,770] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-07-24 15:35:08,770] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:35:08,770] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [25]  [ 2050/40201]  eta: 6:14:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5264 (3.4384)  loss_scale: 16384.0000 (4106.0478)  weight_decay: 0.0500 (0.0500)  time: 0.7830  data: 0.0016  max mem: 6186
Epoch: [25]  [ 2060/40201]  eta: 6:13:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1426 (3.4384)  loss_scale: 32768.0000 (4245.1160)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.0179  max mem: 6186
Epoch: [25]  [ 2070/40201]  eta: 6:13:29  lr: 0.000003  min_lr: 0.000000  loss: 3.2439 (3.4368)  loss_scale: 32768.0000 (4382.8411)  weight_decay: 0.0500 (0.0500)  time: 0.4443  data: 0.1848  max mem: 6186
Epoch: [25]  [ 2080/40201]  eta: 6:13:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0454 (3.4353)  loss_scale: 32768.0000 (4519.2427)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.3097  max mem: 6186
Epoch: [25]  [ 2090/40201]  eta: 6:12:34  lr: 0.000003  min_lr: 0.000000  loss: 3.0454 (3.4354)  loss_scale: 32768.0000 (4654.3396)  weight_decay: 0.0500 (0.0500)  time: 0.4706  data: 0.2168  max mem: 6186
Epoch: [25]  [ 2100/40201]  eta: 6:13:52  lr: 0.000003  min_lr: 0.000000  loss: 3.1733 (3.4327)  loss_scale: 32768.0000 (4788.1504)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1223  max mem: 6186
Epoch: [25]  [ 2110/40201]  eta: 6:13:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3461 (3.4357)  loss_scale: 32768.0000 (4920.6935)  weight_decay: 0.0500 (0.0500)  time: 0.8268  data: 0.0489  max mem: 6186
Epoch: [25]  [ 2120/40201]  eta: 6:13:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6280 (3.4345)  loss_scale: 32768.0000 (5051.9868)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0016  max mem: 6186
Epoch: [25]  [ 2130/40201]  eta: 6:13:49  lr: 0.000003  min_lr: 0.000000  loss: 3.2512 (3.4342)  loss_scale: 32768.0000 (5182.0479)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2140/40201]  eta: 6:13:43  lr: 0.000003  min_lr: 0.000000  loss: 3.4058 (3.4353)  loss_scale: 32768.0000 (5310.8940)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2150/40201]  eta: 6:13:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4058 (3.4375)  loss_scale: 32768.0000 (5438.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2160/40201]  eta: 6:13:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4731 (3.4380)  loss_scale: 32768.0000 (5565.0088)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2170/40201]  eta: 6:11:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8399 (3.4404)  loss_scale: 32768.0000 (5690.3105)  weight_decay: 0.0500 (0.0500)  time: 0.3469  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2180/40201]  eta: 6:11:24  lr: 0.000003  min_lr: 0.000000  loss: 3.3374 (3.4408)  loss_scale: 32768.0000 (5814.4631)  weight_decay: 0.0500 (0.0500)  time: 0.3254  data: 0.0116  max mem: 6186
Epoch: [25]  [ 2190/40201]  eta: 6:10:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1605 (3.4417)  loss_scale: 32768.0000 (5937.4824)  weight_decay: 0.0500 (0.0500)  time: 0.4226  data: 0.0353  max mem: 6186
Epoch: [25]  [ 2200/40201]  eta: 6:10:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3462 (3.4432)  loss_scale: 32768.0000 (6059.3839)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0602  max mem: 6186
Epoch: [25]  [ 2210/40201]  eta: 6:09:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2461 (3.4419)  loss_scale: 32768.0000 (6180.1827)  weight_decay: 0.0500 (0.0500)  time: 0.3579  data: 0.0839  max mem: 6186
Epoch: [25]  [ 2220/40201]  eta: 6:08:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4010 (3.4439)  loss_scale: 32768.0000 (6299.8937)  weight_decay: 0.0500 (0.0500)  time: 0.4204  data: 0.1459  max mem: 6186
Epoch: [25]  [ 2230/40201]  eta: 6:08:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2108 (3.4417)  loss_scale: 32768.0000 (6418.5316)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0987  max mem: 6186
Epoch: [25]  [ 2240/40201]  eta: 6:08:45  lr: 0.000003  min_lr: 0.000000  loss: 2.9414 (3.4404)  loss_scale: 32768.0000 (6536.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2250/40201]  eta: 6:08:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4544 (3.4407)  loss_scale: 32768.0000 (6652.6450)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2260/40201]  eta: 6:08:45  lr: 0.000003  min_lr: 0.000000  loss: 3.4391 (3.4406)  loss_scale: 32768.0000 (6768.1486)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2270/40201]  eta: 6:09:12  lr: 0.000003  min_lr: 0.000000  loss: 3.2606 (3.4413)  loss_scale: 32768.0000 (6882.6350)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.0021  max mem: 6186
Epoch: [25]  [ 2280/40201]  eta: 6:09:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5745 (3.4424)  loss_scale: 32768.0000 (6996.1175)  weight_decay: 0.0500 (0.0500)  time: 0.7358  data: 0.0016  max mem: 6186
Epoch: [25]  [ 2290/40201]  eta: 6:09:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1197 (3.4406)  loss_scale: 32768.0000 (7108.6093)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2300/40201]  eta: 6:09:25  lr: 0.000003  min_lr: 0.000000  loss: 3.4934 (3.4418)  loss_scale: 32768.0000 (7220.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0008  max mem: 6186
[2023-07-24 15:37:31,511] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:37:31,512] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-07-24 15:37:31,567] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:37:31,567] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [25]  [ 2310/40201]  eta: 6:09:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4934 (3.4399)  loss_scale: 32768.0000 (7415.7473)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2320/40201]  eta: 6:09:48  lr: 0.000003  min_lr: 0.000000  loss: 3.3714 (3.4412)  loss_scale: 65536.0000 (7666.1577)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2330/40201]  eta: 6:09:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6823 (3.4420)  loss_scale: 65536.0000 (7914.4196)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2340/40201]  eta: 6:09:43  lr: 0.000003  min_lr: 0.000000  loss: 3.1082 (3.4397)  loss_scale: 65536.0000 (8160.5604)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2350/40201]  eta: 6:09:48  lr: 0.000003  min_lr: 0.000000  loss: 3.1407 (3.4392)  loss_scale: 65536.0000 (8404.6074)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2360/40201]  eta: 6:09:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3869 (3.4391)  loss_scale: 65536.0000 (8646.5870)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2370/40201]  eta: 6:09:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4053 (3.4395)  loss_scale: 65536.0000 (8886.5255)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2380/40201]  eta: 6:10:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5461 (3.4406)  loss_scale: 65536.0000 (9124.4486)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2390/40201]  eta: 6:10:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5253 (3.4417)  loss_scale: 65536.0000 (9360.3814)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.0004  max mem: 6186
Epoch: [25]  [ 2400/40201]  eta: 6:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5060 (3.4425)  loss_scale: 65536.0000 (9594.3490)  weight_decay: 0.0500 (0.0500)  time: 0.7326  data: 0.0370  max mem: 6186
Epoch: [25]  [ 2410/40201]  eta: 6:10:44  lr: 0.000003  min_lr: 0.000000  loss: 3.5060 (3.4443)  loss_scale: 65536.0000 (9826.3758)  weight_decay: 0.0500 (0.0500)  time: 0.7170  data: 0.0372  max mem: 6186
Epoch: [25]  [ 2420/40201]  eta: 6:10:49  lr: 0.000003  min_lr: 0.000000  loss: 3.6291 (3.4435)  loss_scale: 65536.0000 (10056.4857)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2430/40201]  eta: 6:10:48  lr: 0.000003  min_lr: 0.000000  loss: 3.3678 (3.4434)  loss_scale: 65536.0000 (10284.7026)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2440/40201]  eta: 6:11:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3765 (3.4441)  loss_scale: 65536.0000 (10511.0496)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2450/40201]  eta: 6:11:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3855 (3.4455)  loss_scale: 65536.0000 (10735.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2460/40201]  eta: 6:10:54  lr: 0.000003  min_lr: 0.000000  loss: 3.3855 (3.4451)  loss_scale: 65536.0000 (10958.2251)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0017  max mem: 6186
Epoch: [25]  [ 2470/40201]  eta: 6:11:02  lr: 0.000003  min_lr: 0.000000  loss: 3.1453 (3.4436)  loss_scale: 65536.0000 (11179.0983)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0377  max mem: 6186
Epoch: [25]  [ 2480/40201]  eta: 6:11:00  lr: 0.000003  min_lr: 0.000000  loss: 2.9896 (3.4428)  loss_scale: 65536.0000 (11398.1911)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0373  max mem: 6186
Epoch: [25]  [ 2490/40201]  eta: 6:10:55  lr: 0.000003  min_lr: 0.000000  loss: 3.0634 (3.4420)  loss_scale: 65536.0000 (11615.5247)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2500/40201]  eta: 6:10:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3297 (3.4423)  loss_scale: 65536.0000 (11831.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2510/40201]  eta: 6:10:47  lr: 0.000003  min_lr: 0.000000  loss: 3.7238 (3.4436)  loss_scale: 65536.0000 (12044.9988)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2520/40201]  eta: 6:10:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6613 (3.4455)  loss_scale: 65536.0000 (12257.1805)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2530/40201]  eta: 6:10:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3951 (3.4455)  loss_scale: 65536.0000 (12467.6855)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2540/40201]  eta: 6:10:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0660 (3.4438)  loss_scale: 65536.0000 (12676.5336)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0017  max mem: 6186
Epoch: [25]  [ 2550/40201]  eta: 6:10:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0960 (3.4435)  loss_scale: 65536.0000 (12883.7444)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2560/40201]  eta: 6:10:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3741 (3.4440)  loss_scale: 65536.0000 (13089.3370)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0005  max mem: 6186
[2023-07-24 15:40:15,254] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:40:15,254] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2023-07-24 15:40:15,265] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:40:15,265] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2023-07-24 15:40:16,098] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1281
[2023-07-24 15:40:16,098] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-07-24 15:40:16,099] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072, reducing to 65536.0
[2023-07-24 15:40:16,111] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1281
[2023-07-24 15:40:16,112] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
Epoch: [25]  [ 2570/40201]  eta: 6:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.3805 (3.4439)  loss_scale: 65536.0000 (13344.3112)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0018  max mem: 6186
Epoch: [25]  [ 2580/40201]  eta: 6:10:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3920 (3.4452)  loss_scale: 65536.0000 (13546.5262)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0021  max mem: 6186
Epoch: [25]  [ 2590/40201]  eta: 6:10:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7265 (3.4459)  loss_scale: 65536.0000 (13747.1802)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2600/40201]  eta: 6:10:39  lr: 0.000003  min_lr: 0.000000  loss: 3.4842 (3.4460)  loss_scale: 65536.0000 (13946.2914)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2610/40201]  eta: 6:10:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5597 (3.4458)  loss_scale: 65536.0000 (14143.8774)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2620/40201]  eta: 6:10:43  lr: 0.000003  min_lr: 0.000000  loss: 3.3546 (3.4459)  loss_scale: 65536.0000 (14339.9557)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2630/40201]  eta: 6:12:57  lr: 0.000003  min_lr: 0.000000  loss: 3.1373 (3.4451)  loss_scale: 65536.0000 (14534.5435)  weight_decay: 0.0500 (0.0500)  time: 1.0924  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2640/40201]  eta: 6:12:26  lr: 0.000003  min_lr: 0.000000  loss: 3.3179 (3.4455)  loss_scale: 65536.0000 (14727.6577)  weight_decay: 0.0500 (0.0500)  time: 0.9963  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2650/40201]  eta: 6:12:18  lr: 0.000003  min_lr: 0.000000  loss: 3.4586 (3.4457)  loss_scale: 65536.0000 (14919.3150)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2660/40201]  eta: 6:11:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5752 (3.4474)  loss_scale: 65536.0000 (15109.5318)  weight_decay: 0.0500 (0.0500)  time: 0.4085  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2670/40201]  eta: 6:11:08  lr: 0.000003  min_lr: 0.000000  loss: 3.7378 (3.4477)  loss_scale: 65536.0000 (15298.3242)  weight_decay: 0.0500 (0.0500)  time: 0.3887  data: 0.0188  max mem: 6186
Epoch: [25]  [ 2680/40201]  eta: 6:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6248 (3.4485)  loss_scale: 65536.0000 (15485.7083)  weight_decay: 0.0500 (0.0500)  time: 0.4663  data: 0.0187  max mem: 6186
Epoch: [25]  [ 2690/40201]  eta: 6:11:12  lr: 0.000003  min_lr: 0.000000  loss: 3.5033 (3.4493)  loss_scale: 65536.0000 (15671.6997)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2700/40201]  eta: 6:11:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2839 (3.4496)  loss_scale: 65536.0000 (15856.3140)  weight_decay: 0.0500 (0.0500)  time: 0.7570  data: 0.0018  max mem: 6186
Epoch: [25]  [ 2710/40201]  eta: 6:11:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3373 (3.4492)  loss_scale: 65536.0000 (16039.5662)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0019  max mem: 6186
Epoch: [25]  [ 2720/40201]  eta: 6:11:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3778 (3.4502)  loss_scale: 65536.0000 (16221.4715)  weight_decay: 0.0500 (0.0500)  time: 0.7594  data: 0.0013  max mem: 6186
[2023-07-24 15:42:03,277] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1362
[2023-07-24 15:42:03,277] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:42:03,277] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 15:42:03,291] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1362
[2023-07-24 15:42:03,291] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 2730/40201]  eta: 6:11:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5313 (3.4503)  loss_scale: 65536.0000 (16330.0535)  weight_decay: 0.0500 (0.0500)  time: 0.7376  data: 0.0004  max mem: 6186
Epoch: [25]  [ 2740/40201]  eta: 6:11:35  lr: 0.000003  min_lr: 0.000000  loss: 3.5058 (3.4497)  loss_scale: 32768.0000 (16390.0241)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2750/40201]  eta: 6:11:28  lr: 0.000003  min_lr: 0.000000  loss: 3.5058 (3.4500)  loss_scale: 32768.0000 (16449.5587)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2760/40201]  eta: 6:10:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2910 (3.4497)  loss_scale: 32768.0000 (16508.6621)  weight_decay: 0.0500 (0.0500)  time: 0.4583  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2770/40201]  eta: 6:10:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4820 (3.4510)  loss_scale: 32768.0000 (16567.3389)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.1803  max mem: 6186
Epoch: [25]  [ 2780/40201]  eta: 6:10:33  lr: 0.000003  min_lr: 0.000000  loss: 3.5035 (3.4508)  loss_scale: 32768.0000 (16625.5937)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1805  max mem: 6186
Epoch: [25]  [ 2790/40201]  eta: 6:10:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3729 (3.4513)  loss_scale: 32768.0000 (16683.4310)  weight_decay: 0.0500 (0.0500)  time: 0.4633  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2800/40201]  eta: 6:09:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2129 (3.4506)  loss_scale: 32768.0000 (16740.8554)  weight_decay: 0.0500 (0.0500)  time: 0.3939  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2810/40201]  eta: 6:09:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5918 (3.4522)  loss_scale: 32768.0000 (16797.8712)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2820/40201]  eta: 6:09:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5918 (3.4509)  loss_scale: 32768.0000 (16854.4828)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2830/40201]  eta: 6:09:40  lr: 0.000003  min_lr: 0.000000  loss: 3.1087 (3.4507)  loss_scale: 32768.0000 (16910.6945)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2840/40201]  eta: 6:09:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4961 (3.4501)  loss_scale: 32768.0000 (16966.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2850/40201]  eta: 6:09:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6685 (3.4499)  loss_scale: 32768.0000 (17021.9348)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2860/40201]  eta: 6:09:29  lr: 0.000003  min_lr: 0.000000  loss: 2.9047 (3.4482)  loss_scale: 32768.0000 (17076.9717)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2870/40201]  eta: 6:09:34  lr: 0.000003  min_lr: 0.000000  loss: 2.8768 (3.4473)  loss_scale: 32768.0000 (17131.6252)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2880/40201]  eta: 6:09:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3519 (3.4471)  loss_scale: 32768.0000 (17185.8993)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2890/40201]  eta: 6:09:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4714 (3.4466)  loss_scale: 32768.0000 (17239.7980)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2900/40201]  eta: 6:09:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4547 (3.4470)  loss_scale: 32768.0000 (17293.3251)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2910/40201]  eta: 6:09:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2646 (3.4461)  loss_scale: 32768.0000 (17346.4844)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2920/40201]  eta: 6:09:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1164 (3.4459)  loss_scale: 32768.0000 (17399.2797)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2930/40201]  eta: 6:10:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6710 (3.4464)  loss_scale: 32768.0000 (17451.7148)  weight_decay: 0.0500 (0.0500)  time: 0.7977  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2940/40201]  eta: 6:10:00  lr: 0.000003  min_lr: 0.000000  loss: 3.3911 (3.4453)  loss_scale: 32768.0000 (17503.7933)  weight_decay: 0.0500 (0.0500)  time: 0.7169  data: 0.0014  max mem: 6186
Epoch: [25]  [ 2950/40201]  eta: 6:11:33  lr: 0.000003  min_lr: 0.000000  loss: 3.5835 (3.4462)  loss_scale: 32768.0000 (17555.5188)  weight_decay: 0.0500 (0.0500)  time: 0.9792  data: 0.0017  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/pDPbETciXhw_000167_000177.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/pDPbETciXhw_000167_000177.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 2960/40201]  eta: 6:13:04  lr: 0.000003  min_lr: 0.000000  loss: 3.7579 (3.4470)  loss_scale: 32768.0000 (17606.8950)  weight_decay: 0.0500 (0.0500)  time: 1.3772  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2970/40201]  eta: 6:13:13  lr: 0.000003  min_lr: 0.000000  loss: 3.7579 (3.4484)  loss_scale: 32768.0000 (17657.9253)  weight_decay: 0.0500 (0.0500)  time: 1.0467  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2980/40201]  eta: 6:13:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2704 (3.4473)  loss_scale: 32768.0000 (17708.6132)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.0004  max mem: 6186
[2023-07-24 15:44:56,698] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:44:56,698] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 15:44:56,702] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:44:56,702] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 2990/40201]  eta: 6:13:17  lr: 0.000003  min_lr: 0.000000  loss: 3.1244 (3.4472)  loss_scale: 32768.0000 (17846.6065)  weight_decay: 0.0500 (0.0500)  time: 0.6647  data: 0.0005  max mem: 6186
[2023-07-24 15:45:07,766] [INFO] [timer.py:181:stop] 0/3000, SamplesPerSec=12.40598386047661
Epoch: [25]  [ 3000/40201]  eta: 6:13:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2939 (3.4463)  loss_scale: 65536.0000 (18005.5182)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.0015  max mem: 6186
Epoch: [25]  [ 3010/40201]  eta: 6:13:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4502 (3.4466)  loss_scale: 65536.0000 (18163.3743)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0014  max mem: 6186
[2023-07-24 15:45:17,470] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1507
[2023-07-24 15:45:17,471] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:45:17,473] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1507
[2023-07-24 15:45:17,473] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:45:17,473] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 3020/40201]  eta: 6:13:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4502 (3.4467)  loss_scale: 65536.0000 (18255.1049)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3030/40201]  eta: 6:13:00  lr: 0.000003  min_lr: 0.000000  loss: 3.2542 (3.4460)  loss_scale: 32768.0000 (18302.9865)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3040/40201]  eta: 6:12:59  lr: 0.000003  min_lr: 0.000000  loss: 3.7689 (3.4468)  loss_scale: 32768.0000 (18350.5531)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3050/40201]  eta: 6:13:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7557 (3.4476)  loss_scale: 32768.0000 (18397.8079)  weight_decay: 0.0500 (0.0500)  time: 0.7129  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3060/40201]  eta: 6:13:38  lr: 0.000003  min_lr: 0.000000  loss: 3.2406 (3.4471)  loss_scale: 32768.0000 (18444.7540)  weight_decay: 0.0500 (0.0500)  time: 0.8120  data: 0.1019  max mem: 6186
Epoch: [25]  [ 3070/40201]  eta: 6:15:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2639 (3.4480)  loss_scale: 32768.0000 (18491.3943)  weight_decay: 0.0500 (0.0500)  time: 1.3248  data: 0.1022  max mem: 6186
Epoch: [25]  [ 3080/40201]  eta: 6:17:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2639 (3.4470)  loss_scale: 32768.0000 (18537.7319)  weight_decay: 0.0500 (0.0500)  time: 1.6811  data: 0.0015  max mem: 6186
Epoch: [25]  [ 3090/40201]  eta: 6:17:55  lr: 0.000003  min_lr: 0.000000  loss: 2.9532 (3.4463)  loss_scale: 32768.0000 (18583.7697)  weight_decay: 0.0500 (0.0500)  time: 1.1461  data: 0.0016  max mem: 6186
Epoch: [25]  [ 3100/40201]  eta: 6:17:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3903 (3.4464)  loss_scale: 32768.0000 (18629.5105)  weight_decay: 0.0500 (0.0500)  time: 0.7107  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3110/40201]  eta: 6:17:59  lr: 0.000003  min_lr: 0.000000  loss: 3.4532 (3.4465)  loss_scale: 32768.0000 (18674.9572)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3120/40201]  eta: 6:18:03  lr: 0.000003  min_lr: 0.000000  loss: 3.4912 (3.4462)  loss_scale: 32768.0000 (18720.1128)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.0016  max mem: 6186
Epoch: [25]  [ 3130/40201]  eta: 6:18:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4912 (3.4459)  loss_scale: 32768.0000 (18764.9799)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.0016  max mem: 6186
Epoch: [25]  [ 3140/40201]  eta: 6:18:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2964 (3.4445)  loss_scale: 32768.0000 (18809.5613)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3150/40201]  eta: 6:17:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2064 (3.4446)  loss_scale: 32768.0000 (18853.8597)  weight_decay: 0.0500 (0.0500)  time: 0.4678  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3160/40201]  eta: 6:16:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3896 (3.4440)  loss_scale: 32768.0000 (18897.8779)  weight_decay: 0.0500 (0.0500)  time: 0.3178  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3170/40201]  eta: 6:16:13  lr: 0.000003  min_lr: 0.000000  loss: 2.9165 (3.4425)  loss_scale: 32768.0000 (18941.6184)  weight_decay: 0.0500 (0.0500)  time: 0.3553  data: 0.0496  max mem: 6186
Epoch: [25]  [ 3180/40201]  eta: 6:15:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5299 (3.4444)  loss_scale: 32768.0000 (18985.0839)  weight_decay: 0.0500 (0.0500)  time: 0.4287  data: 0.1531  max mem: 6186
Epoch: [25]  [ 3190/40201]  eta: 6:16:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6855 (3.4437)  loss_scale: 32768.0000 (19028.2770)  weight_decay: 0.0500 (0.0500)  time: 0.8145  data: 0.1624  max mem: 6186
Epoch: [25]  [ 3200/40201]  eta: 6:16:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5586 (3.4449)  loss_scale: 32768.0000 (19071.2002)  weight_decay: 0.0500 (0.0500)  time: 0.7211  data: 0.0589  max mem: 6186
Epoch: [25]  [ 3210/40201]  eta: 6:16:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6224 (3.4449)  loss_scale: 32768.0000 (19113.8561)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3220/40201]  eta: 6:16:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2439 (3.4445)  loss_scale: 32768.0000 (19156.2471)  weight_decay: 0.0500 (0.0500)  time: 0.8167  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3230/40201]  eta: 6:17:21  lr: 0.000003  min_lr: 0.000000  loss: 3.1218 (3.4433)  loss_scale: 32768.0000 (19198.3757)  weight_decay: 0.0500 (0.0500)  time: 0.9101  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3240/40201]  eta: 6:17:20  lr: 0.000003  min_lr: 0.000000  loss: 3.0138 (3.4432)  loss_scale: 32768.0000 (19240.2444)  weight_decay: 0.0500 (0.0500)  time: 0.8334  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3250/40201]  eta: 6:16:40  lr: 0.000003  min_lr: 0.000000  loss: 3.0138 (3.4417)  loss_scale: 32768.0000 (19281.8554)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3260/40201]  eta: 6:16:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3324 (3.4415)  loss_scale: 32768.0000 (19323.2113)  weight_decay: 0.0500 (0.0500)  time: 0.3287  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3270/40201]  eta: 6:16:08  lr: 0.000003  min_lr: 0.000000  loss: 3.3513 (3.4418)  loss_scale: 32768.0000 (19364.3143)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.1966  max mem: 6186
[2023-07-24 15:48:22,122] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:48:22,122] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 15:48:22,124] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:48:22,124] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 3280/40201]  eta: 6:16:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4057 (3.4417)  loss_scale: 32768.0000 (19485.0643)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.3758  max mem: 6186
Epoch: [25]  [ 3290/40201]  eta: 6:17:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5196 (3.4421)  loss_scale: 65536.0000 (19624.9942)  weight_decay: 0.0500 (0.0500)  time: 0.9941  data: 0.3452  max mem: 6186
Epoch: [25]  [ 3300/40201]  eta: 6:17:29  lr: 0.000003  min_lr: 0.000000  loss: 3.3803 (3.4420)  loss_scale: 65536.0000 (19764.0763)  weight_decay: 0.0500 (0.0500)  time: 1.0285  data: 0.1665  max mem: 6186
Epoch: [25]  [ 3310/40201]  eta: 6:17:38  lr: 0.000003  min_lr: 0.000000  loss: 3.7437 (3.4432)  loss_scale: 65536.0000 (19902.3183)  weight_decay: 0.0500 (0.0500)  time: 0.7425  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3320/40201]  eta: 6:17:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5546 (3.4433)  loss_scale: 65536.0000 (20039.7278)  weight_decay: 0.0500 (0.0500)  time: 0.7790  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3330/40201]  eta: 6:17:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2842 (3.4422)  loss_scale: 65536.0000 (20176.3122)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3340/40201]  eta: 6:16:52  lr: 0.000003  min_lr: 0.000000  loss: 2.8561 (3.4408)  loss_scale: 65536.0000 (20312.0790)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3350/40201]  eta: 6:16:12  lr: 0.000003  min_lr: 0.000000  loss: 2.5795 (3.4396)  loss_scale: 65536.0000 (20447.0355)  weight_decay: 0.0500 (0.0500)  time: 0.3413  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3360/40201]  eta: 6:17:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2157 (3.4390)  loss_scale: 65536.0000 (20581.1889)  weight_decay: 0.0500 (0.0500)  time: 0.8191  data: 0.4477  max mem: 6186
Epoch: [25]  [ 3370/40201]  eta: 6:16:58  lr: 0.000003  min_lr: 0.000000  loss: 3.2622 (3.4389)  loss_scale: 65536.0000 (20714.5464)  weight_decay: 0.0500 (0.0500)  time: 0.8792  data: 0.4766  max mem: 6186
Epoch: [25]  [ 3380/40201]  eta: 6:17:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5566 (3.4397)  loss_scale: 65536.0000 (20847.1151)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0496  max mem: 6186
Epoch: [25]  [ 3390/40201]  eta: 6:17:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1895 (3.4396)  loss_scale: 65536.0000 (20978.9018)  weight_decay: 0.0500 (0.0500)  time: 0.7640  data: 0.0207  max mem: 6186
Epoch: [25]  [ 3400/40201]  eta: 6:17:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1349 (3.4388)  loss_scale: 65536.0000 (21109.9136)  weight_decay: 0.0500 (0.0500)  time: 0.7644  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3410/40201]  eta: 6:17:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1349 (3.4396)  loss_scale: 65536.0000 (21240.1571)  weight_decay: 0.0500 (0.0500)  time: 0.7379  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3420/40201]  eta: 6:17:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1040 (3.4388)  loss_scale: 65536.0000 (21369.6393)  weight_decay: 0.0500 (0.0500)  time: 0.7577  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3430/40201]  eta: 6:18:00  lr: 0.000003  min_lr: 0.000000  loss: 3.0390 (3.4383)  loss_scale: 65536.0000 (21498.3667)  weight_decay: 0.0500 (0.0500)  time: 0.8027  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3440/40201]  eta: 6:18:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2425 (3.4384)  loss_scale: 65536.0000 (21626.3458)  weight_decay: 0.0500 (0.0500)  time: 0.8060  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3450/40201]  eta: 6:18:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3641 (3.4385)  loss_scale: 65536.0000 (21753.5833)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3460/40201]  eta: 6:18:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2816 (3.4387)  loss_scale: 65536.0000 (21880.0855)  weight_decay: 0.0500 (0.0500)  time: 0.7375  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3470/40201]  eta: 6:18:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2384 (3.4379)  loss_scale: 65536.0000 (22005.8588)  weight_decay: 0.0500 (0.0500)  time: 0.7570  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3480/40201]  eta: 6:17:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2191 (3.4380)  loss_scale: 65536.0000 (22130.9095)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3490/40201]  eta: 6:17:36  lr: 0.000003  min_lr: 0.000000  loss: 3.1453 (3.4369)  loss_scale: 65536.0000 (22255.2438)  weight_decay: 0.0500 (0.0500)  time: 0.3999  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3500/40201]  eta: 6:17:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3195 (3.4371)  loss_scale: 65536.0000 (22378.8678)  weight_decay: 0.0500 (0.0500)  time: 0.4570  data: 0.0732  max mem: 6186
[2023-07-24 15:51:04,562] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1754
[2023-07-24 15:51:04,563] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:51:04,563] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 15:51:04,566] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1754
[2023-07-24 15:51:04,566] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 3510/40201]  eta: 6:16:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5308 (3.4379)  loss_scale: 65536.0000 (22483.1216)  weight_decay: 0.0500 (0.0500)  time: 0.4366  data: 0.1631  max mem: 6186
Epoch: [25]  [ 3520/40201]  eta: 6:16:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5306 (3.4365)  loss_scale: 32768.0000 (22512.3317)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.2471  max mem: 6186
Epoch: [25]  [ 3530/40201]  eta: 6:16:05  lr: 0.000003  min_lr: 0.000000  loss: 2.9578 (3.4368)  loss_scale: 32768.0000 (22541.3764)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.1759  max mem: 6186
Epoch: [25]  [ 3540/40201]  eta: 6:15:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4734 (3.4367)  loss_scale: 32768.0000 (22570.2570)  weight_decay: 0.0500 (0.0500)  time: 0.4538  data: 0.0534  max mem: 6186
Epoch: [25]  [ 3550/40201]  eta: 6:16:15  lr: 0.000003  min_lr: 0.000000  loss: 2.9919 (3.4358)  loss_scale: 32768.0000 (22598.9749)  weight_decay: 0.0500 (0.0500)  time: 0.7248  data: 0.0338  max mem: 6186
Epoch: [25]  [ 3560/40201]  eta: 6:16:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3945 (3.4370)  loss_scale: 32768.0000 (22627.5316)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3570/40201]  eta: 6:16:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3945 (3.4362)  loss_scale: 32768.0000 (22655.9283)  weight_decay: 0.0500 (0.0500)  time: 0.7875  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3580/40201]  eta: 6:16:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3675 (3.4364)  loss_scale: 32768.0000 (22684.1664)  weight_decay: 0.0500 (0.0500)  time: 0.7679  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3590/40201]  eta: 6:17:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4565 (3.4356)  loss_scale: 32768.0000 (22712.2473)  weight_decay: 0.0500 (0.0500)  time: 0.7864  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3600/40201]  eta: 6:17:03  lr: 0.000003  min_lr: 0.000000  loss: 3.4565 (3.4362)  loss_scale: 32768.0000 (22740.1722)  weight_decay: 0.0500 (0.0500)  time: 0.7479  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3610/40201]  eta: 6:17:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4427 (3.4355)  loss_scale: 32768.0000 (22767.9424)  weight_decay: 0.0500 (0.0500)  time: 0.7311  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3620/40201]  eta: 6:17:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1013 (3.4355)  loss_scale: 32768.0000 (22795.5592)  weight_decay: 0.0500 (0.0500)  time: 0.7232  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3630/40201]  eta: 6:17:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0571 (3.4356)  loss_scale: 32768.0000 (22823.0240)  weight_decay: 0.0500 (0.0500)  time: 0.7183  data: 0.0014  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/lk5Ap5gZNj0_000009_000019.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 3640/40201]  eta: 6:17:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0571 (3.4355)  loss_scale: 32768.0000 (22850.3378)  weight_decay: 0.0500 (0.0500)  time: 0.7388  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3650/40201]  eta: 6:17:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4343 (3.4364)  loss_scale: 32768.0000 (22877.5021)  weight_decay: 0.0500 (0.0500)  time: 0.7412  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3660/40201]  eta: 6:17:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6505 (3.4370)  loss_scale: 32768.0000 (22904.5179)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3670/40201]  eta: 6:17:44  lr: 0.000003  min_lr: 0.000000  loss: 3.6079 (3.4384)  loss_scale: 32768.0000 (22931.3865)  weight_decay: 0.0500 (0.0500)  time: 0.7496  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3680/40201]  eta: 6:17:45  lr: 0.000003  min_lr: 0.000000  loss: 3.8502 (3.4397)  loss_scale: 32768.0000 (22958.1092)  weight_decay: 0.0500 (0.0500)  time: 0.7476  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3690/40201]  eta: 6:17:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3085 (3.4392)  loss_scale: 32768.0000 (22984.6871)  weight_decay: 0.0500 (0.0500)  time: 0.7306  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3700/40201]  eta: 6:17:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1728 (3.4388)  loss_scale: 32768.0000 (23011.1213)  weight_decay: 0.0500 (0.0500)  time: 0.7405  data: 0.0020  max mem: 6186
Epoch: [25]  [ 3710/40201]  eta: 6:17:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1728 (3.4386)  loss_scale: 32768.0000 (23037.4131)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.0022  max mem: 6186
Epoch: [25]  [ 3720/40201]  eta: 6:17:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2099 (3.4388)  loss_scale: 32768.0000 (23063.5636)  weight_decay: 0.0500 (0.0500)  time: 0.6920  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3730/40201]  eta: 6:18:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1271 (3.4383)  loss_scale: 32768.0000 (23089.5738)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3740/40201]  eta: 6:18:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3248 (3.4386)  loss_scale: 32768.0000 (23115.4451)  weight_decay: 0.0500 (0.0500)  time: 0.7289  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3750/40201]  eta: 6:18:14  lr: 0.000003  min_lr: 0.000000  loss: 2.9797 (3.4374)  loss_scale: 32768.0000 (23141.1784)  weight_decay: 0.0500 (0.0500)  time: 0.7590  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3760/40201]  eta: 6:18:15  lr: 0.000003  min_lr: 0.000000  loss: 2.8037 (3.4368)  loss_scale: 32768.0000 (23166.7748)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.0007  max mem: 6186
[2023-07-24 15:54:09,176] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:54:09,176] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 15:54:09,178] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:54:09,178] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 3770/40201]  eta: 6:18:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5622 (3.4380)  loss_scale: 32768.0000 (23226.9934)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3780/40201]  eta: 6:18:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5622 (3.4369)  loss_scale: 65536.0000 (23338.8924)  weight_decay: 0.0500 (0.0500)  time: 0.7388  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3790/40201]  eta: 6:18:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2315 (3.4370)  loss_scale: 65536.0000 (23450.2010)  weight_decay: 0.0500 (0.0500)  time: 0.7244  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3800/40201]  eta: 6:18:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2950 (3.4366)  loss_scale: 65536.0000 (23560.9240)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3810/40201]  eta: 6:18:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2817 (3.4362)  loss_scale: 65536.0000 (23671.0659)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0485  max mem: 6186
Epoch: [25]  [ 3820/40201]  eta: 6:17:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0180 (3.4353)  loss_scale: 65536.0000 (23780.6312)  weight_decay: 0.0500 (0.0500)  time: 0.4495  data: 0.0661  max mem: 6186
Epoch: [25]  [ 3830/40201]  eta: 6:17:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5522 (3.4364)  loss_scale: 65536.0000 (23889.6246)  weight_decay: 0.0500 (0.0500)  time: 0.4095  data: 0.0183  max mem: 6186
Epoch: [25]  [ 3840/40201]  eta: 6:16:58  lr: 0.000003  min_lr: 0.000000  loss: 3.6460 (3.4366)  loss_scale: 65536.0000 (23998.0505)  weight_decay: 0.0500 (0.0500)  time: 0.4773  data: 0.1395  max mem: 6186
Epoch: [25]  [ 3850/40201]  eta: 6:16:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4814 (3.4372)  loss_scale: 65536.0000 (24105.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.2161  max mem: 6186
Epoch: [25]  [ 3860/40201]  eta: 6:16:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4814 (3.4375)  loss_scale: 65536.0000 (24213.2173)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.1458  max mem: 6186
Epoch: [25]  [ 3870/40201]  eta: 6:17:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5825 (3.4380)  loss_scale: 65536.0000 (24319.9669)  weight_decay: 0.0500 (0.0500)  time: 0.8376  data: 0.0692  max mem: 6186
Epoch: [25]  [ 3880/40201]  eta: 6:17:36  lr: 0.000003  min_lr: 0.000000  loss: 3.7254 (3.4399)  loss_scale: 65536.0000 (24426.1665)  weight_decay: 0.0500 (0.0500)  time: 1.1012  data: 0.0006  max mem: 6186
[2023-07-24 15:55:25,218] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1941
[2023-07-24 15:55:25,218] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:55:25,221] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1941
[2023-07-24 15:55:25,221] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 15:55:25,221] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 3890/40201]  eta: 6:17:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5044 (3.4393)  loss_scale: 65536.0000 (24464.4482)  weight_decay: 0.0500 (0.0500)  time: 0.9387  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3900/40201]  eta: 6:18:02  lr: 0.000003  min_lr: 0.000000  loss: 2.9262 (3.4378)  loss_scale: 32768.0000 (24485.7339)  weight_decay: 0.0500 (0.0500)  time: 0.8281  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3910/40201]  eta: 6:18:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2469 (3.4387)  loss_scale: 32768.0000 (24506.9108)  weight_decay: 0.0500 (0.0500)  time: 0.7678  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3920/40201]  eta: 6:18:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6441 (3.4379)  loss_scale: 32768.0000 (24527.9796)  weight_decay: 0.0500 (0.0500)  time: 0.7232  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3930/40201]  eta: 6:18:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0653 (3.4367)  loss_scale: 32768.0000 (24548.9412)  weight_decay: 0.0500 (0.0500)  time: 0.7011  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3940/40201]  eta: 6:18:11  lr: 0.000003  min_lr: 0.000000  loss: 2.9706 (3.4361)  loss_scale: 32768.0000 (24569.7965)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3950/40201]  eta: 6:18:23  lr: 0.000003  min_lr: 0.000000  loss: 2.9068 (3.4351)  loss_scale: 32768.0000 (24590.5462)  weight_decay: 0.0500 (0.0500)  time: 0.7840  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3960/40201]  eta: 6:18:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2204 (3.4348)  loss_scale: 32768.0000 (24611.1911)  weight_decay: 0.0500 (0.0500)  time: 0.7650  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3970/40201]  eta: 6:18:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4650 (3.4351)  loss_scale: 32768.0000 (24631.7321)  weight_decay: 0.0500 (0.0500)  time: 0.7264  data: 0.0016  max mem: 6186
Epoch: [25]  [ 3980/40201]  eta: 6:18:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4946 (3.4352)  loss_scale: 32768.0000 (24652.1698)  weight_decay: 0.0500 (0.0500)  time: 0.7255  data: 0.0023  max mem: 6186
Epoch: [25]  [ 3990/40201]  eta: 6:18:34  lr: 0.000003  min_lr: 0.000000  loss: 2.9657 (3.4344)  loss_scale: 32768.0000 (24672.5051)  weight_decay: 0.0500 (0.0500)  time: 0.7212  data: 0.0134  max mem: 6186
[2023-07-24 15:56:52,296] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=5, lr=[7.276711286435383e-08, 7.276711286435383e-08, 9.702281715247177e-08, 9.702281715247177e-08, 1.293637562032957e-07, 1.293637562032957e-07, 1.7248500827106095e-07, 1.7248500827106095e-07, 2.2998001102808125e-07, 2.2998001102808125e-07, 3.066400147041083e-07, 3.066400147041083e-07, 4.0885335293881113e-07, 4.0885335293881113e-07, 5.451378039184148e-07, 5.451378039184148e-07, 7.268504052245531e-07, 7.268504052245531e-07, 9.691338736327373e-07, 9.691338736327373e-07, 1.2921784981769833e-06, 1.2921784981769833e-06, 1.7229046642359777e-06, 1.7229046642359777e-06, 2.2972062189813035e-06, 2.2972062189813035e-06, 3.0629416253084047e-06, 3.0629416253084047e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 15:56:52,298] [INFO] [timer.py:181:stop] 0/4000, SamplesPerSec=12.144357284403029
Epoch: [25]  [ 4000/40201]  eta: 6:18:40  lr: 0.000003  min_lr: 0.000000  loss: 2.9596 (3.4341)  loss_scale: 32768.0000 (24692.7388)  weight_decay: 0.0500 (0.0500)  time: 0.7546  data: 0.0125  max mem: 6186
Epoch: [25]  [ 4010/40201]  eta: 6:18:38  lr: 0.000003  min_lr: 0.000000  loss: 2.7645 (3.4321)  loss_scale: 32768.0000 (24712.8716)  weight_decay: 0.0500 (0.0500)  time: 0.7214  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4020/40201]  eta: 6:18:37  lr: 0.000003  min_lr: 0.000000  loss: 2.7789 (3.4322)  loss_scale: 32768.0000 (24732.9043)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.0017  max mem: 6186
Epoch: [25]  [ 4030/40201]  eta: 6:18:40  lr: 0.000003  min_lr: 0.000000  loss: 3.4297 (3.4324)  loss_scale: 32768.0000 (24752.8375)  weight_decay: 0.0500 (0.0500)  time: 0.7095  data: 0.0019  max mem: 6186
Epoch: [25]  [ 4040/40201]  eta: 6:18:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4400 (3.4327)  loss_scale: 32768.0000 (24772.6721)  weight_decay: 0.0500 (0.0500)  time: 0.7989  data: 0.1005  max mem: 6186
Epoch: [25]  [ 4050/40201]  eta: 6:18:52  lr: 0.000003  min_lr: 0.000000  loss: 3.6171 (3.4335)  loss_scale: 32768.0000 (24792.4088)  weight_decay: 0.0500 (0.0500)  time: 0.7639  data: 0.0994  max mem: 6186
Epoch: [25]  [ 4060/40201]  eta: 6:18:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6176 (3.4345)  loss_scale: 32768.0000 (24812.0483)  weight_decay: 0.0500 (0.0500)  time: 0.6977  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4070/40201]  eta: 6:19:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5894 (3.4342)  loss_scale: 32768.0000 (24831.5913)  weight_decay: 0.0500 (0.0500)  time: 0.7512  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4080/40201]  eta: 6:18:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1221 (3.4338)  loss_scale: 32768.0000 (24851.0385)  weight_decay: 0.0500 (0.0500)  time: 0.7200  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4090/40201]  eta: 6:19:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0524 (3.4329)  loss_scale: 32768.0000 (24870.3906)  weight_decay: 0.0500 (0.0500)  time: 0.7345  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4100/40201]  eta: 6:19:13  lr: 0.000003  min_lr: 0.000000  loss: 3.0292 (3.4328)  loss_scale: 32768.0000 (24889.6484)  weight_decay: 0.0500 (0.0500)  time: 0.7814  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4110/40201]  eta: 6:19:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4558 (3.4326)  loss_scale: 32768.0000 (24908.8125)  weight_decay: 0.0500 (0.0500)  time: 0.7604  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4120/40201]  eta: 6:19:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3975 (3.4325)  loss_scale: 32768.0000 (24927.8835)  weight_decay: 0.0500 (0.0500)  time: 0.7266  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4130/40201]  eta: 6:19:27  lr: 0.000003  min_lr: 0.000000  loss: 3.1272 (3.4320)  loss_scale: 32768.0000 (24946.8623)  weight_decay: 0.0500 (0.0500)  time: 0.7592  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4140/40201]  eta: 6:19:34  lr: 0.000003  min_lr: 0.000000  loss: 3.1272 (3.4319)  loss_scale: 32768.0000 (24965.7493)  weight_decay: 0.0500 (0.0500)  time: 0.8024  data: 0.0009  max mem: 6186
[2023-07-24 15:58:37,706] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:58:37,707] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 15:58:37,713] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 15:58:37,714] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4150/40201]  eta: 6:19:29  lr: 0.000003  min_lr: 0.000000  loss: 3.2769 (3.4314)  loss_scale: 32768.0000 (25063.4854)  weight_decay: 0.0500 (0.0500)  time: 0.7133  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4160/40201]  eta: 6:19:14  lr: 0.000003  min_lr: 0.000000  loss: 3.1184 (3.4311)  loss_scale: 65536.0000 (25160.7517)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0015  max mem: 6186
Epoch: [25]  [ 4170/40201]  eta: 6:18:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4756 (3.4316)  loss_scale: 65536.0000 (25257.5517)  weight_decay: 0.0500 (0.0500)  time: 0.4721  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4180/40201]  eta: 6:18:43  lr: 0.000003  min_lr: 0.000000  loss: 3.5142 (3.4322)  loss_scale: 65536.0000 (25353.8885)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4190/40201]  eta: 6:18:12  lr: 0.000003  min_lr: 0.000000  loss: 3.3075 (3.4308)  loss_scale: 65536.0000 (25449.7657)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0015  max mem: 6186
Epoch: [25]  [ 4200/40201]  eta: 6:18:18  lr: 0.000003  min_lr: 0.000000  loss: 2.7170 (3.4290)  loss_scale: 65536.0000 (25545.1864)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1833  max mem: 6186
Epoch: [25]  [ 4210/40201]  eta: 6:18:02  lr: 0.000003  min_lr: 0.000000  loss: 2.9423 (3.4292)  loss_scale: 65536.0000 (25640.1539)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1888  max mem: 6186
Epoch: [25]  [ 4220/40201]  eta: 6:18:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4347 (3.4286)  loss_scale: 65536.0000 (25734.6714)  weight_decay: 0.0500 (0.0500)  time: 0.7178  data: 0.0068  max mem: 6186
Epoch: [25]  [ 4230/40201]  eta: 6:18:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1067 (3.4274)  loss_scale: 65536.0000 (25828.7421)  weight_decay: 0.0500 (0.0500)  time: 0.9203  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4240/40201]  eta: 6:18:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5399 (3.4283)  loss_scale: 65536.0000 (25922.3693)  weight_decay: 0.0500 (0.0500)  time: 0.8901  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4250/40201]  eta: 6:18:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7478 (3.4282)  loss_scale: 65536.0000 (26015.5559)  weight_decay: 0.0500 (0.0500)  time: 0.6937  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4260/40201]  eta: 6:18:09  lr: 0.000003  min_lr: 0.000000  loss: 2.9606 (3.4281)  loss_scale: 65536.0000 (26108.3051)  weight_decay: 0.0500 (0.0500)  time: 0.4542  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4270/40201]  eta: 6:17:46  lr: 0.000003  min_lr: 0.000000  loss: 2.9958 (3.4275)  loss_scale: 65536.0000 (26200.6200)  weight_decay: 0.0500 (0.0500)  time: 0.4045  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4280/40201]  eta: 6:17:30  lr: 0.000003  min_lr: 0.000000  loss: 3.2716 (3.4274)  loss_scale: 65536.0000 (26292.5036)  weight_decay: 0.0500 (0.0500)  time: 0.4747  data: 0.0062  max mem: 6186
Epoch: [25]  [ 4290/40201]  eta: 6:16:56  lr: 0.000003  min_lr: 0.000000  loss: 3.2980 (3.4270)  loss_scale: 65536.0000 (26383.9590)  weight_decay: 0.0500 (0.0500)  time: 0.4115  data: 0.0061  max mem: 6186
Epoch: [25]  [ 4300/40201]  eta: 6:16:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4096 (3.4277)  loss_scale: 65536.0000 (26474.9891)  weight_decay: 0.0500 (0.0500)  time: 0.3325  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4310/40201]  eta: 6:17:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4461 (3.4278)  loss_scale: 65536.0000 (26565.5968)  weight_decay: 0.0500 (0.0500)  time: 0.9926  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4320/40201]  eta: 6:17:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3587 (3.4284)  loss_scale: 65536.0000 (26655.7852)  weight_decay: 0.0500 (0.0500)  time: 1.1680  data: 0.0019  max mem: 6186
Epoch: [25]  [ 4330/40201]  eta: 6:17:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5422 (3.4281)  loss_scale: 65536.0000 (26745.5571)  weight_decay: 0.0500 (0.0500)  time: 0.6795  data: 0.0021  max mem: 6186
Epoch: [25]  [ 4340/40201]  eta: 6:17:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1031 (3.4266)  loss_scale: 65536.0000 (26834.9155)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4350/40201]  eta: 6:17:09  lr: 0.000003  min_lr: 0.000000  loss: 3.0707 (3.4265)  loss_scale: 65536.0000 (26923.8630)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4360/40201]  eta: 6:16:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4839 (3.4268)  loss_scale: 65536.0000 (27012.4027)  weight_decay: 0.0500 (0.0500)  time: 0.3460  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4370/40201]  eta: 6:16:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5647 (3.4268)  loss_scale: 65536.0000 (27100.5372)  weight_decay: 0.0500 (0.0500)  time: 0.3953  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4380/40201]  eta: 6:15:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5647 (3.4271)  loss_scale: 65536.0000 (27188.2693)  weight_decay: 0.0500 (0.0500)  time: 0.3866  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4390/40201]  eta: 6:15:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1256 (3.4265)  loss_scale: 65536.0000 (27275.6019)  weight_decay: 0.0500 (0.0500)  time: 0.4457  data: 0.0223  max mem: 6186
[2023-07-24 16:01:07,508] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:01:07,508] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 16:01:07,512] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:01:07,512] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 16:01:07,980] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2199
[2023-07-24 16:01:07,980] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 16:01:07,980] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-07-24 16:01:07,980] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2199
[2023-07-24 16:01:07,981] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [25]  [ 4400/40201]  eta: 6:15:04  lr: 0.000003  min_lr: 0.000000  loss: 3.1256 (3.4264)  loss_scale: 65536.0000 (27392.3199)  weight_decay: 0.0500 (0.0500)  time: 0.4345  data: 0.0219  max mem: 6186
Epoch: [25]  [ 4410/40201]  eta: 6:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4568 (3.4269)  loss_scale: 65536.0000 (27478.7939)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4420/40201]  eta: 6:15:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4153 (3.4267)  loss_scale: 65536.0000 (27564.8767)  weight_decay: 0.0500 (0.0500)  time: 0.7573  data: 0.0013  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/_M6Ko0yRfD4_000097_000107.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 4430/40201]  eta: 6:15:17  lr: 0.000003  min_lr: 0.000000  loss: 3.2305 (3.4263)  loss_scale: 65536.0000 (27650.5710)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4440/40201]  eta: 6:15:21  lr: 0.000003  min_lr: 0.000000  loss: 3.2557 (3.4262)  loss_scale: 65536.0000 (27735.8793)  weight_decay: 0.0500 (0.0500)  time: 0.7587  data: 0.0007  max mem: 6186
[2023-07-24 16:01:41,954] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2222
[2023-07-24 16:01:41,954] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:01:41,954] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:01:41,955] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2222
[2023-07-24 16:01:41,956] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 4450/40201]  eta: 6:15:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4763 (3.4275)  loss_scale: 65536.0000 (27776.6327)  weight_decay: 0.0500 (0.0500)  time: 0.7262  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4460/40201]  eta: 6:15:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4040 (3.4273)  loss_scale: 32768.0000 (27787.8216)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4470/40201]  eta: 6:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2246 (3.4270)  loss_scale: 32768.0000 (27798.9604)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4480/40201]  eta: 6:15:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2279 (3.4270)  loss_scale: 32768.0000 (27810.0495)  weight_decay: 0.0500 (0.0500)  time: 0.7157  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4490/40201]  eta: 6:15:20  lr: 0.000003  min_lr: 0.000000  loss: 3.0651 (3.4259)  loss_scale: 32768.0000 (27821.0893)  weight_decay: 0.0500 (0.0500)  time: 0.7403  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4500/40201]  eta: 6:15:26  lr: 0.000003  min_lr: 0.000000  loss: 2.9297 (3.4259)  loss_scale: 32768.0000 (27832.0800)  weight_decay: 0.0500 (0.0500)  time: 0.7440  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4510/40201]  eta: 6:15:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4038 (3.4267)  loss_scale: 32768.0000 (27843.0219)  weight_decay: 0.0500 (0.0500)  time: 0.7698  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4520/40201]  eta: 6:15:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3362 (3.4259)  loss_scale: 32768.0000 (27853.9155)  weight_decay: 0.0500 (0.0500)  time: 0.8082  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4530/40201]  eta: 6:15:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2469 (3.4262)  loss_scale: 32768.0000 (27864.7610)  weight_decay: 0.0500 (0.0500)  time: 0.6661  data: 0.0018  max mem: 6186
Epoch: [25]  [ 4540/40201]  eta: 6:14:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3635 (3.4264)  loss_scale: 32768.0000 (27875.5587)  weight_decay: 0.0500 (0.0500)  time: 0.4079  data: 0.0019  max mem: 6186
Epoch: [25]  [ 4550/40201]  eta: 6:14:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3204 (3.4260)  loss_scale: 32768.0000 (27886.3089)  weight_decay: 0.0500 (0.0500)  time: 0.3341  data: 0.0058  max mem: 6186
Epoch: [25]  [ 4560/40201]  eta: 6:14:13  lr: 0.000003  min_lr: 0.000000  loss: 3.3204 (3.4261)  loss_scale: 32768.0000 (27897.0121)  weight_decay: 0.0500 (0.0500)  time: 0.4535  data: 0.0907  max mem: 6186
Epoch: [25]  [ 4570/40201]  eta: 6:13:57  lr: 0.000003  min_lr: 0.000000  loss: 3.6876 (3.4263)  loss_scale: 32768.0000 (27907.6683)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1306  max mem: 6186
Epoch: [25]  [ 4580/40201]  eta: 6:14:44  lr: 0.000003  min_lr: 0.000000  loss: 3.6876 (3.4262)  loss_scale: 32768.0000 (27918.2781)  weight_decay: 0.0500 (0.0500)  time: 0.9136  data: 0.3574  max mem: 6186
Epoch: [25]  [ 4590/40201]  eta: 6:15:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2380 (3.4263)  loss_scale: 32768.0000 (27928.8416)  weight_decay: 0.0500 (0.0500)  time: 1.1795  data: 0.3128  max mem: 6186
Epoch: [25]  [ 4600/40201]  eta: 6:15:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1218 (3.4259)  loss_scale: 32768.0000 (27939.3593)  weight_decay: 0.0500 (0.0500)  time: 1.0096  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4610/40201]  eta: 6:15:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1218 (3.4258)  loss_scale: 32768.0000 (27949.8313)  weight_decay: 0.0500 (0.0500)  time: 0.9697  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4620/40201]  eta: 6:15:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5093 (3.4256)  loss_scale: 32768.0000 (27960.2580)  weight_decay: 0.0500 (0.0500)  time: 0.8906  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4630/40201]  eta: 6:16:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2390 (3.4246)  loss_scale: 32768.0000 (27970.6396)  weight_decay: 0.0500 (0.0500)  time: 0.8349  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4640/40201]  eta: 6:16:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2911 (3.4251)  loss_scale: 32768.0000 (27980.9765)  weight_decay: 0.0500 (0.0500)  time: 0.8319  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4650/40201]  eta: 6:16:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6710 (3.4249)  loss_scale: 32768.0000 (27991.2690)  weight_decay: 0.0500 (0.0500)  time: 0.7883  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4660/40201]  eta: 6:16:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4291 (3.4255)  loss_scale: 32768.0000 (28001.5173)  weight_decay: 0.0500 (0.0500)  time: 0.7787  data: 0.0016  max mem: 6186
Epoch: [25]  [ 4670/40201]  eta: 6:16:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7855 (3.4265)  loss_scale: 32768.0000 (28011.7217)  weight_decay: 0.0500 (0.0500)  time: 0.7849  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4680/40201]  eta: 6:16:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4023 (3.4256)  loss_scale: 32768.0000 (28021.8825)  weight_decay: 0.0500 (0.0500)  time: 0.7630  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4690/40201]  eta: 6:16:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3715 (3.4258)  loss_scale: 32768.0000 (28032.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7884  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4700/40201]  eta: 6:16:16  lr: 0.000003  min_lr: 0.000000  loss: 3.4110 (3.4262)  loss_scale: 32768.0000 (28042.0745)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0005  max mem: 6186
[2023-07-24 16:04:52,173] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:04:52,173] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:04:52,174] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:04:52,174] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4710/40201]  eta: 6:15:48  lr: 0.000003  min_lr: 0.000000  loss: 3.3564 (3.4256)  loss_scale: 32768.0000 (28107.7512)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4720/40201]  eta: 6:15:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3704 (3.4268)  loss_scale: 65536.0000 (28187.0316)  weight_decay: 0.0500 (0.0500)  time: 0.3459  data: 0.0014  max mem: 6186
[2023-07-24 16:05:00,212] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2361
[2023-07-24 16:05:00,213] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:05:00,212] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2361
[2023-07-24 16:05:00,213] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:05:00,213] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 4730/40201]  eta: 6:15:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4773 (3.4271)  loss_scale: 65536.0000 (28210.5669)  weight_decay: 0.0500 (0.0500)  time: 0.4273  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4740/40201]  eta: 6:14:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4773 (3.4278)  loss_scale: 32768.0000 (28220.1797)  weight_decay: 0.0500 (0.0500)  time: 0.4583  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4750/40201]  eta: 6:15:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5643 (3.4282)  loss_scale: 32768.0000 (28229.7521)  weight_decay: 0.0500 (0.0500)  time: 0.7335  data: 0.0017  max mem: 6186
Epoch: [25]  [ 4760/40201]  eta: 6:15:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4746 (3.4280)  loss_scale: 32768.0000 (28239.2842)  weight_decay: 0.0500 (0.0500)  time: 0.8758  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4770/40201]  eta: 6:15:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2863 (3.4275)  loss_scale: 32768.0000 (28248.7764)  weight_decay: 0.0500 (0.0500)  time: 0.7329  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4780/40201]  eta: 6:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2604 (3.4275)  loss_scale: 32768.0000 (28258.2288)  weight_decay: 0.0500 (0.0500)  time: 0.7897  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4790/40201]  eta: 6:15:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2604 (3.4273)  loss_scale: 32768.0000 (28267.6418)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4800/40201]  eta: 6:15:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1031 (3.4267)  loss_scale: 32768.0000 (28277.0156)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4810/40201]  eta: 6:15:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2504 (3.4268)  loss_scale: 32768.0000 (28286.3504)  weight_decay: 0.0500 (0.0500)  time: 0.8090  data: 0.0022  max mem: 6186
Epoch: [25]  [ 4820/40201]  eta: 6:15:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4858 (3.4263)  loss_scale: 32768.0000 (28295.6465)  weight_decay: 0.0500 (0.0500)  time: 0.7709  data: 0.0017  max mem: 6186
Epoch: [25]  [ 4830/40201]  eta: 6:15:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3925 (3.4262)  loss_scale: 32768.0000 (28304.9042)  weight_decay: 0.0500 (0.0500)  time: 0.7776  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4840/40201]  eta: 6:15:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5421 (3.4262)  loss_scale: 32768.0000 (28314.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4850/40201]  eta: 6:14:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5138 (3.4260)  loss_scale: 32768.0000 (28323.3049)  weight_decay: 0.0500 (0.0500)  time: 0.3628  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4860/40201]  eta: 6:14:30  lr: 0.000003  min_lr: 0.000000  loss: 3.0225 (3.4249)  loss_scale: 32768.0000 (28332.4485)  weight_decay: 0.0500 (0.0500)  time: 0.4677  data: 0.1062  max mem: 6186
Epoch: [25]  [ 4870/40201]  eta: 6:14:06  lr: 0.000003  min_lr: 0.000000  loss: 3.2135 (3.4248)  loss_scale: 32768.0000 (28341.5545)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.1456  max mem: 6186
Epoch: [25]  [ 4880/40201]  eta: 6:14:32  lr: 0.000003  min_lr: 0.000000  loss: 3.3398 (3.4249)  loss_scale: 32768.0000 (28350.6232)  weight_decay: 0.0500 (0.0500)  time: 0.7362  data: 0.3937  max mem: 6186
Epoch: [25]  [ 4890/40201]  eta: 6:14:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6021 (3.4251)  loss_scale: 32768.0000 (28359.6549)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.3564  max mem: 6186
Epoch: [25]  [ 4900/40201]  eta: 6:14:43  lr: 0.000003  min_lr: 0.000000  loss: 3.2687 (3.4247)  loss_scale: 32768.0000 (28368.6497)  weight_decay: 0.0500 (0.0500)  time: 0.8031  data: 0.0031  max mem: 6186
Epoch: [25]  [ 4910/40201]  eta: 6:14:58  lr: 0.000003  min_lr: 0.000000  loss: 3.1355 (3.4243)  loss_scale: 32768.0000 (28377.6078)  weight_decay: 0.0500 (0.0500)  time: 1.0062  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4920/40201]  eta: 6:15:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1355 (3.4249)  loss_scale: 32768.0000 (28386.5296)  weight_decay: 0.0500 (0.0500)  time: 0.8900  data: 0.0015  max mem: 6186
Epoch: [25]  [ 4930/40201]  eta: 6:15:06  lr: 0.000003  min_lr: 0.000000  loss: 3.2051 (3.4248)  loss_scale: 32768.0000 (28395.4151)  weight_decay: 0.0500 (0.0500)  time: 0.7820  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4940/40201]  eta: 6:15:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3989 (3.4249)  loss_scale: 32768.0000 (28404.2647)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4950/40201]  eta: 6:15:18  lr: 0.000003  min_lr: 0.000000  loss: 3.4449 (3.4253)  loss_scale: 32768.0000 (28413.0786)  weight_decay: 0.0500 (0.0500)  time: 0.8065  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4960/40201]  eta: 6:15:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4350 (3.4250)  loss_scale: 32768.0000 (28421.8569)  weight_decay: 0.0500 (0.0500)  time: 0.8605  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4970/40201]  eta: 6:15:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1555 (3.4243)  loss_scale: 32768.0000 (28430.5999)  weight_decay: 0.0500 (0.0500)  time: 0.9034  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4980/40201]  eta: 6:15:48  lr: 0.000003  min_lr: 0.000000  loss: 3.0941 (3.4241)  loss_scale: 32768.0000 (28439.3078)  weight_decay: 0.0500 (0.0500)  time: 0.8600  data: 0.0010  max mem: 6186
[2023-07-24 16:08:11,585] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:08:11,585] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:08:11,596] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:08:11,596] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4990/40201]  eta: 6:15:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6181 (3.4242)  loss_scale: 32768.0000 (28513.6349)  weight_decay: 0.0500 (0.0500)  time: 0.8162  data: 0.0010  max mem: 6186
[2023-07-24 16:08:26,509] [INFO] [timer.py:181:stop] 0/5000, SamplesPerSec=12.008887814796283
Epoch: [25]  [ 5000/40201]  eta: 6:16:01  lr: 0.000003  min_lr: 0.000000  loss: 3.2650 (3.4244)  loss_scale: 65536.0000 (28587.6649)  weight_decay: 0.0500 (0.0500)  time: 0.8270  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5010/40201]  eta: 6:16:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2367 (3.4238)  loss_scale: 65536.0000 (28661.3993)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5020/40201]  eta: 6:16:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2367 (3.4231)  loss_scale: 65536.0000 (28734.8401)  weight_decay: 0.0500 (0.0500)  time: 0.8309  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5030/40201]  eta: 6:16:23  lr: 0.000003  min_lr: 0.000000  loss: 2.9796 (3.4224)  loss_scale: 65536.0000 (28807.9889)  weight_decay: 0.0500 (0.0500)  time: 0.8185  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5040/40201]  eta: 6:16:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4458 (3.4229)  loss_scale: 65536.0000 (28880.8475)  weight_decay: 0.0500 (0.0500)  time: 0.8354  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5050/40201]  eta: 6:16:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5204 (3.4226)  loss_scale: 65536.0000 (28953.4175)  weight_decay: 0.0500 (0.0500)  time: 0.8723  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5060/40201]  eta: 6:16:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2245 (3.4223)  loss_scale: 65536.0000 (29025.7008)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5070/40201]  eta: 6:15:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2245 (3.4227)  loss_scale: 65536.0000 (29097.6991)  weight_decay: 0.0500 (0.0500)  time: 0.3988  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5080/40201]  eta: 6:15:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6572 (3.4228)  loss_scale: 65536.0000 (29169.4139)  weight_decay: 0.0500 (0.0500)  time: 0.4016  data: 0.0008  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/uLaU_15HYdo_000002_000012.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 5090/40201]  eta: 6:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6572 (3.4233)  loss_scale: 65536.0000 (29240.8470)  weight_decay: 0.0500 (0.0500)  time: 0.4400  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5100/40201]  eta: 6:15:03  lr: 0.000003  min_lr: 0.000000  loss: 3.7551 (3.4236)  loss_scale: 65536.0000 (29312.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0026  max mem: 6186
Epoch: [25]  [ 5110/40201]  eta: 6:14:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5641 (3.4235)  loss_scale: 65536.0000 (29382.8746)  weight_decay: 0.0500 (0.0500)  time: 0.4865  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5120/40201]  eta: 6:14:25  lr: 0.000003  min_lr: 0.000000  loss: 3.4779 (3.4234)  loss_scale: 65536.0000 (29453.4724)  weight_decay: 0.0500 (0.0500)  time: 0.4623  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5130/40201]  eta: 6:14:41  lr: 0.000003  min_lr: 0.000000  loss: 2.6281 (3.4224)  loss_scale: 65536.0000 (29523.7950)  weight_decay: 0.0500 (0.0500)  time: 0.7392  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5140/40201]  eta: 6:14:48  lr: 0.000003  min_lr: 0.000000  loss: 2.8173 (3.4223)  loss_scale: 65536.0000 (29593.8440)  weight_decay: 0.0500 (0.0500)  time: 0.8993  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5150/40201]  eta: 6:14:46  lr: 0.000003  min_lr: 0.000000  loss: 3.1913 (3.4220)  loss_scale: 65536.0000 (29663.6210)  weight_decay: 0.0500 (0.0500)  time: 0.7741  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5160/40201]  eta: 6:14:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3236 (3.4229)  loss_scale: 65536.0000 (29733.1277)  weight_decay: 0.0500 (0.0500)  time: 0.7718  data: 0.0003  max mem: 6186
Epoch: [25]  [ 5170/40201]  eta: 6:14:59  lr: 0.000003  min_lr: 0.000000  loss: 3.3044 (3.4225)  loss_scale: 65536.0000 (29802.3655)  weight_decay: 0.0500 (0.0500)  time: 0.8275  data: 0.0002  max mem: 6186
Epoch: [25]  [ 5180/40201]  eta: 6:15:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2647 (3.4230)  loss_scale: 65536.0000 (29871.3360)  weight_decay: 0.0500 (0.0500)  time: 0.8431  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5190/40201]  eta: 6:15:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5336 (3.4227)  loss_scale: 65536.0000 (29940.0408)  weight_decay: 0.0500 (0.0500)  time: 0.8641  data: 0.0017  max mem: 6186
Epoch: [25]  [ 5200/40201]  eta: 6:15:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0360 (3.4223)  loss_scale: 65536.0000 (30008.4814)  weight_decay: 0.0500 (0.0500)  time: 0.8641  data: 0.0012  max mem: 6186
[2023-07-24 16:10:49,356] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2600
[2023-07-24 16:10:49,356] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:10:49,370] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2600
[2023-07-24 16:10:49,371] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:10:49,371] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 5210/40201]  eta: 6:15:29  lr: 0.000003  min_lr: 0.000000  loss: 3.2156 (3.4221)  loss_scale: 32768.0000 (30013.7770)  weight_decay: 0.0500 (0.0500)  time: 0.8401  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5220/40201]  eta: 6:15:32  lr: 0.000003  min_lr: 0.000000  loss: 3.3034 (3.4216)  loss_scale: 32768.0000 (30019.0523)  weight_decay: 0.0500 (0.0500)  time: 0.8012  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5230/40201]  eta: 6:15:40  lr: 0.000003  min_lr: 0.000000  loss: 3.7141 (3.4231)  loss_scale: 32768.0000 (30024.3074)  weight_decay: 0.0500 (0.0500)  time: 0.8173  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5240/40201]  eta: 6:15:46  lr: 0.000003  min_lr: 0.000000  loss: 4.1548 (3.4246)  loss_scale: 32768.0000 (30029.5425)  weight_decay: 0.0500 (0.0500)  time: 0.8457  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5250/40201]  eta: 6:15:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4059 (3.4243)  loss_scale: 32768.0000 (30034.7576)  weight_decay: 0.0500 (0.0500)  time: 0.8813  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5260/40201]  eta: 6:16:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4000 (3.4246)  loss_scale: 32768.0000 (30039.9529)  weight_decay: 0.0500 (0.0500)  time: 0.8771  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5270/40201]  eta: 6:16:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4202 (3.4247)  loss_scale: 32768.0000 (30045.1284)  weight_decay: 0.0500 (0.0500)  time: 0.8154  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5280/40201]  eta: 6:16:14  lr: 0.000003  min_lr: 0.000000  loss: 3.9095 (3.4257)  loss_scale: 32768.0000 (30050.2844)  weight_decay: 0.0500 (0.0500)  time: 0.8153  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5290/40201]  eta: 6:16:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5934 (3.4257)  loss_scale: 32768.0000 (30055.4209)  weight_decay: 0.0500 (0.0500)  time: 0.8110  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5300/40201]  eta: 6:16:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3311 (3.4260)  loss_scale: 32768.0000 (30060.5380)  weight_decay: 0.0500 (0.0500)  time: 0.8470  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5310/40201]  eta: 6:16:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2635 (3.4260)  loss_scale: 32768.0000 (30065.6359)  weight_decay: 0.0500 (0.0500)  time: 0.8185  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5320/40201]  eta: 6:16:29  lr: 0.000003  min_lr: 0.000000  loss: 2.6984 (3.4244)  loss_scale: 32768.0000 (30070.7145)  weight_decay: 0.0500 (0.0500)  time: 0.7577  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5330/40201]  eta: 6:16:27  lr: 0.000003  min_lr: 0.000000  loss: 2.6651 (3.4238)  loss_scale: 32768.0000 (30075.7742)  weight_decay: 0.0500 (0.0500)  time: 0.7533  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5340/40201]  eta: 6:16:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1822 (3.4240)  loss_scale: 32768.0000 (30080.8148)  weight_decay: 0.0500 (0.0500)  time: 0.7833  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5350/40201]  eta: 6:16:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6113 (3.4249)  loss_scale: 32768.0000 (30085.8367)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5360/40201]  eta: 6:16:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4294 (3.4248)  loss_scale: 32768.0000 (30090.8398)  weight_decay: 0.0500 (0.0500)  time: 0.7609  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5370/40201]  eta: 6:16:40  lr: 0.000003  min_lr: 0.000000  loss: 3.0773 (3.4243)  loss_scale: 32768.0000 (30095.8242)  weight_decay: 0.0500 (0.0500)  time: 0.7986  data: 0.0630  max mem: 6186
Epoch: [25]  [ 5380/40201]  eta: 6:16:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2963 (3.4244)  loss_scale: 32768.0000 (30100.7902)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.0627  max mem: 6186
Epoch: [25]  [ 5390/40201]  eta: 6:16:07  lr: 0.000003  min_lr: 0.000000  loss: 2.8370 (3.4231)  loss_scale: 32768.0000 (30105.7377)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5400/40201]  eta: 6:15:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2404 (3.4234)  loss_scale: 32768.0000 (30110.6669)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5410/40201]  eta: 6:15:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4532 (3.4236)  loss_scale: 32768.0000 (30115.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5420/40201]  eta: 6:15:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6688 (3.4247)  loss_scale: 32768.0000 (30120.4708)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.1297  max mem: 6186
Epoch: [25]  [ 5430/40201]  eta: 6:15:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3842 (3.4240)  loss_scale: 32768.0000 (30125.3456)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.1292  max mem: 6186
Epoch: [25]  [ 5440/40201]  eta: 6:15:30  lr: 0.000003  min_lr: 0.000000  loss: 2.9055 (3.4230)  loss_scale: 32768.0000 (30130.2025)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5450/40201]  eta: 6:15:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1285 (3.4230)  loss_scale: 32768.0000 (30135.0416)  weight_decay: 0.0500 (0.0500)  time: 0.7399  data: 0.0018  max mem: 6186
[2023-07-24 16:14:02,950] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:14:02,951] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:14:02,968] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:14:02,968] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 5460/40201]  eta: 6:15:31  lr: 0.000003  min_lr: 0.000000  loss: 3.5853 (3.4230)  loss_scale: 32768.0000 (30151.8638)  weight_decay: 0.0500 (0.0500)  time: 0.7609  data: 0.0017  max mem: 6186
Epoch: [25]  [ 5470/40201]  eta: 6:15:31  lr: 0.000003  min_lr: 0.000000  loss: 3.3647 (3.4231)  loss_scale: 65536.0000 (30216.5396)  weight_decay: 0.0500 (0.0500)  time: 0.7839  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5480/40201]  eta: 6:15:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3647 (3.4230)  loss_scale: 65536.0000 (30280.9794)  weight_decay: 0.0500 (0.0500)  time: 0.7305  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5490/40201]  eta: 6:15:26  lr: 0.000003  min_lr: 0.000000  loss: 3.7150 (3.4237)  loss_scale: 65536.0000 (30345.1845)  weight_decay: 0.0500 (0.0500)  time: 0.7119  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5500/40201]  eta: 6:15:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6916 (3.4235)  loss_scale: 65536.0000 (30409.1562)  weight_decay: 0.0500 (0.0500)  time: 0.7679  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5510/40201]  eta: 6:15:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5030 (3.4242)  loss_scale: 65536.0000 (30472.8957)  weight_decay: 0.0500 (0.0500)  time: 0.7582  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5520/40201]  eta: 6:15:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5030 (3.4243)  loss_scale: 65536.0000 (30536.4043)  weight_decay: 0.0500 (0.0500)  time: 0.7084  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5530/40201]  eta: 6:15:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3473 (3.4240)  loss_scale: 65536.0000 (30599.6832)  weight_decay: 0.0500 (0.0500)  time: 0.7229  data: 0.0007  max mem: 6186
[2023-07-24 16:14:56,827] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2766
[2023-07-24 16:14:56,827] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:14:56,829] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2766
[2023-07-24 16:14:56,829] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:14:56,830] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 5540/40201]  eta: 6:15:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3948 (3.4242)  loss_scale: 65536.0000 (30615.4239)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5550/40201]  eta: 6:15:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3948 (3.4244)  loss_scale: 32768.0000 (30619.3017)  weight_decay: 0.0500 (0.0500)  time: 0.7196  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5560/40201]  eta: 6:15:08  lr: 0.000003  min_lr: 0.000000  loss: 3.3892 (3.4243)  loss_scale: 32768.0000 (30623.1656)  weight_decay: 0.0500 (0.0500)  time: 0.6905  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5570/40201]  eta: 6:15:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4158 (3.4246)  loss_scale: 32768.0000 (30627.0156)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5580/40201]  eta: 6:14:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3413 (3.4241)  loss_scale: 32768.0000 (30630.8518)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5590/40201]  eta: 6:14:50  lr: 0.000003  min_lr: 0.000000  loss: 2.8503 (3.4238)  loss_scale: 32768.0000 (30634.6743)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5600/40201]  eta: 6:14:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3566 (3.4242)  loss_scale: 32768.0000 (30638.4831)  weight_decay: 0.0500 (0.0500)  time: 0.6779  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5610/40201]  eta: 6:14:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3566 (3.4242)  loss_scale: 32768.0000 (30642.2784)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5620/40201]  eta: 6:14:37  lr: 0.000003  min_lr: 0.000000  loss: 3.1013 (3.4244)  loss_scale: 32768.0000 (30646.0601)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5630/40201]  eta: 6:14:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4770 (3.4244)  loss_scale: 32768.0000 (30649.8284)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5640/40201]  eta: 6:14:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6226 (3.4243)  loss_scale: 32768.0000 (30653.5834)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5650/40201]  eta: 6:14:23  lr: 0.000003  min_lr: 0.000000  loss: 3.5133 (3.4244)  loss_scale: 32768.0000 (30657.3251)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5660/40201]  eta: 6:14:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5133 (3.4250)  loss_scale: 32768.0000 (30661.0535)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5670/40201]  eta: 6:13:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5603 (3.4254)  loss_scale: 32768.0000 (30664.7688)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5680/40201]  eta: 6:13:34  lr: 0.000003  min_lr: 0.000000  loss: 3.4708 (3.4255)  loss_scale: 32768.0000 (30668.4710)  weight_decay: 0.0500 (0.0500)  time: 0.3783  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5690/40201]  eta: 6:13:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4708 (3.4255)  loss_scale: 32768.0000 (30672.1603)  weight_decay: 0.0500 (0.0500)  time: 0.4724  data: 0.0408  max mem: 6186
Epoch: [25]  [ 5700/40201]  eta: 6:13:19  lr: 0.000003  min_lr: 0.000000  loss: 3.2429 (3.4255)  loss_scale: 32768.0000 (30675.8365)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.2524  max mem: 6186
Epoch: [25]  [ 5710/40201]  eta: 6:12:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1890 (3.4257)  loss_scale: 32768.0000 (30679.4999)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.2342  max mem: 6186
Epoch: [25]  [ 5720/40201]  eta: 6:12:23  lr: 0.000003  min_lr: 0.000000  loss: 3.2329 (3.4254)  loss_scale: 32768.0000 (30683.1505)  weight_decay: 0.0500 (0.0500)  time: 0.2942  data: 0.0341  max mem: 6186
Epoch: [25]  [ 5730/40201]  eta: 6:12:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1310 (3.4250)  loss_scale: 32768.0000 (30686.7883)  weight_decay: 0.0500 (0.0500)  time: 0.4166  data: 0.0297  max mem: 6186
Epoch: [25]  [ 5740/40201]  eta: 6:12:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2579 (3.4256)  loss_scale: 32768.0000 (30690.4135)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0182  max mem: 6186
Epoch: [25]  [ 5750/40201]  eta: 6:12:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2579 (3.4255)  loss_scale: 32768.0000 (30694.0261)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5760/40201]  eta: 6:11:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5554 (3.4268)  loss_scale: 32768.0000 (30697.6261)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5770/40201]  eta: 6:11:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5826 (3.4268)  loss_scale: 32768.0000 (30701.2137)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.0493  max mem: 6186
Epoch: [25]  [ 5780/40201]  eta: 6:11:54  lr: 0.000003  min_lr: 0.000000  loss: 3.2031 (3.4263)  loss_scale: 32768.0000 (30704.7888)  weight_decay: 0.0500 (0.0500)  time: 0.7274  data: 0.0498  max mem: 6186
Epoch: [25]  [ 5790/40201]  eta: 6:11:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2031 (3.4263)  loss_scale: 32768.0000 (30708.3516)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.0015  max mem: 6186
[2023-07-24 16:17:37,614] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:17:37,614] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:17:37,617] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:17:37,618] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 5800/40201]  eta: 6:11:48  lr: 0.000003  min_lr: 0.000000  loss: 3.3068 (3.4261)  loss_scale: 32768.0000 (30768.3889)  weight_decay: 0.0500 (0.0500)  time: 0.7067  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5810/40201]  eta: 6:11:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1969 (3.4260)  loss_scale: 65536.0000 (30828.2196)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.0004  max mem: 6186
[2023-07-24 16:17:56,165] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2909
[2023-07-24 16:17:56,165] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:17:56,165] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:17:56,167] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2909
[2023-07-24 16:17:56,167] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 5820/40201]  eta: 6:11:34  lr: 0.000003  min_lr: 0.000000  loss: 3.1091 (3.4254)  loss_scale: 65536.0000 (30876.5862)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5830/40201]  eta: 6:11:29  lr: 0.000003  min_lr: 0.000000  loss: 3.6293 (3.4262)  loss_scale: 32768.0000 (30879.8299)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5840/40201]  eta: 6:11:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6780 (3.4262)  loss_scale: 32768.0000 (30883.0625)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5850/40201]  eta: 6:11:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1832 (3.4261)  loss_scale: 32768.0000 (30886.2841)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5860/40201]  eta: 6:11:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4900 (3.4263)  loss_scale: 32768.0000 (30889.4946)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5870/40201]  eta: 6:11:03  lr: 0.000003  min_lr: 0.000000  loss: 3.6956 (3.4273)  loss_scale: 32768.0000 (30892.6943)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5880/40201]  eta: 6:10:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3428 (3.4266)  loss_scale: 32768.0000 (30895.8830)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5890/40201]  eta: 6:10:50  lr: 0.000003  min_lr: 0.000000  loss: 3.5048 (3.4274)  loss_scale: 32768.0000 (30899.0609)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5900/40201]  eta: 6:10:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4983 (3.4264)  loss_scale: 32768.0000 (30902.2281)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5910/40201]  eta: 6:10:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3809 (3.4269)  loss_scale: 32768.0000 (30905.3845)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5920/40201]  eta: 6:10:28  lr: 0.000003  min_lr: 0.000000  loss: 3.5301 (3.4268)  loss_scale: 32768.0000 (30908.5303)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0016  max mem: 6186
Epoch: [25]  [ 5930/40201]  eta: 6:10:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1743 (3.4267)  loss_scale: 32768.0000 (30911.6655)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5940/40201]  eta: 6:10:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2618 (3.4267)  loss_scale: 32768.0000 (30914.7901)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5950/40201]  eta: 6:10:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3337 (3.4268)  loss_scale: 32768.0000 (30917.9042)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5960/40201]  eta: 6:09:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3422 (3.4270)  loss_scale: 32768.0000 (30921.0079)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5970/40201]  eta: 6:09:49  lr: 0.000003  min_lr: 0.000000  loss: 3.6288 (3.4271)  loss_scale: 32768.0000 (30924.1012)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5980/40201]  eta: 6:09:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6345 (3.4274)  loss_scale: 32768.0000 (30927.1841)  weight_decay: 0.0500 (0.0500)  time: 0.7383  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5990/40201]  eta: 6:09:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6207 (3.4276)  loss_scale: 32768.0000 (30930.2567)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.0015  max mem: 6186
[2023-07-24 16:19:52,837] [INFO] [logging.py:69:log_dist] [Rank 0] step=3000, skipped=11, lr=[7.135088821681422e-08, 7.135088821681422e-08, 9.513451762241897e-08, 9.513451762241897e-08, 1.2684602349655861e-07, 1.2684602349655861e-07, 1.6912803132874483e-07, 1.6912803132874483e-07, 2.2550404177165977e-07, 2.2550404177165977e-07, 3.0067205569554634e-07, 3.0067205569554634e-07, 4.008960742607285e-07, 4.008960742607285e-07, 5.345280990143046e-07, 5.345280990143046e-07, 7.127041320190729e-07, 7.127041320190729e-07, 9.502721760254305e-07, 9.502721760254305e-07, 1.2670295680339072e-06, 1.2670295680339072e-06, 1.689372757378543e-06, 1.689372757378543e-06, 2.2524970098380575e-06, 2.2524970098380575e-06, 3.003329346450743e-06, 3.003329346450743e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 16:19:52,840] [INFO] [timer.py:181:stop] 0/6000, SamplesPerSec=11.849491085977416
Epoch: [25]  [ 6000/40201]  eta: 6:09:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1996 (3.4279)  loss_scale: 32768.0000 (30933.3191)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6010/40201]  eta: 6:09:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1996 (3.4277)  loss_scale: 32768.0000 (30936.3713)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6020/40201]  eta: 6:09:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1618 (3.4277)  loss_scale: 32768.0000 (30939.4134)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6030/40201]  eta: 6:09:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2210 (3.4277)  loss_scale: 32768.0000 (30942.4454)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0024  max mem: 6186
Epoch: [25]  [ 6040/40201]  eta: 6:08:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2210 (3.4278)  loss_scale: 32768.0000 (30945.4673)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0023  max mem: 6186
Epoch: [25]  [ 6050/40201]  eta: 6:08:48  lr: 0.000003  min_lr: 0.000000  loss: 3.1337 (3.4273)  loss_scale: 32768.0000 (30948.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6060/40201]  eta: 6:08:37  lr: 0.000003  min_lr: 0.000000  loss: 2.9405 (3.4268)  loss_scale: 32768.0000 (30951.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6070/40201]  eta: 6:08:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3867 (3.4271)  loss_scale: 32768.0000 (30954.4734)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0006  max mem: 6186
[2023-07-24 16:20:38,687] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:20:38,687] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:20:38,701] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:20:38,701] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6080/40201]  eta: 6:08:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5007 (3.4271)  loss_scale: 32768.0000 (30979.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0009  max mem: 6186
[2023-07-24 16:20:45,711] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3044
[2023-07-24 16:20:45,711] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:20:45,711] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:20:45,712] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3044
[2023-07-24 16:20:45,713] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 6090/40201]  eta: 6:08:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3288 (3.4271)  loss_scale: 65536.0000 (31024.9851)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6100/40201]  eta: 6:07:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2258 (3.4263)  loss_scale: 32768.0000 (31027.8420)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6110/40201]  eta: 6:07:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1340 (3.4259)  loss_scale: 32768.0000 (31030.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6120/40201]  eta: 6:07:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3904 (3.4260)  loss_scale: 32768.0000 (31033.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6130/40201]  eta: 6:07:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5245 (3.4257)  loss_scale: 32768.0000 (31036.3569)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0282  max mem: 6186
Epoch: [25]  [ 6140/40201]  eta: 6:07:00  lr: 0.000003  min_lr: 0.000000  loss: 3.0836 (3.4254)  loss_scale: 32768.0000 (31039.1767)  weight_decay: 0.0500 (0.0500)  time: 0.4141  data: 0.0276  max mem: 6186
Epoch: [25]  [ 6150/40201]  eta: 6:06:43  lr: 0.000003  min_lr: 0.000000  loss: 3.2412 (3.4255)  loss_scale: 32768.0000 (31041.9873)  weight_decay: 0.0500 (0.0500)  time: 0.3371  data: 0.1296  max mem: 6186
Epoch: [25]  [ 6160/40201]  eta: 6:06:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2451 (3.4256)  loss_scale: 32768.0000 (31044.7888)  weight_decay: 0.0500 (0.0500)  time: 0.4053  data: 0.2047  max mem: 6186
Epoch: [25]  [ 6170/40201]  eta: 6:05:52  lr: 0.000003  min_lr: 0.000000  loss: 3.6663 (3.4261)  loss_scale: 32768.0000 (31047.5813)  weight_decay: 0.0500 (0.0500)  time: 0.3041  data: 0.0915  max mem: 6186
Epoch: [25]  [ 6180/40201]  eta: 6:05:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7908 (3.4266)  loss_scale: 32768.0000 (31050.3647)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0925  max mem: 6186
Epoch: [25]  [ 6190/40201]  eta: 6:05:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6216 (3.4271)  loss_scale: 32768.0000 (31053.1391)  weight_decay: 0.0500 (0.0500)  time: 0.4004  data: 0.1948  max mem: 6186
Epoch: [25]  [ 6200/40201]  eta: 6:04:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6070 (3.4271)  loss_scale: 32768.0000 (31055.9045)  weight_decay: 0.0500 (0.0500)  time: 0.4420  data: 0.1490  max mem: 6186
Epoch: [25]  [ 6210/40201]  eta: 6:04:46  lr: 0.000003  min_lr: 0.000000  loss: 3.1236 (3.4268)  loss_scale: 32768.0000 (31058.6611)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0307  max mem: 6186
Epoch: [25]  [ 6220/40201]  eta: 6:04:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0771 (3.4270)  loss_scale: 32768.0000 (31061.4088)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6230/40201]  eta: 6:04:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1676 (3.4268)  loss_scale: 32768.0000 (31064.1476)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6240/40201]  eta: 6:04:18  lr: 0.000003  min_lr: 0.000000  loss: 3.5940 (3.4273)  loss_scale: 32768.0000 (31066.8777)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6250/40201]  eta: 6:04:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7202 (3.4281)  loss_scale: 32768.0000 (31069.5991)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.1129  max mem: 6186
Epoch: [25]  [ 6260/40201]  eta: 6:04:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7851 (3.4286)  loss_scale: 32768.0000 (31072.3118)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1130  max mem: 6186
Epoch: [25]  [ 6270/40201]  eta: 6:04:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4757 (3.4286)  loss_scale: 32768.0000 (31075.0158)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6280/40201]  eta: 6:03:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4644 (3.4288)  loss_scale: 32768.0000 (31077.7112)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6290/40201]  eta: 6:03:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4561 (3.4287)  loss_scale: 32768.0000 (31080.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6300/40201]  eta: 6:03:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3054 (3.4288)  loss_scale: 32768.0000 (31083.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6310/40201]  eta: 6:03:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4082 (3.4291)  loss_scale: 32768.0000 (31085.7462)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6320/40201]  eta: 6:03:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4082 (3.4292)  loss_scale: 32768.0000 (31088.4075)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6330/40201]  eta: 6:03:02  lr: 0.000003  min_lr: 0.000000  loss: 2.9098 (3.4289)  loss_scale: 32768.0000 (31091.0605)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6340/40201]  eta: 6:02:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4153 (3.4288)  loss_scale: 32768.0000 (31093.7051)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0015  max mem: 6186
[2023-07-24 16:23:03,647] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:23:03,647] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:23:03,649] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:23:03,649] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6350/40201]  eta: 6:02:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5820 (3.4290)  loss_scale: 32768.0000 (31116.9794)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6360/40201]  eta: 6:02:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1531 (3.4283)  loss_scale: 65536.0000 (31171.0888)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6370/40201]  eta: 6:02:21  lr: 0.000003  min_lr: 0.000000  loss: 3.3720 (3.4289)  loss_scale: 65536.0000 (31225.0284)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6380/40201]  eta: 6:02:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6442 (3.4288)  loss_scale: 65536.0000 (31278.7989)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0004  max mem: 6186
[2023-07-24 16:23:28,335] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3193
[2023-07-24 16:23:28,335] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:23:28,373] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3193
[2023-07-24 16:23:28,374] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:23:28,374] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 6390/40201]  eta: 6:02:09  lr: 0.000003  min_lr: 0.000000  loss: 2.9296 (3.4279)  loss_scale: 65536.0000 (31311.8923)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6400/40201]  eta: 6:01:57  lr: 0.000003  min_lr: 0.000000  loss: 3.0195 (3.4285)  loss_scale: 32768.0000 (31314.1672)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6410/40201]  eta: 6:01:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4240 (3.4279)  loss_scale: 32768.0000 (31316.4349)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6420/40201]  eta: 6:01:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1351 (3.4281)  loss_scale: 32768.0000 (31318.6955)  weight_decay: 0.0500 (0.0500)  time: 0.3345  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6430/40201]  eta: 6:00:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2895 (3.4282)  loss_scale: 32768.0000 (31320.9492)  weight_decay: 0.0500 (0.0500)  time: 0.2483  data: 0.0432  max mem: 6186
Epoch: [25]  [ 6440/40201]  eta: 6:00:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3737 (3.4279)  loss_scale: 32768.0000 (31323.1958)  weight_decay: 0.0500 (0.0500)  time: 0.3488  data: 0.1438  max mem: 6186
Epoch: [25]  [ 6450/40201]  eta: 6:00:10  lr: 0.000003  min_lr: 0.000000  loss: 3.3822 (3.4282)  loss_scale: 32768.0000 (31325.4354)  weight_decay: 0.0500 (0.0500)  time: 0.4141  data: 0.1571  max mem: 6186
Epoch: [25]  [ 6460/40201]  eta: 5:59:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5293 (3.4281)  loss_scale: 32768.0000 (31327.6682)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0559  max mem: 6186
Epoch: [25]  [ 6470/40201]  eta: 5:59:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5293 (3.4283)  loss_scale: 32768.0000 (31329.8940)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6480/40201]  eta: 5:59:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5052 (3.4285)  loss_scale: 32768.0000 (31332.1129)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0749  max mem: 6186
Epoch: [25]  [ 6490/40201]  eta: 5:59:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3275 (3.4283)  loss_scale: 32768.0000 (31334.3251)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.0751  max mem: 6186
Epoch: [25]  [ 6500/40201]  eta: 5:59:10  lr: 0.000003  min_lr: 0.000000  loss: 3.0754 (3.4274)  loss_scale: 32768.0000 (31336.5304)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6510/40201]  eta: 5:58:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2303 (3.4275)  loss_scale: 32768.0000 (31338.7289)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6520/40201]  eta: 5:58:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4158 (3.4274)  loss_scale: 32768.0000 (31340.9207)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6530/40201]  eta: 5:58:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5570 (3.4275)  loss_scale: 32768.0000 (31343.1058)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6540/40201]  eta: 5:58:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6275 (3.4283)  loss_scale: 32768.0000 (31345.2842)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6550/40201]  eta: 5:58:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3512 (3.4282)  loss_scale: 32768.0000 (31347.4560)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6560/40201]  eta: 5:58:12  lr: 0.000003  min_lr: 0.000000  loss: 3.3512 (3.4286)  loss_scale: 32768.0000 (31349.6211)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6570/40201]  eta: 5:57:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2101 (3.4282)  loss_scale: 32768.0000 (31351.7796)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6580/40201]  eta: 5:57:29  lr: 0.000003  min_lr: 0.000000  loss: 2.8759 (3.4275)  loss_scale: 32768.0000 (31353.9316)  weight_decay: 0.0500 (0.0500)  time: 0.3450  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6590/40201]  eta: 5:57:07  lr: 0.000003  min_lr: 0.000000  loss: 3.1808 (3.4280)  loss_scale: 32768.0000 (31356.0771)  weight_decay: 0.0500 (0.0500)  time: 0.2697  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6600/40201]  eta: 5:57:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4991 (3.4280)  loss_scale: 32768.0000 (31358.2160)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0027  max mem: 6186
Epoch: [25]  [ 6610/40201]  eta: 5:56:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3001 (3.4282)  loss_scale: 32768.0000 (31360.3485)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0027  max mem: 6186
Epoch: [25]  [ 6620/40201]  eta: 5:56:29  lr: 0.000003  min_lr: 0.000000  loss: 3.6323 (3.4288)  loss_scale: 32768.0000 (31362.4746)  weight_decay: 0.0500 (0.0500)  time: 0.3839  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6630/40201]  eta: 5:56:09  lr: 0.000003  min_lr: 0.000000  loss: 3.4414 (3.4282)  loss_scale: 32768.0000 (31364.5942)  weight_decay: 0.0500 (0.0500)  time: 0.4171  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6640/40201]  eta: 5:56:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1948 (3.4284)  loss_scale: 32768.0000 (31366.7074)  weight_decay: 0.0500 (0.0500)  time: 0.4683  data: 0.0013  max mem: 6186
[2023-07-24 16:25:31,997] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:25:31,997] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:25:32,004] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:25:32,004] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6650/40201]  eta: 5:55:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4490 (3.4284)  loss_scale: 32768.0000 (31398.3750)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6660/40201]  eta: 5:55:39  lr: 0.000003  min_lr: 0.000000  loss: 3.6268 (3.4288)  loss_scale: 65536.0000 (31449.6250)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0011  max mem: 6186
[2023-07-24 16:25:44,200] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3333
[2023-07-24 16:25:44,200] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:25:44,200] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:25:44,206] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3333
[2023-07-24 16:25:44,207] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 6670/40201]  eta: 5:55:27  lr: 0.000003  min_lr: 0.000000  loss: 3.6810 (3.4286)  loss_scale: 65536.0000 (31481.0733)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6680/40201]  eta: 5:55:15  lr: 0.000003  min_lr: 0.000000  loss: 3.1933 (3.4286)  loss_scale: 32768.0000 (31482.9996)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6690/40201]  eta: 5:55:03  lr: 0.000003  min_lr: 0.000000  loss: 2.9516 (3.4278)  loss_scale: 32768.0000 (31484.9200)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0016  max mem: 6186
Epoch: [25]  [ 6700/40201]  eta: 5:54:51  lr: 0.000003  min_lr: 0.000000  loss: 2.9156 (3.4274)  loss_scale: 32768.0000 (31486.8348)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6710/40201]  eta: 5:54:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2204 (3.4277)  loss_scale: 32768.0000 (31488.7439)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6720/40201]  eta: 5:54:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5161 (3.4282)  loss_scale: 32768.0000 (31490.6472)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6730/40201]  eta: 5:54:17  lr: 0.000003  min_lr: 0.000000  loss: 3.2960 (3.4277)  loss_scale: 32768.0000 (31492.5449)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6740/40201]  eta: 5:54:06  lr: 0.000003  min_lr: 0.000000  loss: 3.0863 (3.4276)  loss_scale: 32768.0000 (31494.4370)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6750/40201]  eta: 5:53:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4285 (3.4279)  loss_scale: 32768.0000 (31496.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6760/40201]  eta: 5:53:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3583 (3.4278)  loss_scale: 32768.0000 (31498.2044)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6770/40201]  eta: 5:53:34  lr: 0.000003  min_lr: 0.000000  loss: 2.9510 (3.4274)  loss_scale: 32768.0000 (31500.0798)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6780/40201]  eta: 5:53:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2367 (3.4276)  loss_scale: 32768.0000 (31501.9496)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6790/40201]  eta: 5:53:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2367 (3.4276)  loss_scale: 32768.0000 (31503.8139)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0531  max mem: 6186
Epoch: [25]  [ 6800/40201]  eta: 5:53:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1888 (3.4279)  loss_scale: 32768.0000 (31505.6727)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0532  max mem: 6186
Epoch: [25]  [ 6810/40201]  eta: 5:52:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8773 (3.4286)  loss_scale: 32768.0000 (31507.5261)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6820/40201]  eta: 5:52:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7831 (3.4284)  loss_scale: 32768.0000 (31509.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6830/40201]  eta: 5:52:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2346 (3.4280)  loss_scale: 32768.0000 (31511.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6840/40201]  eta: 5:52:27  lr: 0.000003  min_lr: 0.000000  loss: 3.4309 (3.4286)  loss_scale: 32768.0000 (31513.0536)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6850/40201]  eta: 5:52:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7335 (3.4287)  loss_scale: 32768.0000 (31514.8854)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6860/40201]  eta: 5:52:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2834 (3.4285)  loss_scale: 32768.0000 (31516.7118)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6870/40201]  eta: 5:51:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1555 (3.4282)  loss_scale: 32768.0000 (31518.5330)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6880/40201]  eta: 5:51:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2669 (3.4279)  loss_scale: 32768.0000 (31520.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0017  max mem: 6186
Epoch: [25]  [ 6890/40201]  eta: 5:51:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2130 (3.4273)  loss_scale: 32768.0000 (31522.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0019  max mem: 6186
Epoch: [25]  [ 6900/40201]  eta: 5:51:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2220 (3.4274)  loss_scale: 32768.0000 (31523.9646)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6910/40201]  eta: 5:51:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1360 (3.4269)  loss_scale: 32768.0000 (31525.7647)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6920/40201]  eta: 5:51:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6105 (3.4276)  loss_scale: 32768.0000 (31527.5596)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1226  max mem: 6186
[2023-07-24 16:28:10,165] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:28:10,165] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:28:10,179] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:28:10,180] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6930/40201]  eta: 5:51:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6333 (3.4277)  loss_scale: 32768.0000 (31557.7158)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.1231  max mem: 6186
Epoch: [25]  [ 6940/40201]  eta: 5:51:04  lr: 0.000003  min_lr: 0.000000  loss: 3.1003 (3.4272)  loss_scale: 65536.0000 (31606.6688)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6950/40201]  eta: 5:50:54  lr: 0.000003  min_lr: 0.000000  loss: 3.0315 (3.4265)  loss_scale: 65536.0000 (31655.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6960/40201]  eta: 5:50:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1464 (3.4262)  loss_scale: 65536.0000 (31704.1529)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6970/40201]  eta: 5:50:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4836 (3.4263)  loss_scale: 65536.0000 (31752.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6980/40201]  eta: 5:50:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2074 (3.4257)  loss_scale: 65536.0000 (31801.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6990/40201]  eta: 5:50:19  lr: 0.000003  min_lr: 0.000000  loss: 2.9630 (3.4251)  loss_scale: 65536.0000 (31849.3331)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0014  max mem: 6186
[2023-07-24 16:28:48,657] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3496
[2023-07-24 16:28:48,657] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:28:48,663] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3496
[2023-07-24 16:28:48,664] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:28:48,664] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:28:52,256] [INFO] [timer.py:181:stop] 0/7000, SamplesPerSec=12.025857282128989
Epoch: [25]  [ 7000/40201]  eta: 5:50:09  lr: 0.000003  min_lr: 0.000000  loss: 2.9969 (3.4246)  loss_scale: 65536.0000 (31860.0063)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0017  max mem: 6186
Epoch: [25]  [ 7010/40201]  eta: 5:50:05  lr: 0.000003  min_lr: 0.000000  loss: 3.1373 (3.4244)  loss_scale: 32768.0000 (31861.3014)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7020/40201]  eta: 5:49:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2410 (3.4240)  loss_scale: 32768.0000 (31862.5928)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7030/40201]  eta: 5:49:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1646 (3.4240)  loss_scale: 32768.0000 (31863.8805)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0010  max mem: 6186
Epoch: [25]  [ 7040/40201]  eta: 5:49:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0972 (3.4236)  loss_scale: 32768.0000 (31865.1646)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7050/40201]  eta: 5:49:30  lr: 0.000003  min_lr: 0.000000  loss: 3.0972 (3.4238)  loss_scale: 32768.0000 (31866.4450)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0298  max mem: 6186
Epoch: [25]  [ 7060/40201]  eta: 5:49:20  lr: 0.000003  min_lr: 0.000000  loss: 3.6823 (3.4244)  loss_scale: 32768.0000 (31867.7219)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0300  max mem: 6186
Epoch: [25]  [ 7070/40201]  eta: 5:49:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7900 (3.4246)  loss_scale: 32768.0000 (31868.9951)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0782  max mem: 6186
Epoch: [25]  [ 7080/40201]  eta: 5:49:06  lr: 0.000003  min_lr: 0.000000  loss: 3.4126 (3.4243)  loss_scale: 32768.0000 (31870.2647)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0783  max mem: 6186
Epoch: [25]  [ 7090/40201]  eta: 5:48:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4126 (3.4246)  loss_scale: 32768.0000 (31871.5307)  weight_decay: 0.0500 (0.0500)  time: 0.4564  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7100/40201]  eta: 5:48:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4824 (3.4253)  loss_scale: 32768.0000 (31872.7931)  weight_decay: 0.0500 (0.0500)  time: 0.3182  data: 0.0119  max mem: 6186
Epoch: [25]  [ 7110/40201]  eta: 5:48:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5441 (3.4251)  loss_scale: 32768.0000 (31874.0520)  weight_decay: 0.0500 (0.0500)  time: 0.3222  data: 0.0440  max mem: 6186
Epoch: [25]  [ 7120/40201]  eta: 5:47:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3879 (3.4248)  loss_scale: 32768.0000 (31875.3074)  weight_decay: 0.0500 (0.0500)  time: 0.4593  data: 0.0951  max mem: 6186
Epoch: [25]  [ 7130/40201]  eta: 5:47:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2397 (3.4249)  loss_scale: 32768.0000 (31876.5592)  weight_decay: 0.0500 (0.0500)  time: 0.4576  data: 0.0624  max mem: 6186
Epoch: [25]  [ 7140/40201]  eta: 5:47:21  lr: 0.000003  min_lr: 0.000000  loss: 3.2643 (3.4249)  loss_scale: 32768.0000 (31877.8076)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0004  max mem: 6186
Epoch: [25]  [ 7150/40201]  eta: 5:47:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6127 (3.4250)  loss_scale: 32768.0000 (31879.0524)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.2433  max mem: 6186
Epoch: [25]  [ 7160/40201]  eta: 5:47:22  lr: 0.000003  min_lr: 0.000000  loss: 3.6345 (3.4254)  loss_scale: 32768.0000 (31880.2938)  weight_decay: 0.0500 (0.0500)  time: 0.7819  data: 0.2443  max mem: 6186
Epoch: [25]  [ 7170/40201]  eta: 5:47:14  lr: 0.000003  min_lr: 0.000000  loss: 3.7924 (3.4259)  loss_scale: 32768.0000 (31881.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0015  max mem: 6186
Epoch: [25]  [ 7180/40201]  eta: 5:47:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7164 (3.4255)  loss_scale: 32768.0000 (31882.7662)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0005  max mem: 6186
Epoch: [25]  [ 7190/40201]  eta: 5:47:02  lr: 0.000003  min_lr: 0.000000  loss: 3.1195 (3.4251)  loss_scale: 32768.0000 (31883.9972)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0523  max mem: 6186
Epoch: [25]  [ 7200/40201]  eta: 5:46:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3015 (3.4252)  loss_scale: 32768.0000 (31885.2248)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0530  max mem: 6186
Epoch: [25]  [ 7210/40201]  eta: 5:46:45  lr: 0.000003  min_lr: 0.000000  loss: 2.9965 (3.4242)  loss_scale: 32768.0000 (31886.4490)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7220/40201]  eta: 5:46:35  lr: 0.000003  min_lr: 0.000000  loss: 2.7457 (3.4236)  loss_scale: 32768.0000 (31887.6699)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7230/40201]  eta: 5:46:27  lr: 0.000003  min_lr: 0.000000  loss: 2.8621 (3.4230)  loss_scale: 32768.0000 (31888.8873)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0014  max mem: 6186
Epoch: [25]  [ 7240/40201]  eta: 5:46:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1126 (3.4232)  loss_scale: 32768.0000 (31890.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7250/40201]  eta: 5:46:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4882 (3.4232)  loss_scale: 32768.0000 (31891.3121)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0008  max mem: 6186
[2023-07-24 16:31:14,119] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:31:14,119] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:31:14,121] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:31:14,121] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 7260/40201]  eta: 5:46:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3232 (3.4231)  loss_scale: 32768.0000 (31937.6483)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0010  max mem: 6186
Epoch: [25]  [ 7270/40201]  eta: 5:45:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1991 (3.4228)  loss_scale: 65536.0000 (31983.8570)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7280/40201]  eta: 5:45:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1991 (3.4231)  loss_scale: 65536.0000 (32029.9387)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0006  max mem: 6186
Epoch: [25]  [ 7290/40201]  eta: 5:45:43  lr: 0.000003  min_lr: 0.000000  loss: 3.5026 (3.4235)  loss_scale: 65536.0000 (32075.8941)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0719  max mem: 6186
Epoch: [25]  [ 7300/40201]  eta: 5:45:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5111 (3.4234)  loss_scale: 65536.0000 (32121.7236)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.0720  max mem: 6186
Epoch: [25]  [ 7310/40201]  eta: 5:45:45  lr: 0.000003  min_lr: 0.000000  loss: 3.5111 (3.4236)  loss_scale: 65536.0000 (32167.4277)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.2221  max mem: 6186
Epoch: [25]  [ 7320/40201]  eta: 5:45:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2506 (3.4231)  loss_scale: 65536.0000 (32213.0070)  weight_decay: 0.0500 (0.0500)  time: 0.8079  data: 0.2230  max mem: 6186
Epoch: [25]  [ 7330/40201]  eta: 5:45:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2128 (3.4229)  loss_scale: 65536.0000 (32258.4619)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0017  max mem: 6186
Epoch: [25]  [ 7340/40201]  eta: 5:45:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5719 (3.4231)  loss_scale: 65536.0000 (32303.7929)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0011  max mem: 6186
Epoch: [25]  [ 7350/40201]  eta: 5:45:11  lr: 0.000003  min_lr: 0.000000  loss: 3.8884 (3.4233)  loss_scale: 65536.0000 (32349.0007)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7360/40201]  eta: 5:45:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7540 (3.4235)  loss_scale: 65536.0000 (32394.0856)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7370/40201]  eta: 5:44:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3840 (3.4236)  loss_scale: 65536.0000 (32439.0482)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7380/40201]  eta: 5:44:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2665 (3.4233)  loss_scale: 65536.0000 (32483.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7390/40201]  eta: 5:44:36  lr: 0.000003  min_lr: 0.000000  loss: 3.2696 (3.4235)  loss_scale: 65536.0000 (32528.6083)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7400/40201]  eta: 5:44:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2696 (3.4231)  loss_scale: 65536.0000 (32573.2069)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0006  max mem: 6186
Epoch: [25]  [ 7410/40201]  eta: 5:44:17  lr: 0.000003  min_lr: 0.000000  loss: 2.9789 (3.4227)  loss_scale: 65536.0000 (32617.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7420/40201]  eta: 5:44:10  lr: 0.000003  min_lr: 0.000000  loss: 3.4522 (3.4227)  loss_scale: 65536.0000 (32662.0434)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0193  max mem: 6186
Epoch: [25]  [ 7430/40201]  eta: 5:44:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4552 (3.4229)  loss_scale: 65536.0000 (32706.2823)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0195  max mem: 6186
Epoch: [25]  [ 7440/40201]  eta: 5:43:51  lr: 0.000003  min_lr: 0.000000  loss: 3.1767 (3.4226)  loss_scale: 65536.0000 (32750.4024)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7450/40201]  eta: 5:43:43  lr: 0.000003  min_lr: 0.000000  loss: 3.1746 (3.4226)  loss_scale: 65536.0000 (32794.4040)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0010  max mem: 6186
Epoch: [25]  [ 7460/40201]  eta: 5:43:35  lr: 0.000003  min_lr: 0.000000  loss: 2.9937 (3.4225)  loss_scale: 65536.0000 (32838.2876)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7470/40201]  eta: 5:43:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0821 (3.4225)  loss_scale: 65536.0000 (32882.0538)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0004  max mem: 6186
Epoch: [25]  [ 7480/40201]  eta: 5:43:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4343 (3.4227)  loss_scale: 65536.0000 (32925.7030)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0008  max mem: 6186
[2023-07-24 16:33:34,338] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3742
[2023-07-24 16:33:34,339] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:33:34,340] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3742
[2023-07-24 16:33:34,340] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:33:34,341] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 7490/40201]  eta: 5:43:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3566 (3.4225)  loss_scale: 65536.0000 (32942.9897)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7500/40201]  eta: 5:42:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4772 (3.4228)  loss_scale: 32768.0000 (32942.7564)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7510/40201]  eta: 5:42:50  lr: 0.000003  min_lr: 0.000000  loss: 3.5354 (3.4233)  loss_scale: 32768.0000 (32942.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7520/40201]  eta: 5:42:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3895 (3.4230)  loss_scale: 32768.0000 (32942.2917)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7530/40201]  eta: 5:42:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6832 (3.4236)  loss_scale: 32768.0000 (32942.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7540/40201]  eta: 5:42:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1795 (3.4231)  loss_scale: 32768.0000 (32941.8295)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7550/40201]  eta: 5:42:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4506 (3.4240)  loss_scale: 32768.0000 (32941.5993)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7560/40201]  eta: 5:41:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4909 (3.4234)  loss_scale: 32768.0000 (32941.3697)  weight_decay: 0.0500 (0.0500)  time: 0.4716  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7570/40201]  eta: 5:41:37  lr: 0.000003  min_lr: 0.000000  loss: 2.8596 (3.4230)  loss_scale: 32768.0000 (32941.1407)  weight_decay: 0.0500 (0.0500)  time: 0.3676  data: 0.0010  max mem: 6186
Epoch: [25]  [ 7580/40201]  eta: 5:41:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1915 (3.4232)  loss_scale: 32768.0000 (32940.9123)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7590/40201]  eta: 5:41:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4898 (3.4239)  loss_scale: 32768.0000 (32940.6845)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7600/40201]  eta: 5:41:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4898 (3.4236)  loss_scale: 32768.0000 (32940.4573)  weight_decay: 0.0500 (0.0500)  time: 0.4839  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7610/40201]  eta: 5:41:17  lr: 0.000003  min_lr: 0.000000  loss: 3.1712 (3.4238)  loss_scale: 32768.0000 (32940.2307)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.0014  max mem: 6186
Epoch: [25]  [ 7620/40201]  eta: 5:41:15  lr: 0.000003  min_lr: 0.000000  loss: 3.1712 (3.4236)  loss_scale: 32768.0000 (32940.0047)  weight_decay: 0.0500 (0.0500)  time: 0.7272  data: 0.0011  max mem: 6186
Epoch: [25]  [ 7630/40201]  eta: 5:41:07  lr: 0.000003  min_lr: 0.000000  loss: 2.9895 (3.4231)  loss_scale: 32768.0000 (32939.7793)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7640/40201]  eta: 5:40:59  lr: 0.000003  min_lr: 0.000000  loss: 3.0368 (3.4228)  loss_scale: 32768.0000 (32939.5545)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7650/40201]  eta: 5:40:50  lr: 0.000003  min_lr: 0.000000  loss: 2.9384 (3.4223)  loss_scale: 32768.0000 (32939.3303)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7660/40201]  eta: 5:40:50  lr: 0.000003  min_lr: 0.000000  loss: 2.9813 (3.4225)  loss_scale: 32768.0000 (32939.1066)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.0011  max mem: 6186
Epoch: [25]  [ 7670/40201]  eta: 5:40:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1935 (3.4222)  loss_scale: 32768.0000 (32938.8836)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.0010  max mem: 6186
Epoch: [25]  [ 7680/40201]  eta: 5:40:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1935 (3.4222)  loss_scale: 32768.0000 (32938.6611)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7690/40201]  eta: 5:40:21  lr: 0.000003  min_lr: 0.000000  loss: 3.0678 (3.4219)  loss_scale: 32768.0000 (32938.4392)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7700/40201]  eta: 5:40:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4081 (3.4223)  loss_scale: 32768.0000 (32938.2179)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0009  max mem: 6186
Epoch: [25]  [ 7710/40201]  eta: 5:40:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4081 (3.4220)  loss_scale: 32768.0000 (32937.9971)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7720/40201]  eta: 5:39:51  lr: 0.000003  min_lr: 0.000000  loss: 3.1463 (3.4217)  loss_scale: 32768.0000 (32937.7770)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0005  max mem: 6186
Epoch: [25]  [ 7730/40201]  eta: 5:39:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4626 (3.4224)  loss_scale: 32768.0000 (32937.5574)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0006  max mem: 6186
Epoch: [25]  [ 7740/40201]  eta: 5:39:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6995 (3.4227)  loss_scale: 32768.0000 (32937.3383)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0008  max mem: 6186
[2023-07-24 16:36:02,263] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:36:02,263] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:36:02,265] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:36:02,266] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 7750/40201]  eta: 5:39:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1079 (3.4219)  loss_scale: 32768.0000 (32970.9405)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7760/40201]  eta: 5:39:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2331 (3.4224)  loss_scale: 65536.0000 (33012.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7770/40201]  eta: 5:39:02  lr: 0.000003  min_lr: 0.000000  loss: 3.5371 (3.4224)  loss_scale: 65536.0000 (33054.7523)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7780/40201]  eta: 5:38:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2981 (3.4222)  loss_scale: 65536.0000 (33096.4966)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7790/40201]  eta: 5:38:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4154 (3.4225)  loss_scale: 65536.0000 (33138.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7800/40201]  eta: 5:38:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2616 (3.4222)  loss_scale: 65536.0000 (33179.6641)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7810/40201]  eta: 5:38:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1856 (3.4222)  loss_scale: 65536.0000 (33221.0882)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0005  max mem: 6186
Epoch: [25]  [ 7820/40201]  eta: 5:38:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4649 (3.4222)  loss_scale: 65536.0000 (33262.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0005  max mem: 6186
Epoch: [25]  [ 7830/40201]  eta: 5:38:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2900 (3.4220)  loss_scale: 65536.0000 (33303.6190)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0006  max mem: 6186
Epoch: [25]  [ 7840/40201]  eta: 5:38:03  lr: 0.000003  min_lr: 0.000000  loss: 3.0967 (3.4217)  loss_scale: 65536.0000 (33344.7264)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7850/40201]  eta: 5:37:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1752 (3.4221)  loss_scale: 65536.0000 (33385.7292)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7860/40201]  eta: 5:37:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5025 (3.4217)  loss_scale: 65536.0000 (33426.6277)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0014  max mem: 6186
Epoch: [25]  [ 7870/40201]  eta: 5:37:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6945 (3.4222)  loss_scale: 65536.0000 (33467.4222)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0013  max mem: 6186
Epoch: [25]  [ 7880/40201]  eta: 5:37:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6945 (3.4228)  loss_scale: 65536.0000 (33508.1132)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0015  max mem: 6186
Epoch: [25]  [ 7890/40201]  eta: 5:37:21  lr: 0.000003  min_lr: 0.000000  loss: 3.6998 (3.4235)  loss_scale: 65536.0000 (33548.7011)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0012  max mem: 6186
Epoch: [25]  [ 7900/40201]  eta: 5:37:03  lr: 0.000003  min_lr: 0.000000  loss: 3.6671 (3.4235)  loss_scale: 65536.0000 (33589.1862)  weight_decay: 0.0500 (0.0500)  time: 0.4444  data: 0.0007  max mem: 6186
Epoch: [25]  [ 7910/40201]  eta: 5:36:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5912 (3.4239)  loss_scale: 65536.0000 (33629.5690)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7920/40201]  eta: 5:36:25  lr: 0.000003  min_lr: 0.000000  loss: 3.4373 (3.4240)  loss_scale: 65536.0000 (33669.8498)  weight_decay: 0.0500 (0.0500)  time: 0.3132  data: 0.0301  max mem: 6186
Epoch: [25]  [ 7930/40201]  eta: 5:36:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2387 (3.4238)  loss_scale: 65536.0000 (33710.0290)  weight_decay: 0.0500 (0.0500)  time: 0.4347  data: 0.0606  max mem: 6186
Epoch: [25]  [ 7940/40201]  eta: 5:36:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4119 (3.4242)  loss_scale: 65536.0000 (33750.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0794  max mem: 6186
Epoch: [25]  [ 7950/40201]  eta: 5:36:12  lr: 0.000003  min_lr: 0.000000  loss: 3.3948 (3.4241)  loss_scale: 65536.0000 (33790.0843)  weight_decay: 0.0500 (0.0500)  time: 0.7608  data: 0.3009  max mem: 6186
Epoch: [25]  [ 7960/40201]  eta: 5:36:03  lr: 0.000003  min_lr: 0.000000  loss: 2.9989 (3.4237)  loss_scale: 65536.0000 (33829.9611)  weight_decay: 0.0500 (0.0500)  time: 0.7261  data: 0.2527  max mem: 6186
Epoch: [25]  [ 7970/40201]  eta: 5:36:10  lr: 0.000003  min_lr: 0.000000  loss: 2.8956 (3.4231)  loss_scale: 65536.0000 (33869.7378)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.0008  max mem: 6186
Epoch: [25]  [ 7980/40201]  eta: 5:36:01  lr: 0.000003  min_lr: 0.000000  loss: 2.9542 (3.4230)  loss_scale: 65536.0000 (33909.4149)  weight_decay: 0.0500 (0.0500)  time: 0.7604  data: 0.0009  max mem: 6186
[2023-07-24 16:38:17,057] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3990
[2023-07-24 16:38:17,057] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:38:17,069] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3990
[2023-07-24 16:38:17,070] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:38:17,070] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 7990/40201]  eta: 5:35:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5966 (3.4235)  loss_scale: 32768.0000 (33907.9865)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0011  max mem: 6186
[2023-07-24 16:38:27,131] [INFO] [logging.py:69:log_dist] [Rank 0] step=4000, skipped=17, lr=[6.99478557143514e-08, 6.99478557143514e-08, 9.32638076191352e-08, 9.32638076191352e-08, 1.2435174349218027e-07, 1.2435174349218027e-07, 1.6580232465624037e-07, 1.6580232465624037e-07, 2.2106976620832048e-07, 2.2106976620832048e-07, 2.9475968827776064e-07, 2.9475968827776064e-07, 3.930129177036808e-07, 3.930129177036808e-07, 5.240172236049078e-07, 5.240172236049078e-07, 6.986896314732104e-07, 6.986896314732104e-07, 9.315861752976139e-07, 9.315861752976139e-07, 1.2421149003968184e-06, 1.2421149003968184e-06, 1.6561532005290913e-06, 1.6561532005290913e-06, 2.2082042673721217e-06, 2.2082042673721217e-06, 2.9442723564961623e-06, 2.9442723564961623e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 16:38:27,133] [INFO] [timer.py:181:stop] 0/8000, SamplesPerSec=12.079567794084305
Epoch: [25]  [ 8000/40201]  eta: 5:35:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6856 (3.4235)  loss_scale: 32768.0000 (33906.5617)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0013  max mem: 6186
Epoch: [25]  [ 8010/40201]  eta: 5:35:33  lr: 0.000003  min_lr: 0.000000  loss: 3.5560 (3.4237)  loss_scale: 32768.0000 (33905.1404)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0017  max mem: 6186
Epoch: [25]  [ 8020/40201]  eta: 5:35:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2555 (3.4234)  loss_scale: 32768.0000 (33903.7227)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0014  max mem: 6186
Epoch: [25]  [ 8030/40201]  eta: 5:35:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2678 (3.4232)  loss_scale: 32768.0000 (33902.3086)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0010  max mem: 6186
Epoch: [25]  [ 8040/40201]  eta: 5:35:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6417 (3.4239)  loss_scale: 32768.0000 (33900.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0011  max mem: 6186
Epoch: [25]  [ 8050/40201]  eta: 5:35:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6417 (3.4236)  loss_scale: 32768.0000 (33899.4907)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0008  max mem: 6186
Epoch: [25]  [ 8060/40201]  eta: 5:34:53  lr: 0.000003  min_lr: 0.000000  loss: 2.8926 (3.4232)  loss_scale: 32768.0000 (33898.0871)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8070/40201]  eta: 5:34:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3541 (3.4230)  loss_scale: 32768.0000 (33896.6869)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8080/40201]  eta: 5:34:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2925 (3.4225)  loss_scale: 32768.0000 (33895.2902)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8090/40201]  eta: 5:34:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3007 (3.4230)  loss_scale: 32768.0000 (33893.8969)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8100/40201]  eta: 5:34:19  lr: 0.000003  min_lr: 0.000000  loss: 3.8178 (3.4235)  loss_scale: 32768.0000 (33892.5071)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8110/40201]  eta: 5:34:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7233 (3.4238)  loss_scale: 32768.0000 (33891.1207)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0013  max mem: 6186
Epoch: [25]  [ 8120/40201]  eta: 5:34:03  lr: 0.000003  min_lr: 0.000000  loss: 3.4398 (3.4238)  loss_scale: 32768.0000 (33889.7377)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0011  max mem: 6186
Epoch: [25]  [ 8130/40201]  eta: 5:33:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2006 (3.4235)  loss_scale: 32768.0000 (33888.3581)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8140/40201]  eta: 5:33:46  lr: 0.000003  min_lr: 0.000000  loss: 3.1061 (3.4227)  loss_scale: 32768.0000 (33886.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0008  max mem: 6186
Epoch: [25]  [ 8150/40201]  eta: 5:33:38  lr: 0.000003  min_lr: 0.000000  loss: 3.1486 (3.4229)  loss_scale: 32768.0000 (33885.6091)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8160/40201]  eta: 5:33:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7148 (3.4230)  loss_scale: 32768.0000 (33884.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0010  max mem: 6186
Epoch: [25]  [ 8170/40201]  eta: 5:33:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4240 (3.4225)  loss_scale: 32768.0000 (33882.8736)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0019  max mem: 6186
Epoch: [25]  [ 8180/40201]  eta: 5:33:26  lr: 0.000003  min_lr: 0.000000  loss: 3.0998 (3.4223)  loss_scale: 32768.0000 (33881.5108)  weight_decay: 0.0500 (0.0500)  time: 0.7347  data: 0.1391  max mem: 6186
Epoch: [25]  [ 8190/40201]  eta: 5:33:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2007 (3.4221)  loss_scale: 32768.0000 (33880.1514)  weight_decay: 0.0500 (0.0500)  time: 0.7318  data: 0.1393  max mem: 6186
Epoch: [25]  [ 8200/40201]  eta: 5:33:10  lr: 0.000003  min_lr: 0.000000  loss: 3.4579 (3.4225)  loss_scale: 32768.0000 (33878.7953)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0015  max mem: 6186
Epoch: [25]  [ 8210/40201]  eta: 5:33:03  lr: 0.000003  min_lr: 0.000000  loss: 3.7050 (3.4227)  loss_scale: 32768.0000 (33877.4425)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0015  max mem: 6186
Epoch: [25]  [ 8220/40201]  eta: 5:32:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3799 (3.4226)  loss_scale: 32768.0000 (33876.0929)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0024  max mem: 6186
Epoch: [25]  [ 8230/40201]  eta: 5:32:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3066 (3.4224)  loss_scale: 32768.0000 (33874.7467)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0014  max mem: 6186
[2023-07-24 16:40:47,706] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:40:47,707] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:40:47,712] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:40:47,712] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 8240/40201]  eta: 5:32:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2108 (3.4224)  loss_scale: 32768.0000 (33881.3561)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8250/40201]  eta: 5:32:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2108 (3.4228)  loss_scale: 65536.0000 (33919.7208)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8260/40201]  eta: 5:32:15  lr: 0.000003  min_lr: 0.000000  loss: 3.5368 (3.4231)  loss_scale: 65536.0000 (33957.9925)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8270/40201]  eta: 5:32:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5368 (3.4235)  loss_scale: 65536.0000 (33996.1717)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8280/40201]  eta: 5:31:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1316 (3.4234)  loss_scale: 65536.0000 (34034.2587)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8290/40201]  eta: 5:31:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1316 (3.4237)  loss_scale: 65536.0000 (34072.2538)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8300/40201]  eta: 5:31:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4091 (3.4236)  loss_scale: 65536.0000 (34110.1573)  weight_decay: 0.0500 (0.0500)  time: 0.3258  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8310/40201]  eta: 5:31:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4091 (3.4235)  loss_scale: 65536.0000 (34147.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.3566  max mem: 6186
[2023-07-24 16:41:24,888] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4155
[2023-07-24 16:41:24,888] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4155
[2023-07-24 16:41:24,888] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:41:24,888] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:41:24,888] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 8320/40201]  eta: 5:31:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3419 (3.4233)  loss_scale: 32768.0000 (34146.3113)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.4392  max mem: 6186
Epoch: [25]  [ 8330/40201]  eta: 5:30:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3207 (3.4231)  loss_scale: 32768.0000 (34144.6568)  weight_decay: 0.0500 (0.0500)  time: 0.3586  data: 0.1587  max mem: 6186
Epoch: [25]  [ 8340/40201]  eta: 5:30:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4731 (3.4230)  loss_scale: 32768.0000 (34143.0064)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0979  max mem: 6186
Epoch: [25]  [ 8350/40201]  eta: 5:30:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4502 (3.4230)  loss_scale: 32768.0000 (34141.3598)  weight_decay: 0.0500 (0.0500)  time: 0.3857  data: 0.0810  max mem: 6186
Epoch: [25]  [ 8360/40201]  eta: 5:30:11  lr: 0.000003  min_lr: 0.000000  loss: 3.5076 (3.4233)  loss_scale: 32768.0000 (34139.7173)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0594  max mem: 6186
Epoch: [25]  [ 8370/40201]  eta: 5:30:02  lr: 0.000003  min_lr: 0.000000  loss: 3.3465 (3.4231)  loss_scale: 32768.0000 (34138.0786)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8380/40201]  eta: 5:29:52  lr: 0.000003  min_lr: 0.000000  loss: 3.2619 (3.4234)  loss_scale: 32768.0000 (34136.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8390/40201]  eta: 5:29:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1736 (3.4232)  loss_scale: 32768.0000 (34134.8130)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8400/40201]  eta: 5:29:33  lr: 0.000003  min_lr: 0.000000  loss: 3.0573 (3.4232)  loss_scale: 32768.0000 (34133.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8410/40201]  eta: 5:29:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2567 (3.4233)  loss_scale: 32768.0000 (34131.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8420/40201]  eta: 5:29:15  lr: 0.000003  min_lr: 0.000000  loss: 3.3826 (3.4235)  loss_scale: 32768.0000 (34129.9437)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8430/40201]  eta: 5:29:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7405 (3.4239)  loss_scale: 32768.0000 (34128.3283)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0008  max mem: 6186
Epoch: [25]  [ 8440/40201]  eta: 5:28:57  lr: 0.000003  min_lr: 0.000000  loss: 3.7405 (3.4239)  loss_scale: 32768.0000 (34126.7167)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0009  max mem: 6186
Epoch: [25]  [ 8450/40201]  eta: 5:28:47  lr: 0.000003  min_lr: 0.000000  loss: 3.7115 (3.4245)  loss_scale: 32768.0000 (34125.1090)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0008  max mem: 6186
Epoch: [25]  [ 8460/40201]  eta: 5:28:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4277 (3.4241)  loss_scale: 32768.0000 (34123.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8470/40201]  eta: 5:28:28  lr: 0.000003  min_lr: 0.000000  loss: 3.1351 (3.4239)  loss_scale: 32768.0000 (34121.9049)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8480/40201]  eta: 5:28:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4429 (3.4241)  loss_scale: 32768.0000 (34120.3085)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8490/40201]  eta: 5:28:12  lr: 0.000003  min_lr: 0.000000  loss: 3.2329 (3.4236)  loss_scale: 32768.0000 (34118.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0009  max mem: 6186
Epoch: [25]  [ 8500/40201]  eta: 5:28:04  lr: 0.000003  min_lr: 0.000000  loss: 2.9496 (3.4236)  loss_scale: 32768.0000 (34117.1269)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0008  max mem: 6186
Epoch: [25]  [ 8510/40201]  eta: 5:27:56  lr: 0.000003  min_lr: 0.000000  loss: 3.7609 (3.4240)  loss_scale: 32768.0000 (34115.5418)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8520/40201]  eta: 5:27:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2831 (3.4236)  loss_scale: 32768.0000 (34113.9603)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8530/40201]  eta: 5:27:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1160 (3.4234)  loss_scale: 32768.0000 (34112.3826)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0007  max mem: 6186
Epoch: [25]  [ 8540/40201]  eta: 5:27:32  lr: 0.000003  min_lr: 0.000000  loss: 3.3835 (3.4234)  loss_scale: 32768.0000 (34110.8086)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0009  max mem: 6186
Epoch: [25]  [ 8550/40201]  eta: 5:27:15  lr: 0.000003  min_lr: 0.000000  loss: 3.3270 (3.4233)  loss_scale: 32768.0000 (34109.2382)  weight_decay: 0.0500 (0.0500)  time: 0.4570  data: 0.0010  max mem: 6186
Epoch: [25]  [ 8560/40201]  eta: 5:26:54  lr: 0.000003  min_lr: 0.000000  loss: 3.2507 (3.4232)  loss_scale: 32768.0000 (34107.6715)  weight_decay: 0.0500 (0.0500)  time: 0.2735  data: 0.0103  max mem: 6186
[2023-07-24 16:43:33,272] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:43:33,272] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:43:33,272] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:43:33,272] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 8570/40201]  eta: 5:26:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5066 (3.4236)  loss_scale: 32768.0000 (34113.7548)  weight_decay: 0.0500 (0.0500)  time: 0.2939  data: 0.0931  max mem: 6186
Epoch: [25]  [ 8580/40201]  eta: 5:26:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5066 (3.4235)  loss_scale: 65536.0000 (34150.3731)  weight_decay: 0.0500 (0.0500)  time: 0.4645  data: 0.2686  max mem: 6186
Epoch: [25]  [ 8590/40201]  eta: 5:26:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1465 (3.4232)  loss_scale: 65536.0000 (34186.9063)  weight_decay: 0.0500 (0.0500)  time: 0.4096  data: 0.2140  max mem: 6186
Epoch: [25]  [ 8600/40201]  eta: 5:25:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5832 (3.4233)  loss_scale: 65536.0000 (34223.3545)  weight_decay: 0.0500 (0.0500)  time: 0.2870  data: 0.0900  max mem: 6186
Epoch: [25]  [ 8610/40201]  eta: 5:25:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6889 (3.4238)  loss_scale: 65536.0000 (34259.7180)  weight_decay: 0.0500 (0.0500)  time: 0.3896  data: 0.1278  max mem: 6186
Epoch: [25]  [ 8620/40201]  eta: 5:25:47  lr: 0.000003  min_lr: 0.000000  loss: 3.6240 (3.4241)  loss_scale: 65536.0000 (34295.9972)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.2719  max mem: 6186
Epoch: [25]  [ 8630/40201]  eta: 5:25:37  lr: 0.000003  min_lr: 0.000000  loss: 3.3709 (3.4239)  loss_scale: 65536.0000 (34332.1923)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.2059  max mem: 6186
Epoch: [25]  [ 8640/40201]  eta: 5:25:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3024 (3.4237)  loss_scale: 65536.0000 (34368.3037)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8650/40201]  eta: 5:25:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3024 (3.4232)  loss_scale: 65536.0000 (34404.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8660/40201]  eta: 5:25:08  lr: 0.000003  min_lr: 0.000000  loss: 2.8914 (3.4228)  loss_scale: 65536.0000 (34440.2762)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0013  max mem: 6186
Epoch: [25]  [ 8670/40201]  eta: 5:24:58  lr: 0.000003  min_lr: 0.000000  loss: 3.1943 (3.4229)  loss_scale: 65536.0000 (34476.1379)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0012  max mem: 6186
Epoch: [25]  [ 8680/40201]  eta: 5:24:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4439 (3.4230)  loss_scale: 65536.0000 (34511.9171)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8690/40201]  eta: 5:24:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5952 (3.4232)  loss_scale: 65536.0000 (34547.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8700/40201]  eta: 5:24:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5952 (3.4235)  loss_scale: 65536.0000 (34583.2286)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8710/40201]  eta: 5:24:20  lr: 0.000003  min_lr: 0.000000  loss: 3.7193 (3.4240)  loss_scale: 65536.0000 (34618.7616)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0006  max mem: 6186
Epoch: [25]  [ 8720/40201]  eta: 5:24:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6757 (3.4242)  loss_scale: 65536.0000 (34654.2130)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0013  max mem: 6186
Epoch: [25]  [ 8730/40201]  eta: 5:24:01  lr: 0.000003  min_lr: 0.000000  loss: 3.1138 (3.4235)  loss_scale: 65536.0000 (34689.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0012  max mem: 6186
Epoch: [25]  [ 8740/40201]  eta: 5:23:51  lr: 0.000003  min_lr: 0.000000  loss: 3.1037 (3.4235)  loss_scale: 65536.0000 (34724.8727)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8750/40201]  eta: 5:23:40  lr: 0.000003  min_lr: 0.000000  loss: 3.1016 (3.4227)  loss_scale: 65536.0000 (34760.0814)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8760/40201]  eta: 5:23:30  lr: 0.000003  min_lr: 0.000000  loss: 2.9580 (3.4228)  loss_scale: 65536.0000 (34795.2097)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8770/40201]  eta: 5:23:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7279 (3.4234)  loss_scale: 65536.0000 (34830.2579)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8780/40201]  eta: 5:23:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6480 (3.4233)  loss_scale: 65536.0000 (34865.2263)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8790/40201]  eta: 5:23:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3360 (3.4231)  loss_scale: 65536.0000 (34900.1151)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8800/40201]  eta: 5:22:52  lr: 0.000003  min_lr: 0.000000  loss: 3.2774 (3.4229)  loss_scale: 65536.0000 (34934.9247)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0004  max mem: 6186
[2023-07-24 16:45:33,877] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4401
[2023-07-24 16:45:33,877] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:45:33,883] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4401
[2023-07-24 16:45:33,884] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:45:33,884] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 8810/40201]  eta: 5:22:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3350 (3.4231)  loss_scale: 65536.0000 (34939.9033)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8820/40201]  eta: 5:22:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5865 (3.4229)  loss_scale: 32768.0000 (34937.4411)  weight_decay: 0.0500 (0.0500)  time: 0.4872  data: 0.0011  max mem: 6186
Epoch: [25]  [ 8830/40201]  eta: 5:22:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1768 (3.4232)  loss_scale: 32768.0000 (34934.9845)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0011  max mem: 6186
Epoch: [25]  [ 8840/40201]  eta: 5:22:10  lr: 0.000003  min_lr: 0.000000  loss: 3.7700 (3.4237)  loss_scale: 32768.0000 (34932.5334)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8850/40201]  eta: 5:21:59  lr: 0.000003  min_lr: 0.000000  loss: 3.6000 (3.4236)  loss_scale: 32768.0000 (34930.0879)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8860/40201]  eta: 5:21:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2598 (3.4238)  loss_scale: 32768.0000 (34927.6479)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0004  max mem: 6186
Epoch: [25]  [ 8870/40201]  eta: 5:21:41  lr: 0.000003  min_lr: 0.000000  loss: 3.8308 (3.4242)  loss_scale: 32768.0000 (34925.2134)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8880/40201]  eta: 5:21:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1626 (3.4237)  loss_scale: 32768.0000 (34922.7844)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0005  max mem: 6186
Epoch: [25]  [ 8890/40201]  eta: 5:21:14  lr: 0.000003  min_lr: 0.000000  loss: 3.0595 (3.4234)  loss_scale: 32768.0000 (34920.3608)  weight_decay: 0.0500 (0.0500)  time: 0.4075  data: 0.0164  max mem: 6186
Epoch: [25]  [ 8900/40201]  eta: 5:20:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3109 (3.4233)  loss_scale: 32768.0000 (34917.9427)  weight_decay: 0.0500 (0.0500)  time: 0.2812  data: 0.0567  max mem: 6186
Epoch: [25]  [ 8910/40201]  eta: 5:20:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1047 (3.4233)  loss_scale: 32768.0000 (34915.5300)  weight_decay: 0.0500 (0.0500)  time: 0.2952  data: 0.0626  max mem: 6186
Epoch: [25]  [ 8920/40201]  eta: 5:20:28  lr: 0.000003  min_lr: 0.000000  loss: 3.0777 (3.4227)  loss_scale: 32768.0000 (34913.1227)  weight_decay: 0.0500 (0.0500)  time: 0.3927  data: 0.1603  max mem: 6186
Epoch: [25]  [ 8930/40201]  eta: 5:20:10  lr: 0.000003  min_lr: 0.000000  loss: 3.0777 (3.4229)  loss_scale: 32768.0000 (34910.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.1694  max mem: 6186
Epoch: [25]  [ 8940/40201]  eta: 5:19:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1272 (3.4227)  loss_scale: 32768.0000 (34908.3243)  weight_decay: 0.0500 (0.0500)  time: 0.3219  data: 0.1020  max mem: 6186
Epoch: [25]  [ 8950/40201]  eta: 5:19:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2556 (3.4226)  loss_scale: 32768.0000 (34905.9332)  weight_decay: 0.0500 (0.0500)  time: 0.4625  data: 0.1773  max mem: 6186
Epoch: [25]  [ 8960/40201]  eta: 5:19:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3691 (3.4227)  loss_scale: 32768.0000 (34903.5474)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.1073  max mem: 6186
Epoch: [25]  [ 8970/40201]  eta: 5:19:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6193 (3.4232)  loss_scale: 32768.0000 (34901.1669)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0535  max mem: 6186
Epoch: [25]  [ 8980/40201]  eta: 5:19:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4231)  loss_scale: 32768.0000 (34898.7917)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0530  max mem: 6186
Epoch: [25]  [ 8990/40201]  eta: 5:19:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2734 (3.4231)  loss_scale: 32768.0000 (34896.4218)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0006  max mem: 6186
[2023-07-24 16:47:05,565] [INFO] [timer.py:181:stop] 0/9000, SamplesPerSec=12.268565767570385
Epoch: [25]  [ 9000/40201]  eta: 5:19:04  lr: 0.000003  min_lr: 0.000000  loss: 2.9259 (3.4223)  loss_scale: 32768.0000 (34894.0571)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9010/40201]  eta: 5:18:54  lr: 0.000003  min_lr: 0.000000  loss: 2.7897 (3.4220)  loss_scale: 32768.0000 (34891.6977)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9020/40201]  eta: 5:18:44  lr: 0.000003  min_lr: 0.000000  loss: 3.2555 (3.4221)  loss_scale: 32768.0000 (34889.3435)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9030/40201]  eta: 5:18:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5026 (3.4223)  loss_scale: 32768.0000 (34886.9946)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0014  max mem: 6186
Epoch: [25]  [ 9040/40201]  eta: 5:18:25  lr: 0.000003  min_lr: 0.000000  loss: 3.6510 (3.4224)  loss_scale: 32768.0000 (34884.6508)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0014  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/OErKBwdGJIk_000057_000067.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 9050/40201]  eta: 5:18:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2077 (3.4221)  loss_scale: 32768.0000 (34882.3122)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9060/40201]  eta: 5:18:05  lr: 0.000003  min_lr: 0.000000  loss: 2.8372 (3.4215)  loss_scale: 32768.0000 (34879.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
[2023-07-24 16:47:36,886] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:47:36,886] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:47:36,888] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:47:36,888] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 9070/40201]  eta: 5:17:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1039 (3.4216)  loss_scale: 32768.0000 (34913.7744)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9080/40201]  eta: 5:17:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4338 (3.4216)  loss_scale: 65536.0000 (34947.4957)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9090/40201]  eta: 5:17:36  lr: 0.000003  min_lr: 0.000000  loss: 3.3162 (3.4213)  loss_scale: 65536.0000 (34981.1427)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0012  max mem: 6186
[2023-07-24 16:47:51,921] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4545
[2023-07-24 16:47:51,921] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:47:51,921] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:47:51,924] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4545
[2023-07-24 16:47:51,924] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 9100/40201]  eta: 5:17:25  lr: 0.000003  min_lr: 0.000000  loss: 3.3852 (3.4215)  loss_scale: 32768.0000 (34978.7109)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9110/40201]  eta: 5:17:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7765 (3.4223)  loss_scale: 32768.0000 (34976.2845)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9120/40201]  eta: 5:17:05  lr: 0.000003  min_lr: 0.000000  loss: 3.7430 (3.4220)  loss_scale: 32768.0000 (34973.8634)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9130/40201]  eta: 5:16:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1535 (3.4221)  loss_scale: 32768.0000 (34971.4476)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9140/40201]  eta: 5:16:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5035 (3.4223)  loss_scale: 32768.0000 (34969.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9150/40201]  eta: 5:16:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0063 (3.4219)  loss_scale: 32768.0000 (34966.6318)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9160/40201]  eta: 5:16:16  lr: 0.000003  min_lr: 0.000000  loss: 3.0063 (3.4215)  loss_scale: 32768.0000 (34964.2319)  weight_decay: 0.0500 (0.0500)  time: 0.3422  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9170/40201]  eta: 5:15:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3342 (3.4214)  loss_scale: 32768.0000 (34961.8371)  weight_decay: 0.0500 (0.0500)  time: 0.1839  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9180/40201]  eta: 5:15:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2082 (3.4211)  loss_scale: 32768.0000 (34959.4476)  weight_decay: 0.0500 (0.0500)  time: 0.2461  data: 0.0329  max mem: 6186
Epoch: [25]  [ 9190/40201]  eta: 5:15:21  lr: 0.000003  min_lr: 0.000000  loss: 3.2082 (3.4214)  loss_scale: 32768.0000 (34957.0632)  weight_decay: 0.0500 (0.0500)  time: 0.2821  data: 0.0395  max mem: 6186
Epoch: [25]  [ 9200/40201]  eta: 5:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2949 (3.4215)  loss_scale: 32768.0000 (34954.6841)  weight_decay: 0.0500 (0.0500)  time: 0.4366  data: 0.1722  max mem: 6186
Epoch: [25]  [ 9210/40201]  eta: 5:14:54  lr: 0.000003  min_lr: 0.000000  loss: 3.2949 (3.4216)  loss_scale: 32768.0000 (34952.3101)  weight_decay: 0.0500 (0.0500)  time: 0.3986  data: 0.1648  max mem: 6186
Epoch: [25]  [ 9220/40201]  eta: 5:14:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1811 (3.4213)  loss_scale: 32768.0000 (34949.9412)  weight_decay: 0.0500 (0.0500)  time: 0.2058  data: 0.0023  max mem: 6186
Epoch: [25]  [ 9230/40201]  eta: 5:14:28  lr: 0.000003  min_lr: 0.000000  loss: 3.1402 (3.4210)  loss_scale: 32768.0000 (34947.5775)  weight_decay: 0.0500 (0.0500)  time: 0.4015  data: 0.0023  max mem: 6186
Epoch: [25]  [ 9240/40201]  eta: 5:14:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0870 (3.4212)  loss_scale: 32768.0000 (34945.2189)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9250/40201]  eta: 5:14:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1597 (3.4212)  loss_scale: 32768.0000 (34942.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9260/40201]  eta: 5:13:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2917 (3.4208)  loss_scale: 32768.0000 (34940.5170)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9270/40201]  eta: 5:13:49  lr: 0.000003  min_lr: 0.000000  loss: 3.2116 (3.4208)  loss_scale: 32768.0000 (34938.1737)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9280/40201]  eta: 5:13:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5643 (3.4208)  loss_scale: 32768.0000 (34935.8354)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9290/40201]  eta: 5:13:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6439 (3.4215)  loss_scale: 32768.0000 (34933.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9300/40201]  eta: 5:13:20  lr: 0.000003  min_lr: 0.000000  loss: 3.8189 (3.4218)  loss_scale: 32768.0000 (34931.1739)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9310/40201]  eta: 5:13:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6820 (3.4219)  loss_scale: 32768.0000 (34928.8506)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9320/40201]  eta: 5:13:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3276 (3.4219)  loss_scale: 32768.0000 (34926.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9330/40201]  eta: 5:12:51  lr: 0.000003  min_lr: 0.000000  loss: 3.2317 (3.4216)  loss_scale: 32768.0000 (34924.2191)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9340/40201]  eta: 5:12:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3231 (3.4216)  loss_scale: 32768.0000 (34921.9107)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0004  max mem: 6186
[2023-07-24 16:49:46,555] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:49:46,556] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:49:46,562] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:49:46,562] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 9350/40201]  eta: 5:12:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2161 (3.4212)  loss_scale: 32768.0000 (34926.6158)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9360/40201]  eta: 5:12:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1982 (3.4212)  loss_scale: 65536.0000 (34959.3146)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9370/40201]  eta: 5:12:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5365 (3.4218)  loss_scale: 65536.0000 (34991.9437)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9380/40201]  eta: 5:12:03  lr: 0.000003  min_lr: 0.000000  loss: 3.5365 (3.4216)  loss_scale: 65536.0000 (35024.5031)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9390/40201]  eta: 5:11:54  lr: 0.000003  min_lr: 0.000000  loss: 3.0931 (3.4217)  loss_scale: 65536.0000 (35056.9933)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9400/40201]  eta: 5:11:44  lr: 0.000003  min_lr: 0.000000  loss: 2.9342 (3.4215)  loss_scale: 65536.0000 (35089.4143)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9410/40201]  eta: 5:11:35  lr: 0.000003  min_lr: 0.000000  loss: 2.9342 (3.4214)  loss_scale: 65536.0000 (35121.7664)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9420/40201]  eta: 5:11:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1202 (3.4215)  loss_scale: 65536.0000 (35154.0499)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9430/40201]  eta: 5:11:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3893 (3.4217)  loss_scale: 65536.0000 (35186.2649)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9440/40201]  eta: 5:11:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0129 (3.4214)  loss_scale: 65536.0000 (35218.4116)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0014  max mem: 6186
Epoch: [25]  [ 9450/40201]  eta: 5:10:57  lr: 0.000003  min_lr: 0.000000  loss: 2.9240 (3.4212)  loss_scale: 65536.0000 (35250.4903)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0014  max mem: 6186
Epoch: [25]  [ 9460/40201]  eta: 5:10:42  lr: 0.000003  min_lr: 0.000000  loss: 3.0144 (3.4208)  loss_scale: 65536.0000 (35282.5012)  weight_decay: 0.0500 (0.0500)  time: 0.4164  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9470/40201]  eta: 5:10:22  lr: 0.000003  min_lr: 0.000000  loss: 3.3825 (3.4210)  loss_scale: 65536.0000 (35314.4445)  weight_decay: 0.0500 (0.0500)  time: 0.2537  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9480/40201]  eta: 5:10:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5522 (3.4210)  loss_scale: 65536.0000 (35346.3204)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0031  max mem: 6186
Epoch: [25]  [ 9490/40201]  eta: 5:10:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2830 (3.4209)  loss_scale: 65536.0000 (35378.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0030  max mem: 6186
Epoch: [25]  [ 9500/40201]  eta: 5:09:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5984 (3.4211)  loss_scale: 65536.0000 (35409.8710)  weight_decay: 0.0500 (0.0500)  time: 0.2551  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9510/40201]  eta: 5:09:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4418 (3.4207)  loss_scale: 65536.0000 (35441.5460)  weight_decay: 0.0500 (0.0500)  time: 0.4378  data: 0.1750  max mem: 6186
Epoch: [25]  [ 9520/40201]  eta: 5:09:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4418 (3.4207)  loss_scale: 65536.0000 (35473.1545)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1750  max mem: 6186
Epoch: [25]  [ 9530/40201]  eta: 5:09:24  lr: 0.000003  min_lr: 0.000000  loss: 3.3550 (3.4208)  loss_scale: 65536.0000 (35504.6967)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9540/40201]  eta: 5:09:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2185 (3.4209)  loss_scale: 65536.0000 (35536.1727)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9550/40201]  eta: 5:09:05  lr: 0.000003  min_lr: 0.000000  loss: 3.0166 (3.4203)  loss_scale: 65536.0000 (35567.5829)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9560/40201]  eta: 5:08:56  lr: 0.000003  min_lr: 0.000000  loss: 3.0378 (3.4201)  loss_scale: 65536.0000 (35598.9273)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0004  max mem: 6186
[2023-07-24 16:51:29,379] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4782
[2023-07-24 16:51:29,379] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:51:29,383] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4782
[2023-07-24 16:51:29,383] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:51:29,383] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 9570/40201]  eta: 5:08:45  lr: 0.000003  min_lr: 0.000000  loss: 3.0891 (3.4197)  loss_scale: 65536.0000 (35609.6642)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9580/40201]  eta: 5:08:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1248 (3.4195)  loss_scale: 32768.0000 (35606.6983)  weight_decay: 0.0500 (0.0500)  time: 0.3167  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9590/40201]  eta: 5:08:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1908 (3.4195)  loss_scale: 32768.0000 (35603.7385)  weight_decay: 0.0500 (0.0500)  time: 0.1838  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9600/40201]  eta: 5:07:51  lr: 0.000003  min_lr: 0.000000  loss: 3.2779 (3.4196)  loss_scale: 32768.0000 (35600.7849)  weight_decay: 0.0500 (0.0500)  time: 0.2543  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9610/40201]  eta: 5:07:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2564 (3.4197)  loss_scale: 32768.0000 (35597.8375)  weight_decay: 0.0500 (0.0500)  time: 0.3900  data: 0.0034  max mem: 6186
Epoch: [25]  [ 9620/40201]  eta: 5:07:29  lr: 0.000003  min_lr: 0.000000  loss: 3.2564 (3.4195)  loss_scale: 32768.0000 (35594.8962)  weight_decay: 0.0500 (0.0500)  time: 0.4486  data: 0.0903  max mem: 6186
Epoch: [25]  [ 9630/40201]  eta: 5:07:12  lr: 0.000003  min_lr: 0.000000  loss: 3.2728 (3.4194)  loss_scale: 32768.0000 (35591.9610)  weight_decay: 0.0500 (0.0500)  time: 0.3425  data: 0.1211  max mem: 6186
Epoch: [25]  [ 9640/40201]  eta: 5:07:01  lr: 0.000003  min_lr: 0.000000  loss: 3.2437 (3.4191)  loss_scale: 32768.0000 (35589.0318)  weight_decay: 0.0500 (0.0500)  time: 0.3590  data: 0.0533  max mem: 6186
Epoch: [25]  [ 9650/40201]  eta: 5:06:52  lr: 0.000003  min_lr: 0.000000  loss: 3.2437 (3.4193)  loss_scale: 32768.0000 (35586.1088)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0195  max mem: 6186
Epoch: [25]  [ 9660/40201]  eta: 5:06:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6440 (3.4196)  loss_scale: 32768.0000 (35583.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9670/40201]  eta: 5:06:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6429 (3.4196)  loss_scale: 32768.0000 (35580.2808)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9680/40201]  eta: 5:06:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5124 (3.4198)  loss_scale: 32768.0000 (35577.3759)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9690/40201]  eta: 5:06:15  lr: 0.000003  min_lr: 0.000000  loss: 3.3176 (3.4198)  loss_scale: 32768.0000 (35574.4769)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9700/40201]  eta: 5:06:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1406 (3.4196)  loss_scale: 32768.0000 (35571.5840)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9710/40201]  eta: 5:05:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5272 (3.4201)  loss_scale: 32768.0000 (35568.6969)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9720/40201]  eta: 5:05:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4485 (3.4199)  loss_scale: 32768.0000 (35565.8159)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9730/40201]  eta: 5:05:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1010 (3.4199)  loss_scale: 32768.0000 (35562.9407)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9740/40201]  eta: 5:05:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5825 (3.4200)  loss_scale: 32768.0000 (35560.0715)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9750/40201]  eta: 5:05:20  lr: 0.000003  min_lr: 0.000000  loss: 3.6241 (3.4203)  loss_scale: 32768.0000 (35557.2081)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9760/40201]  eta: 5:05:11  lr: 0.000003  min_lr: 0.000000  loss: 3.1139 (3.4199)  loss_scale: 32768.0000 (35554.3506)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9770/40201]  eta: 5:05:02  lr: 0.000003  min_lr: 0.000000  loss: 3.3065 (3.4200)  loss_scale: 32768.0000 (35551.4989)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9780/40201]  eta: 5:04:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3652 (3.4195)  loss_scale: 32768.0000 (35548.6531)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9790/40201]  eta: 5:04:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3888 (3.4198)  loss_scale: 32768.0000 (35545.8131)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9800/40201]  eta: 5:04:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3181 (3.4196)  loss_scale: 32768.0000 (35542.9789)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9810/40201]  eta: 5:04:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1595 (3.4197)  loss_scale: 32768.0000 (35540.1504)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9820/40201]  eta: 5:04:16  lr: 0.000003  min_lr: 0.000000  loss: 3.4775 (3.4199)  loss_scale: 32768.0000 (35537.3278)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0004  max mem: 6186
[2023-07-24 16:53:26,521] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:53:26,521] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:53:26,527] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:53:26,527] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 9830/40201]  eta: 5:04:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4775 (3.4200)  loss_scale: 32768.0000 (35561.1759)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9840/40201]  eta: 5:03:58  lr: 0.000003  min_lr: 0.000000  loss: 3.2872 (3.4199)  loss_scale: 65536.0000 (35591.6350)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9850/40201]  eta: 5:03:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2872 (3.4202)  loss_scale: 65536.0000 (35622.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9860/40201]  eta: 5:03:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1168 (3.4199)  loss_scale: 65536.0000 (35652.3679)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0017  max mem: 6186
Epoch: [25]  [ 9870/40201]  eta: 5:03:23  lr: 0.000003  min_lr: 0.000000  loss: 3.1185 (3.4201)  loss_scale: 65536.0000 (35682.6421)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0009  max mem: 6186
Epoch: [25]  [ 9880/40201]  eta: 5:03:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2897 (3.4198)  loss_scale: 65536.0000 (35712.8550)  weight_decay: 0.0500 (0.0500)  time: 0.2121  data: 0.0004  max mem: 6186
Epoch: [25]  [ 9890/40201]  eta: 5:02:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2897 (3.4198)  loss_scale: 65536.0000 (35743.0068)  weight_decay: 0.0500 (0.0500)  time: 0.2128  data: 0.0020  max mem: 6186
Epoch: [25]  [ 9900/40201]  eta: 5:02:38  lr: 0.000003  min_lr: 0.000000  loss: 3.1752 (3.4196)  loss_scale: 65536.0000 (35773.0977)  weight_decay: 0.0500 (0.0500)  time: 0.3641  data: 0.0405  max mem: 6186
Epoch: [25]  [ 9910/40201]  eta: 5:02:23  lr: 0.000003  min_lr: 0.000000  loss: 2.9747 (3.4194)  loss_scale: 65536.0000 (35803.1278)  weight_decay: 0.0500 (0.0500)  time: 0.3907  data: 0.0390  max mem: 6186
Epoch: [25]  [ 9920/40201]  eta: 5:02:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2909 (3.4194)  loss_scale: 65536.0000 (35833.0975)  weight_decay: 0.0500 (0.0500)  time: 0.2989  data: 0.0587  max mem: 6186
Epoch: [25]  [ 9930/40201]  eta: 5:01:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4622 (3.4193)  loss_scale: 65536.0000 (35863.0067)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.1235  max mem: 6186
Epoch: [25]  [ 9940/40201]  eta: 5:01:45  lr: 0.000003  min_lr: 0.000000  loss: 3.5115 (3.4197)  loss_scale: 65536.0000 (35892.8558)  weight_decay: 0.0500 (0.0500)  time: 0.4145  data: 0.0653  max mem: 6186
Epoch: [25]  [ 9950/40201]  eta: 5:01:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6584 (3.4198)  loss_scale: 65536.0000 (35922.6450)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0012  max mem: 6186
Epoch: [25]  [ 9960/40201]  eta: 5:01:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3795 (3.4195)  loss_scale: 65536.0000 (35952.3743)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0013  max mem: 6186
[2023-07-24 16:54:25,402] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4984
[2023-07-24 16:54:25,402] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:54:25,402] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:54:25,402] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 4984
[2023-07-24 16:54:25,402] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 9970/40201]  eta: 5:01:17  lr: 0.000003  min_lr: 0.000000  loss: 3.1565 (3.4194)  loss_scale: 65536.0000 (35975.4713)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9980/40201]  eta: 5:01:09  lr: 0.000003  min_lr: 0.000000  loss: 3.0934 (3.4192)  loss_scale: 32768.0000 (35972.2577)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0005  max mem: 6186
Epoch: [25]  [ 9990/40201]  eta: 5:01:00  lr: 0.000003  min_lr: 0.000000  loss: 3.0742 (3.4191)  loss_scale: 32768.0000 (35969.0505)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0009  max mem: 6186
[2023-07-24 16:54:40,430] [INFO] [logging.py:69:log_dist] [Rank 0] step=5000, skipped=22, lr=[6.855805344002831e-08, 6.855805344002831e-08, 9.141073792003775e-08, 9.141073792003775e-08, 1.2188098389338367e-07, 1.2188098389338367e-07, 1.6250797852451154e-07, 1.6250797852451154e-07, 2.1667730469934872e-07, 2.1667730469934872e-07, 2.88903072932465e-07, 2.88903072932465e-07, 3.852040972432866e-07, 3.852040972432866e-07, 5.136054629910488e-07, 5.136054629910488e-07, 6.848072839880651e-07, 6.848072839880651e-07, 9.130763786507535e-07, 9.130763786507535e-07, 1.217435171534338e-06, 1.217435171534338e-06, 1.6232468953791173e-06, 1.6232468953791173e-06, 2.164329193838823e-06, 2.164329193838823e-06, 2.885772258451764e-06, 2.885772258451764e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 16:54:40,434] [INFO] [timer.py:181:stop] 0/10000, SamplesPerSec=12.484196205942936
Epoch: [25]  [10000/40201]  eta: 5:00:50  lr: 0.000003  min_lr: 0.000000  loss: 3.4315 (3.4194)  loss_scale: 32768.0000 (35965.8498)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0009  max mem: 6186
Epoch: [25]  [10010/40201]  eta: 5:00:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6926 (3.4195)  loss_scale: 32768.0000 (35962.6555)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0004  max mem: 6186
Epoch: [25]  [10020/40201]  eta: 5:00:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5887 (3.4196)  loss_scale: 32768.0000 (35959.4675)  weight_decay: 0.0500 (0.0500)  time: 0.4433  data: 0.0004  max mem: 6186
Epoch: [25]  [10030/40201]  eta: 5:00:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3596 (3.4195)  loss_scale: 32768.0000 (35956.2859)  weight_decay: 0.0500 (0.0500)  time: 0.2845  data: 0.0005  max mem: 6186
Epoch: [25]  [10040/40201]  eta: 4:59:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3596 (3.4200)  loss_scale: 32768.0000 (35953.1106)  weight_decay: 0.0500 (0.0500)  time: 0.2273  data: 0.0377  max mem: 6186
Epoch: [25]  [10050/40201]  eta: 4:59:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7440 (3.4201)  loss_scale: 32768.0000 (35949.9417)  weight_decay: 0.0500 (0.0500)  time: 0.2642  data: 0.0744  max mem: 6186
Epoch: [25]  [10060/40201]  eta: 4:59:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5159 (3.4203)  loss_scale: 32768.0000 (35946.7790)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0380  max mem: 6186
Epoch: [25]  [10070/40201]  eta: 4:59:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5788 (3.4206)  loss_scale: 32768.0000 (35943.6227)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0012  max mem: 6186
Epoch: [25]  [10080/40201]  eta: 4:59:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4730 (3.4204)  loss_scale: 32768.0000 (35940.4726)  weight_decay: 0.0500 (0.0500)  time: 0.4591  data: 0.0007  max mem: 6186
Epoch: [25]  [10090/40201]  eta: 4:59:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4730 (3.4205)  loss_scale: 32768.0000 (35937.3287)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0014  max mem: 6186
Epoch: [25]  [10100/40201]  eta: 4:59:00  lr: 0.000003  min_lr: 0.000000  loss: 3.7011 (3.4210)  loss_scale: 32768.0000 (35934.1911)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0012  max mem: 6186
Epoch: [25]  [10110/40201]  eta: 4:58:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6636 (3.4212)  loss_scale: 32768.0000 (35931.0596)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0004  max mem: 6186
Epoch: [25]  [10120/40201]  eta: 4:58:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6100 (3.4213)  loss_scale: 32768.0000 (35927.9344)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0005  max mem: 6186
Epoch: [25]  [10130/40201]  eta: 4:58:34  lr: 0.000003  min_lr: 0.000000  loss: 4.0101 (3.4218)  loss_scale: 32768.0000 (35924.8153)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0005  max mem: 6186
Epoch: [25]  [10140/40201]  eta: 4:58:25  lr: 0.000003  min_lr: 0.000000  loss: 3.3697 (3.4216)  loss_scale: 32768.0000 (35921.7024)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [10150/40201]  eta: 4:58:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1318 (3.4215)  loss_scale: 32768.0000 (35918.5956)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0004  max mem: 6186
Epoch: [25]  [10160/40201]  eta: 4:58:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3816 (3.4216)  loss_scale: 32768.0000 (35915.4949)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0004  max mem: 6186
Epoch: [25]  [10170/40201]  eta: 4:57:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3963 (3.4217)  loss_scale: 32768.0000 (35912.4004)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0017  max mem: 6186
Epoch: [25]  [10180/40201]  eta: 4:57:50  lr: 0.000003  min_lr: 0.000000  loss: 3.6365 (3.4219)  loss_scale: 32768.0000 (35909.3119)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0016  max mem: 6186
Epoch: [25]  [10190/40201]  eta: 4:57:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6365 (3.4221)  loss_scale: 32768.0000 (35906.2294)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0004  max mem: 6186
Epoch: [25]  [10200/40201]  eta: 4:57:32  lr: 0.000003  min_lr: 0.000000  loss: 3.4415 (3.4222)  loss_scale: 32768.0000 (35903.1530)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0004  max mem: 6186
Epoch: [25]  [10210/40201]  eta: 4:57:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2416 (3.4222)  loss_scale: 32768.0000 (35900.0827)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0004  max mem: 6186
Epoch: [25]  [10220/40201]  eta: 4:57:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2491 (3.4220)  loss_scale: 32768.0000 (35897.0183)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0005  max mem: 6186
[2023-07-24 16:56:27,216] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:56:27,217] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:56:27,219] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:56:27,219] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [10230/40201]  eta: 4:57:06  lr: 0.000003  min_lr: 0.000000  loss: 2.8262 (3.4215)  loss_scale: 32768.0000 (35906.7712)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0004  max mem: 6186
[2023-07-24 16:56:28,862] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5115
[2023-07-24 16:56:28,862] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5115
[2023-07-24 16:56:28,862] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:56:28,862] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:56:28,862] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [10240/40201]  eta: 4:56:56  lr: 0.000003  min_lr: 0.000000  loss: 3.0140 (3.4212)  loss_scale: 32768.0000 (35903.7063)  weight_decay: 0.0500 (0.0500)  time: 0.4788  data: 0.0005  max mem: 6186
Epoch: [25]  [10250/40201]  eta: 4:56:48  lr: 0.000003  min_lr: 0.000000  loss: 3.5683 (3.4220)  loss_scale: 32768.0000 (35900.6474)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0005  max mem: 6186
Epoch: [25]  [10260/40201]  eta: 4:56:39  lr: 0.000003  min_lr: 0.000000  loss: 3.6079 (3.4221)  loss_scale: 32768.0000 (35897.5944)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0004  max mem: 6186
Epoch: [25]  [10270/40201]  eta: 4:56:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5556 (3.4222)  loss_scale: 32768.0000 (35894.5474)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0004  max mem: 6186
Epoch: [25]  [10280/40201]  eta: 4:56:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4806 (3.4222)  loss_scale: 32768.0000 (35891.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0004  max mem: 6186
Epoch: [25]  [10290/40201]  eta: 4:56:14  lr: 0.000003  min_lr: 0.000000  loss: 3.1507 (3.4217)  loss_scale: 32768.0000 (35888.4711)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0013  max mem: 6186
Epoch: [25]  [10300/40201]  eta: 4:56:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4103 (3.4224)  loss_scale: 32768.0000 (35885.4418)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0012  max mem: 6186
Epoch: [25]  [10310/40201]  eta: 4:55:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5564 (3.4226)  loss_scale: 32768.0000 (35882.4184)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0004  max mem: 6186
Epoch: [25]  [10320/40201]  eta: 4:55:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4743 (3.4226)  loss_scale: 32768.0000 (35879.4008)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [10330/40201]  eta: 4:55:39  lr: 0.000003  min_lr: 0.000000  loss: 3.4162 (3.4225)  loss_scale: 32768.0000 (35876.3891)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [10340/40201]  eta: 4:55:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4201 (3.4227)  loss_scale: 32768.0000 (35873.3832)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0004  max mem: 6186
Epoch: [25]  [10350/40201]  eta: 4:55:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4145 (3.4226)  loss_scale: 32768.0000 (35870.3832)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0020  max mem: 6186
Epoch: [25]  [10360/40201]  eta: 4:55:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3891 (3.4230)  loss_scale: 32768.0000 (35867.3889)  weight_decay: 0.0500 (0.0500)  time: 0.4751  data: 0.0020  max mem: 6186
Epoch: [25]  [10370/40201]  eta: 4:54:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4371 (3.4229)  loss_scale: 32768.0000 (35864.4003)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0009  max mem: 6186
Epoch: [25]  [10380/40201]  eta: 4:54:38  lr: 0.000003  min_lr: 0.000000  loss: 3.1004 (3.4224)  loss_scale: 32768.0000 (35861.4176)  weight_decay: 0.0500 (0.0500)  time: 0.2257  data: 0.0016  max mem: 6186
Epoch: [25]  [10390/40201]  eta: 4:54:23  lr: 0.000003  min_lr: 0.000000  loss: 3.0074 (3.4222)  loss_scale: 32768.0000 (35858.4406)  weight_decay: 0.0500 (0.0500)  time: 0.2789  data: 0.0109  max mem: 6186
Epoch: [25]  [10400/40201]  eta: 4:54:14  lr: 0.000003  min_lr: 0.000000  loss: 3.5849 (3.4225)  loss_scale: 32768.0000 (35855.4693)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.1651  max mem: 6186
Epoch: [25]  [10410/40201]  eta: 4:53:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5849 (3.4225)  loss_scale: 32768.0000 (35852.5037)  weight_decay: 0.0500 (0.0500)  time: 0.3416  data: 0.1553  max mem: 6186
Epoch: [25]  [10420/40201]  eta: 4:53:42  lr: 0.000003  min_lr: 0.000000  loss: 3.7837 (3.4229)  loss_scale: 32768.0000 (35849.5438)  weight_decay: 0.0500 (0.0500)  time: 0.2354  data: 0.0418  max mem: 6186
Epoch: [25]  [10430/40201]  eta: 4:53:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6492 (3.4227)  loss_scale: 32768.0000 (35846.5896)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0418  max mem: 6186
Epoch: [25]  [10440/40201]  eta: 4:53:39  lr: 0.000003  min_lr: 0.000000  loss: 2.8107 (3.4221)  loss_scale: 32768.0000 (35843.6410)  weight_decay: 0.0500 (0.0500)  time: 0.7472  data: 0.0004  max mem: 6186
Epoch: [25]  [10450/40201]  eta: 4:53:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1102 (3.4219)  loss_scale: 32768.0000 (35840.6981)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0012  max mem: 6186
Epoch: [25]  [10460/40201]  eta: 4:53:22  lr: 0.000003  min_lr: 0.000000  loss: 3.3639 (3.4218)  loss_scale: 32768.0000 (35837.7608)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0012  max mem: 6186
Epoch: [25]  [10470/40201]  eta: 4:53:13  lr: 0.000003  min_lr: 0.000000  loss: 3.4920 (3.4223)  loss_scale: 32768.0000 (35834.8291)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0005  max mem: 6186
Epoch: [25]  [10480/40201]  eta: 4:53:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8033 (3.4223)  loss_scale: 32768.0000 (35831.9031)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0005  max mem: 6186
[2023-07-24 16:58:29,266] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:58:29,266] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 16:58:29,268] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 16:58:29,268] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [10490/40201]  eta: 4:52:56  lr: 0.000003  min_lr: 0.000000  loss: 3.2850 (3.4224)  loss_scale: 32768.0000 (35835.2294)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [10500/40201]  eta: 4:52:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2850 (3.4226)  loss_scale: 65536.0000 (35863.5132)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0004  max mem: 6186
Epoch: [25]  [10510/40201]  eta: 4:52:40  lr: 0.000003  min_lr: 0.000000  loss: 3.1457 (3.4224)  loss_scale: 65536.0000 (35891.7431)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0005  max mem: 6186
Epoch: [25]  [10520/40201]  eta: 4:52:31  lr: 0.000003  min_lr: 0.000000  loss: 2.9199 (3.4221)  loss_scale: 65536.0000 (35919.9194)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0005  max mem: 6186
Epoch: [25]  [10530/40201]  eta: 4:52:22  lr: 0.000003  min_lr: 0.000000  loss: 2.7969 (3.4218)  loss_scale: 65536.0000 (35948.0422)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0004  max mem: 6186
Epoch: [25]  [10540/40201]  eta: 4:52:14  lr: 0.000003  min_lr: 0.000000  loss: 2.8049 (3.4218)  loss_scale: 65536.0000 (35976.1116)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [10550/40201]  eta: 4:52:06  lr: 0.000003  min_lr: 0.000000  loss: 2.9797 (3.4217)  loss_scale: 65536.0000 (36004.1278)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0004  max mem: 6186
Epoch: [25]  [10560/40201]  eta: 4:51:57  lr: 0.000003  min_lr: 0.000000  loss: 2.9713 (3.4212)  loss_scale: 65536.0000 (36032.0909)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0004  max mem: 6186
Epoch: [25]  [10570/40201]  eta: 4:51:49  lr: 0.000003  min_lr: 0.000000  loss: 2.9546 (3.4211)  loss_scale: 65536.0000 (36060.0011)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0004  max mem: 6186
[2023-07-24 16:59:10,219] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5285
[2023-07-24 16:59:10,219] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 16:59:10,219] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 16:59:10,222] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5285
[2023-07-24 16:59:10,223] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [10580/40201]  eta: 4:51:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1156 (3.4209)  loss_scale: 32768.0000 (36056.8899)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0004  max mem: 6186
Epoch: [25]  [10590/40201]  eta: 4:51:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1852 (3.4207)  loss_scale: 32768.0000 (36053.7845)  weight_decay: 0.0500 (0.0500)  time: 0.4756  data: 0.0005  max mem: 6186
Epoch: [25]  [10600/40201]  eta: 4:51:22  lr: 0.000003  min_lr: 0.000000  loss: 2.8951 (3.4204)  loss_scale: 32768.0000 (36050.6850)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0004  max mem: 6186
Epoch: [25]  [10610/40201]  eta: 4:51:14  lr: 0.000003  min_lr: 0.000000  loss: 2.9388 (3.4202)  loss_scale: 32768.0000 (36047.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0004  max mem: 6186
Epoch: [25]  [10620/40201]  eta: 4:51:05  lr: 0.000003  min_lr: 0.000000  loss: 2.9388 (3.4200)  loss_scale: 32768.0000 (36044.5035)  weight_decay: 0.0500 (0.0500)  time: 0.4934  data: 0.0005  max mem: 6186
Epoch: [25]  [10630/40201]  eta: 4:50:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5148 (3.4200)  loss_scale: 32768.0000 (36041.4215)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0004  max mem: 6186
Epoch: [25]  [10640/40201]  eta: 4:50:49  lr: 0.000003  min_lr: 0.000000  loss: 3.6577 (3.4204)  loss_scale: 32768.0000 (36038.3453)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0011  max mem: 6186
Epoch: [25]  [10650/40201]  eta: 4:50:40  lr: 0.000003  min_lr: 0.000000  loss: 3.4503 (3.4204)  loss_scale: 32768.0000 (36035.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0011  max mem: 6186
Epoch: [25]  [10660/40201]  eta: 4:50:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1869 (3.4199)  loss_scale: 32768.0000 (36032.2101)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0004  max mem: 6186
Epoch: [25]  [10670/40201]  eta: 4:50:23  lr: 0.000003  min_lr: 0.000000  loss: 3.1411 (3.4201)  loss_scale: 32768.0000 (36029.1512)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0012  max mem: 6186
Epoch: [25]  [10680/40201]  eta: 4:50:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4775 (3.4205)  loss_scale: 32768.0000 (36026.0979)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0012  max mem: 6186
Epoch: [25]  [10690/40201]  eta: 4:50:06  lr: 0.000003  min_lr: 0.000000  loss: 3.6390 (3.4206)  loss_scale: 32768.0000 (36023.0504)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0005  max mem: 6186
Epoch: [25]  [10700/40201]  eta: 4:49:58  lr: 0.000003  min_lr: 0.000000  loss: 3.5808 (3.4206)  loss_scale: 32768.0000 (36020.0086)  weight_decay: 0.0500 (0.0500)  time: 0.4947  data: 0.0013  max mem: 6186
Epoch: [25]  [10710/40201]  eta: 4:49:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2187 (3.4205)  loss_scale: 32768.0000 (36016.9725)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0013  max mem: 6186
Epoch: [25]  [10720/40201]  eta: 4:49:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3554 (3.4208)  loss_scale: 32768.0000 (36013.9420)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0004  max mem: 6186
Epoch: [25]  [10730/40201]  eta: 4:49:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3554 (3.4204)  loss_scale: 32768.0000 (36010.9172)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0004  max mem: 6186
Epoch: [25]  [10740/40201]  eta: 4:49:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0215 (3.4205)  loss_scale: 32768.0000 (36007.8980)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0004  max mem: 6186
Epoch: [25]  [10750/40201]  eta: 4:49:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3517 (3.4204)  loss_scale: 32768.0000 (36004.8844)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0004  max mem: 6186
Epoch: [25]  [10760/40201]  eta: 4:49:08  lr: 0.000003  min_lr: 0.000000  loss: 3.0977 (3.4201)  loss_scale: 32768.0000 (36001.8764)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [10770/40201]  eta: 4:49:00  lr: 0.000003  min_lr: 0.000000  loss: 2.8170 (3.4199)  loss_scale: 32768.0000 (35998.8740)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0004  max mem: 6186
Epoch: [25]  [10780/40201]  eta: 4:48:52  lr: 0.000003  min_lr: 0.000000  loss: 3.1138 (3.4199)  loss_scale: 32768.0000 (35995.8772)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0012  max mem: 6186
Epoch: [25]  [10790/40201]  eta: 4:48:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3949 (3.4201)  loss_scale: 32768.0000 (35992.8859)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0020  max mem: 6186
Epoch: [25]  [10800/40201]  eta: 4:48:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2818 (3.4197)  loss_scale: 32768.0000 (35989.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0012  max mem: 6186
Epoch: [25]  [10810/40201]  eta: 4:48:27  lr: 0.000003  min_lr: 0.000000  loss: 3.0247 (3.4194)  loss_scale: 32768.0000 (35986.9200)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0004  max mem: 6186
Epoch: [25]  [10820/40201]  eta: 4:48:19  lr: 0.000003  min_lr: 0.000000  loss: 3.3571 (3.4196)  loss_scale: 32768.0000 (35983.9453)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0004  max mem: 6186
[2023-07-24 17:01:19,412] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:01:19,412] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:01:19,417] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:01:19,417] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [10830/40201]  eta: 4:48:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3571 (3.4196)  loss_scale: 32768.0000 (35987.0269)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0004  max mem: 6186
Epoch: [25]  [10840/40201]  eta: 4:48:02  lr: 0.000003  min_lr: 0.000000  loss: 3.3809 (3.4199)  loss_scale: 65536.0000 (36014.2836)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0004  max mem: 6186
Epoch: [25]  [10850/40201]  eta: 4:47:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6383 (3.4202)  loss_scale: 65536.0000 (36041.4900)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0013  max mem: 6186
Epoch: [25]  [10860/40201]  eta: 4:47:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4976 (3.4203)  loss_scale: 65536.0000 (36068.6463)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0013  max mem: 6186
[2023-07-24 17:01:38,168] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5433
[2023-07-24 17:01:38,168] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:01:38,168] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:01:38,170] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5433
[2023-07-24 17:01:38,171] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [10870/40201]  eta: 4:47:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2155 (3.4201)  loss_scale: 65536.0000 (36083.6957)  weight_decay: 0.0500 (0.0500)  time: 0.4826  data: 0.0004  max mem: 6186
Epoch: [25]  [10880/40201]  eta: 4:47:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3471 (3.4201)  loss_scale: 32768.0000 (36080.6485)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0004  max mem: 6186
Epoch: [25]  [10890/40201]  eta: 4:47:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5519 (3.4203)  loss_scale: 32768.0000 (36077.6068)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0004  max mem: 6186
Epoch: [25]  [10900/40201]  eta: 4:47:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4289 (3.4203)  loss_scale: 32768.0000 (36074.5708)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0005  max mem: 6186
Epoch: [25]  [10910/40201]  eta: 4:47:04  lr: 0.000003  min_lr: 0.000000  loss: 3.2034 (3.4204)  loss_scale: 32768.0000 (36071.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0005  max mem: 6186
Epoch: [25]  [10920/40201]  eta: 4:46:56  lr: 0.000003  min_lr: 0.000000  loss: 3.2034 (3.4204)  loss_scale: 32768.0000 (36068.5153)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0005  max mem: 6186
Epoch: [25]  [10930/40201]  eta: 4:46:48  lr: 0.000003  min_lr: 0.000000  loss: 3.0966 (3.4201)  loss_scale: 32768.0000 (36065.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0016  max mem: 6186
Epoch: [25]  [10940/40201]  eta: 4:46:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2655 (3.4202)  loss_scale: 32768.0000 (36062.4820)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0016  max mem: 6186
Epoch: [25]  [10950/40201]  eta: 4:46:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1234 (3.4199)  loss_scale: 32768.0000 (36059.4737)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0004  max mem: 6186
Epoch: [25]  [10960/40201]  eta: 4:46:23  lr: 0.000003  min_lr: 0.000000  loss: 3.5526 (3.4204)  loss_scale: 32768.0000 (36056.4708)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0019  max mem: 6186
Epoch: [25]  [10970/40201]  eta: 4:46:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6895 (3.4205)  loss_scale: 32768.0000 (36053.4733)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0028  max mem: 6186
Epoch: [25]  [10980/40201]  eta: 4:46:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5845 (3.4206)  loss_scale: 32768.0000 (36050.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0013  max mem: 6186
Epoch: [25]  [10990/40201]  eta: 4:45:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5684 (3.4206)  loss_scale: 32768.0000 (36047.4949)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0005  max mem: 6186
[2023-07-24 17:02:44,440] [INFO] [timer.py:181:stop] 0/11000, SamplesPerSec=12.57536636226153
Epoch: [25]  [11000/40201]  eta: 4:45:50  lr: 0.000003  min_lr: 0.000000  loss: 3.5529 (3.4206)  loss_scale: 32768.0000 (36044.5138)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0004  max mem: 6186
Epoch: [25]  [11010/40201]  eta: 4:45:43  lr: 0.000003  min_lr: 0.000000  loss: 3.2453 (3.4206)  loss_scale: 32768.0000 (36041.5381)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0004  max mem: 6186
Epoch: [25]  [11020/40201]  eta: 4:45:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3458 (3.4208)  loss_scale: 32768.0000 (36038.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0004  max mem: 6186
Epoch: [25]  [11030/40201]  eta: 4:45:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5861 (3.4212)  loss_scale: 32768.0000 (36035.6029)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0015  max mem: 6186
Epoch: [25]  [11040/40201]  eta: 4:45:18  lr: 0.000003  min_lr: 0.000000  loss: 3.5821 (3.4211)  loss_scale: 32768.0000 (36032.6434)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0028  max mem: 6186
Epoch: [25]  [11050/40201]  eta: 4:45:10  lr: 0.000003  min_lr: 0.000000  loss: 3.3249 (3.4212)  loss_scale: 32768.0000 (36029.6893)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0025  max mem: 6186
Epoch: [25]  [11060/40201]  eta: 4:45:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7393 (3.4214)  loss_scale: 32768.0000 (36026.7404)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0012  max mem: 6186
Epoch: [25]  [11070/40201]  eta: 4:44:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7904 (3.4218)  loss_scale: 32768.0000 (36023.7969)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0005  max mem: 6186
Epoch: [25]  [11080/40201]  eta: 4:44:40  lr: 0.000003  min_lr: 0.000000  loss: 3.6250 (3.4219)  loss_scale: 32768.0000 (36020.8588)  weight_decay: 0.0500 (0.0500)  time: 0.3763  data: 0.0005  max mem: 6186
Epoch: [25]  [11090/40201]  eta: 4:44:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4573 (3.4219)  loss_scale: 32768.0000 (36017.9259)  weight_decay: 0.0500 (0.0500)  time: 0.2185  data: 0.0005  max mem: 6186
Epoch: [25]  [11100/40201]  eta: 4:44:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4865 (3.4222)  loss_scale: 32768.0000 (36014.9983)  weight_decay: 0.0500 (0.0500)  time: 0.4131  data: 0.0004  max mem: 6186
Epoch: [25]  [11110/40201]  eta: 4:44:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7223 (3.4223)  loss_scale: 32768.0000 (36012.0760)  weight_decay: 0.0500 (0.0500)  time: 0.7773  data: 0.0004  max mem: 6186
Epoch: [25]  [11120/40201]  eta: 4:44:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2435 (3.4220)  loss_scale: 32768.0000 (36009.1589)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.0005  max mem: 6186
[2023-07-24 17:03:47,696] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:03:47,696] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:03:47,701] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:03:47,701] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [11130/40201]  eta: 4:44:05  lr: 0.000003  min_lr: 0.000000  loss: 3.1958 (3.4221)  loss_scale: 32768.0000 (36023.9102)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0004  max mem: 6186
Epoch: [25]  [11140/40201]  eta: 4:43:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3143 (3.4222)  loss_scale: 65536.0000 (36050.3998)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0004  max mem: 6186
Epoch: [25]  [11150/40201]  eta: 4:43:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5807 (3.4226)  loss_scale: 65536.0000 (36076.8419)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0013  max mem: 6186
Epoch: [25]  [11160/40201]  eta: 4:43:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2795 (3.4223)  loss_scale: 65536.0000 (36103.2366)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0012  max mem: 6186
Epoch: [25]  [11170/40201]  eta: 4:43:33  lr: 0.000003  min_lr: 0.000000  loss: 3.2795 (3.4225)  loss_scale: 65536.0000 (36129.5841)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0004  max mem: 6186
Epoch: [25]  [11180/40201]  eta: 4:43:25  lr: 0.000003  min_lr: 0.000000  loss: 3.7619 (3.4228)  loss_scale: 65536.0000 (36155.8844)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0004  max mem: 6186
Epoch: [25]  [11190/40201]  eta: 4:43:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5054 (3.4224)  loss_scale: 65536.0000 (36182.1378)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0004  max mem: 6186
Epoch: [25]  [11200/40201]  eta: 4:43:09  lr: 0.000003  min_lr: 0.000000  loss: 2.8882 (3.4222)  loss_scale: 65536.0000 (36208.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0013  max mem: 6186
Epoch: [25]  [11210/40201]  eta: 4:43:01  lr: 0.000003  min_lr: 0.000000  loss: 2.9487 (3.4222)  loss_scale: 65536.0000 (36234.5040)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0013  max mem: 6186
Epoch: [25]  [11220/40201]  eta: 4:42:53  lr: 0.000003  min_lr: 0.000000  loss: 2.9279 (3.4218)  loss_scale: 65536.0000 (36260.6171)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [11230/40201]  eta: 4:42:44  lr: 0.000003  min_lr: 0.000000  loss: 2.9279 (3.4214)  loss_scale: 65536.0000 (36286.6836)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0004  max mem: 6186
Epoch: [25]  [11240/40201]  eta: 4:42:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0168 (3.4211)  loss_scale: 65536.0000 (36312.7039)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0004  max mem: 6186
Epoch: [25]  [11250/40201]  eta: 4:42:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3062 (3.4211)  loss_scale: 65536.0000 (36338.6778)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
Epoch: [25]  [11260/40201]  eta: 4:42:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1164 (3.4209)  loss_scale: 65536.0000 (36364.6056)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0005  max mem: 6186
[2023-07-24 17:04:59,457] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5634
[2023-07-24 17:04:59,457] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:04:59,457] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:04:59,459] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5634
[2023-07-24 17:04:59,459] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [11270/40201]  eta: 4:42:11  lr: 0.000003  min_lr: 0.000000  loss: 3.1850 (3.4210)  loss_scale: 65536.0000 (36384.6729)  weight_decay: 0.0500 (0.0500)  time: 0.4753  data: 0.0005  max mem: 6186
Epoch: [25]  [11280/40201]  eta: 4:42:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3607 (3.4211)  loss_scale: 32768.0000 (36381.4669)  weight_decay: 0.0500 (0.0500)  time: 0.4737  data: 0.0005  max mem: 6186
Epoch: [25]  [11290/40201]  eta: 4:41:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4625 (3.4216)  loss_scale: 32768.0000 (36378.2666)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [11300/40201]  eta: 4:41:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2027 (3.4215)  loss_scale: 32768.0000 (36375.0719)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0004  max mem: 6186
Epoch: [25]  [11310/40201]  eta: 4:41:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0818 (3.4215)  loss_scale: 32768.0000 (36371.8829)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0005  max mem: 6186
Epoch: [25]  [11320/40201]  eta: 4:41:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1623 (3.4217)  loss_scale: 32768.0000 (36368.6996)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0005  max mem: 6186
Epoch: [25]  [11330/40201]  eta: 4:41:23  lr: 0.000003  min_lr: 0.000000  loss: 3.5878 (3.4219)  loss_scale: 32768.0000 (36365.5218)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0004  max mem: 6186
Epoch: [25]  [11340/40201]  eta: 4:41:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2774 (3.4217)  loss_scale: 32768.0000 (36362.3497)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0005  max mem: 6186
Epoch: [25]  [11350/40201]  eta: 4:41:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3198 (3.4221)  loss_scale: 32768.0000 (36359.1832)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0011  max mem: 6186
Epoch: [25]  [11360/40201]  eta: 4:40:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5045 (3.4221)  loss_scale: 32768.0000 (36356.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0011  max mem: 6186
Epoch: [25]  [11370/40201]  eta: 4:40:51  lr: 0.000003  min_lr: 0.000000  loss: 3.4574 (3.4223)  loss_scale: 32768.0000 (36352.8668)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0005  max mem: 6186
Epoch: [25]  [11380/40201]  eta: 4:40:43  lr: 0.000003  min_lr: 0.000000  loss: 3.3762 (3.4221)  loss_scale: 32768.0000 (36349.7169)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0005  max mem: 6186
Epoch: [25]  [11390/40201]  eta: 4:40:35  lr: 0.000003  min_lr: 0.000000  loss: 3.5290 (3.4224)  loss_scale: 32768.0000 (36346.5726)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0005  max mem: 6186
Epoch: [25]  [11400/40201]  eta: 4:40:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3775 (3.4224)  loss_scale: 32768.0000 (36343.4337)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0004  max mem: 6186
Epoch: [25]  [11410/40201]  eta: 4:40:19  lr: 0.000003  min_lr: 0.000000  loss: 3.2374 (3.4223)  loss_scale: 32768.0000 (36340.3004)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0005  max mem: 6186
Epoch: [25]  [11420/40201]  eta: 4:40:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3804 (3.4225)  loss_scale: 32768.0000 (36337.1726)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0005  max mem: 6186
Epoch: [25]  [11430/40201]  eta: 4:40:03  lr: 0.000003  min_lr: 0.000000  loss: 3.5757 (3.4229)  loss_scale: 32768.0000 (36334.0502)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0005  max mem: 6186
Epoch: [25]  [11440/40201]  eta: 4:39:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2655 (3.4227)  loss_scale: 32768.0000 (36330.9333)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0005  max mem: 6186
Epoch: [25]  [11450/40201]  eta: 4:39:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3598 (3.4228)  loss_scale: 32768.0000 (36327.8218)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0004  max mem: 6186
Epoch: [25]  [11460/40201]  eta: 4:39:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1358 (3.4224)  loss_scale: 32768.0000 (36324.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0012  max mem: 6186
Epoch: [25]  [11470/40201]  eta: 4:39:31  lr: 0.000003  min_lr: 0.000000  loss: 3.0850 (3.4223)  loss_scale: 32768.0000 (36321.6152)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0013  max mem: 6186
Epoch: [25]  [11480/40201]  eta: 4:39:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0936 (3.4222)  loss_scale: 32768.0000 (36318.5200)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0004  max mem: 6186
Epoch: [25]  [11490/40201]  eta: 4:39:16  lr: 0.000003  min_lr: 0.000000  loss: 3.4655 (3.4226)  loss_scale: 32768.0000 (36315.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0004  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/99ABSLQdgUc_000046_000056.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [11500/40201]  eta: 4:39:08  lr: 0.000003  min_lr: 0.000000  loss: 3.5030 (3.4224)  loss_scale: 32768.0000 (36312.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [11510/40201]  eta: 4:39:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4253 (3.4226)  loss_scale: 32768.0000 (36309.2666)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0004  max mem: 6186
Epoch: [25]  [11520/40201]  eta: 4:38:52  lr: 0.000003  min_lr: 0.000000  loss: 3.6387 (3.4226)  loss_scale: 32768.0000 (36306.1929)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0004  max mem: 6186
[2023-07-24 17:07:08,612] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:07:08,612] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:07:08,616] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:07:08,617] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [11530/40201]  eta: 4:38:44  lr: 0.000003  min_lr: 0.000000  loss: 2.7571 (3.4221)  loss_scale: 32768.0000 (36314.4914)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0004  max mem: 6186
Epoch: [25]  [11540/40201]  eta: 4:38:36  lr: 0.000003  min_lr: 0.000000  loss: 2.5961 (3.4221)  loss_scale: 65536.0000 (36339.8111)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0005  max mem: 6186
Epoch: [25]  [11550/40201]  eta: 4:38:28  lr: 0.000003  min_lr: 0.000000  loss: 3.8534 (3.4228)  loss_scale: 65536.0000 (36365.0870)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0004  max mem: 6186
Epoch: [25]  [11560/40201]  eta: 4:38:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4633 (3.4225)  loss_scale: 65536.0000 (36390.3192)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0011  max mem: 6186
Epoch: [25]  [11570/40201]  eta: 4:38:13  lr: 0.000003  min_lr: 0.000000  loss: 3.1337 (3.4225)  loss_scale: 65536.0000 (36415.5077)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0011  max mem: 6186
Epoch: [25]  [11580/40201]  eta: 4:38:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5254 (3.4227)  loss_scale: 65536.0000 (36440.6528)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0005  max mem: 6186
Epoch: [25]  [11590/40201]  eta: 4:37:57  lr: 0.000003  min_lr: 0.000000  loss: 3.1893 (3.4225)  loss_scale: 65536.0000 (36465.7545)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0005  max mem: 6186
Epoch: [25]  [11600/40201]  eta: 4:37:49  lr: 0.000003  min_lr: 0.000000  loss: 2.9865 (3.4220)  loss_scale: 65536.0000 (36490.8129)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0005  max mem: 6186
Epoch: [25]  [11610/40201]  eta: 4:37:41  lr: 0.000003  min_lr: 0.000000  loss: 2.9966 (3.4223)  loss_scale: 65536.0000 (36515.8281)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0009  max mem: 6186
Epoch: [25]  [11620/40201]  eta: 4:37:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3606 (3.4222)  loss_scale: 65536.0000 (36540.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0009  max mem: 6186
Epoch: [25]  [11630/40201]  eta: 4:37:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1134 (3.4219)  loss_scale: 65536.0000 (36565.7295)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0009  max mem: 6186
Epoch: [25]  [11640/40201]  eta: 4:37:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2528 (3.4221)  loss_scale: 65536.0000 (36590.6159)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0009  max mem: 6186
Epoch: [25]  [11650/40201]  eta: 4:37:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2504 (3.4218)  loss_scale: 65536.0000 (36615.4596)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0004  max mem: 6186
Epoch: [25]  [11660/40201]  eta: 4:37:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2504 (3.4223)  loss_scale: 65536.0000 (36640.2607)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0005  max mem: 6186
Epoch: [25]  [11670/40201]  eta: 4:36:54  lr: 0.000003  min_lr: 0.000000  loss: 3.9490 (3.4224)  loss_scale: 65536.0000 (36665.0193)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0004  max mem: 6186
Epoch: [25]  [11680/40201]  eta: 4:36:47  lr: 0.000003  min_lr: 0.000000  loss: 2.9599 (3.4224)  loss_scale: 65536.0000 (36689.7355)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0004  max mem: 6186
Epoch: [25]  [11690/40201]  eta: 4:36:39  lr: 0.000003  min_lr: 0.000000  loss: 3.1417 (3.4223)  loss_scale: 65536.0000 (36714.4094)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0004  max mem: 6186
Epoch: [25]  [11700/40201]  eta: 4:36:31  lr: 0.000003  min_lr: 0.000000  loss: 3.3004 (3.4225)  loss_scale: 65536.0000 (36739.0411)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [11710/40201]  eta: 4:36:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3419 (3.4227)  loss_scale: 65536.0000 (36763.6308)  weight_decay: 0.0500 (0.0500)  time: 0.4933  data: 0.0004  max mem: 6186
Epoch: [25]  [11720/40201]  eta: 4:36:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4195 (3.4227)  loss_scale: 65536.0000 (36788.1785)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0004  max mem: 6186
Epoch: [25]  [11730/40201]  eta: 4:36:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0488 (3.4223)  loss_scale: 65536.0000 (36812.6843)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0004  max mem: 6186
Epoch: [25]  [11740/40201]  eta: 4:35:59  lr: 0.000003  min_lr: 0.000000  loss: 3.0331 (3.4225)  loss_scale: 65536.0000 (36837.1485)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0005  max mem: 6186
Epoch: [25]  [11750/40201]  eta: 4:35:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5826 (3.4226)  loss_scale: 65536.0000 (36861.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0004  max mem: 6186
Epoch: [25]  [11760/40201]  eta: 4:35:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3910 (3.4225)  loss_scale: 65536.0000 (36885.9519)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0004  max mem: 6186
Epoch: [25]  [11770/40201]  eta: 4:35:37  lr: 0.000003  min_lr: 0.000000  loss: 3.1194 (3.4226)  loss_scale: 65536.0000 (36910.2914)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0004  max mem: 6186
Epoch: [25]  [11780/40201]  eta: 4:35:29  lr: 0.000003  min_lr: 0.000000  loss: 3.9379 (3.4230)  loss_scale: 65536.0000 (36934.5896)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0004  max mem: 6186
[2023-07-24 17:09:16,840] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:09:16,840] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 17:09:16,843] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:09:16,843] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [11790/40201]  eta: 4:35:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7314 (3.4232)  loss_scale: 65536.0000 (37003.3117)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0004  max mem: 6186
[2023-07-24 17:09:21,511] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5896
[2023-07-24 17:09:21,511] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 17:09:21,511] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5896
[2023-07-24 17:09:21,511] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 17:09:21,511] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [11800/40201]  eta: 4:35:12  lr: 0.000003  min_lr: 0.000000  loss: 3.3891 (3.4231)  loss_scale: 65536.0000 (37038.5967)  weight_decay: 0.0500 (0.0500)  time: 0.4760  data: 0.0004  max mem: 6186
[2023-07-24 17:09:26,346] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5904
[2023-07-24 17:09:26,346] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:09:26,346] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:09:26,346] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 5904
[2023-07-24 17:09:26,346] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [11810/40201]  eta: 4:34:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3631 (3.4232)  loss_scale: 65536.0000 (37057.1759)  weight_decay: 0.0500 (0.0500)  time: 0.3209  data: 0.0005  max mem: 6186
Epoch: [25]  [11820/40201]  eta: 4:34:50  lr: 0.000003  min_lr: 0.000000  loss: 3.3830 (3.4230)  loss_scale: 32768.0000 (37053.5474)  weight_decay: 0.0500 (0.0500)  time: 0.3752  data: 0.1945  max mem: 6186
Epoch: [25]  [11830/40201]  eta: 4:34:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0784 (3.4229)  loss_scale: 32768.0000 (37049.9251)  weight_decay: 0.0500 (0.0500)  time: 0.4046  data: 0.2260  max mem: 6186
Epoch: [25]  [11840/40201]  eta: 4:34:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5000 (3.4232)  loss_scale: 32768.0000 (37046.3089)  weight_decay: 0.0500 (0.0500)  time: 0.2324  data: 0.0518  max mem: 6186
Epoch: [25]  [11850/40201]  eta: 4:34:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6416 (3.4233)  loss_scale: 32768.0000 (37042.6988)  weight_decay: 0.0500 (0.0500)  time: 0.4048  data: 0.2116  max mem: 6186
Epoch: [25]  [11860/40201]  eta: 4:34:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1401 (3.4228)  loss_scale: 32768.0000 (37039.0948)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.1917  max mem: 6186
Epoch: [25]  [11870/40201]  eta: 4:33:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3029 (3.4229)  loss_scale: 32768.0000 (37035.4969)  weight_decay: 0.0500 (0.0500)  time: 0.4397  data: 0.0004  max mem: 6186
Epoch: [25]  [11880/40201]  eta: 4:33:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5394 (3.4233)  loss_scale: 32768.0000 (37031.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0004  max mem: 6186
Epoch: [25]  [11890/40201]  eta: 4:33:43  lr: 0.000003  min_lr: 0.000000  loss: 2.8398 (3.4227)  loss_scale: 32768.0000 (37028.3192)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0013  max mem: 6186
Epoch: [25]  [11900/40201]  eta: 4:33:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3678 (3.4231)  loss_scale: 32768.0000 (37024.7394)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0013  max mem: 6186
Epoch: [25]  [11910/40201]  eta: 4:33:28  lr: 0.000003  min_lr: 0.000000  loss: 3.6391 (3.4231)  loss_scale: 32768.0000 (37021.1656)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0004  max mem: 6186
Epoch: [25]  [11920/40201]  eta: 4:33:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4469 (3.4232)  loss_scale: 32768.0000 (37017.5979)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0004  max mem: 6186
Epoch: [25]  [11930/40201]  eta: 4:33:12  lr: 0.000003  min_lr: 0.000000  loss: 3.3250 (3.4230)  loss_scale: 32768.0000 (37014.0360)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [11940/40201]  eta: 4:33:04  lr: 0.000003  min_lr: 0.000000  loss: 3.1477 (3.4230)  loss_scale: 32768.0000 (37010.4802)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0004  max mem: 6186
Epoch: [25]  [11950/40201]  eta: 4:32:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4456 (3.4232)  loss_scale: 32768.0000 (37006.9303)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0004  max mem: 6186
Epoch: [25]  [11960/40201]  eta: 4:32:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4161 (3.4231)  loss_scale: 32768.0000 (37003.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0004  max mem: 6186
Epoch: [25]  [11970/40201]  eta: 4:32:41  lr: 0.000003  min_lr: 0.000000  loss: 3.0219 (3.4229)  loss_scale: 32768.0000 (36999.8483)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0005  max mem: 6186
Epoch: [25]  [11980/40201]  eta: 4:32:34  lr: 0.000003  min_lr: 0.000000  loss: 3.1979 (3.4230)  loss_scale: 32768.0000 (36996.3162)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0005  max mem: 6186
Epoch: [25]  [11990/40201]  eta: 4:32:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1979 (3.4229)  loss_scale: 32768.0000 (36992.7899)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0017  max mem: 6186
[2023-07-24 17:10:56,499] [INFO] [logging.py:69:log_dist] [Rank 0] step=6000, skipped=28, lr=[6.718151911779468e-08, 6.718151911779468e-08, 8.957535882372623e-08, 8.957535882372623e-08, 1.194338117649683e-07, 1.194338117649683e-07, 1.592450823532911e-07, 1.592450823532911e-07, 2.123267764710548e-07, 2.123267764710548e-07, 2.83102368628073e-07, 2.83102368628073e-07, 3.774698248374307e-07, 3.774698248374307e-07, 5.03293099783241e-07, 5.03293099783241e-07, 6.710574663776546e-07, 6.710574663776546e-07, 8.947432885035395e-07, 8.947432885035395e-07, 1.1929910513380526e-06, 1.1929910513380526e-06, 1.5906547351174035e-06, 1.5906547351174035e-06, 2.120872980156538e-06, 2.120872980156538e-06, 2.8278306402087174e-06, 2.8278306402087174e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 17:10:56,503] [INFO] [timer.py:181:stop] 0/12000, SamplesPerSec=12.642214147223912
Epoch: [25]  [12000/40201]  eta: 4:32:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1191 (3.4230)  loss_scale: 32768.0000 (36989.2696)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0017  max mem: 6186
Epoch: [25]  [12010/40201]  eta: 4:32:11  lr: 0.000003  min_lr: 0.000000  loss: 3.0539 (3.4229)  loss_scale: 32768.0000 (36985.7551)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0004  max mem: 6186
Epoch: [25]  [12020/40201]  eta: 4:32:03  lr: 0.000003  min_lr: 0.000000  loss: 3.1095 (3.4231)  loss_scale: 32768.0000 (36982.2464)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0005  max mem: 6186
Epoch: [25]  [12030/40201]  eta: 4:31:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5209 (3.4234)  loss_scale: 32768.0000 (36978.7436)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0005  max mem: 6186
Epoch: [25]  [12040/40201]  eta: 4:31:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5440 (3.4234)  loss_scale: 32768.0000 (36975.2466)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0005  max mem: 6186
Epoch: [25]  [12050/40201]  eta: 4:31:41  lr: 0.000003  min_lr: 0.000000  loss: 2.9773 (3.4232)  loss_scale: 32768.0000 (36971.7554)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0004  max mem: 6186
Epoch: [25]  [12060/40201]  eta: 4:31:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1841 (3.4235)  loss_scale: 32768.0000 (36968.2700)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0005  max mem: 6186
[2023-07-24 17:11:30,827] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:11:30,827] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:11:30,831] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:11:30,831] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [12070/40201]  eta: 4:31:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6526 (3.4238)  loss_scale: 32768.0000 (36975.6487)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0005  max mem: 6186
Epoch: [25]  [12080/40201]  eta: 4:31:18  lr: 0.000003  min_lr: 0.000000  loss: 3.7384 (3.4241)  loss_scale: 65536.0000 (36999.2895)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0005  max mem: 6186
Epoch: [25]  [12090/40201]  eta: 4:31:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6591 (3.4242)  loss_scale: 65536.0000 (37022.8911)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
Epoch: [25]  [12100/40201]  eta: 4:31:03  lr: 0.000003  min_lr: 0.000000  loss: 3.1591 (3.4238)  loss_scale: 65536.0000 (37046.4537)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0005  max mem: 6186
Epoch: [25]  [12110/40201]  eta: 4:30:56  lr: 0.000003  min_lr: 0.000000  loss: 2.9466 (3.4237)  loss_scale: 65536.0000 (37069.9774)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0011  max mem: 6186
Epoch: [25]  [12120/40201]  eta: 4:30:48  lr: 0.000003  min_lr: 0.000000  loss: 2.9466 (3.4233)  loss_scale: 65536.0000 (37093.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0027  max mem: 6186
Epoch: [25]  [12130/40201]  eta: 4:30:41  lr: 0.000003  min_lr: 0.000000  loss: 3.4034 (3.4237)  loss_scale: 65536.0000 (37116.9084)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0021  max mem: 6186
Epoch: [25]  [12140/40201]  eta: 4:30:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4402 (3.4236)  loss_scale: 65536.0000 (37140.3160)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0005  max mem: 6186
Epoch: [25]  [12150/40201]  eta: 4:30:26  lr: 0.000003  min_lr: 0.000000  loss: 3.0359 (3.4234)  loss_scale: 65536.0000 (37163.6850)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0005  max mem: 6186
[2023-07-24 17:12:15,165] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6077
[2023-07-24 17:12:15,165] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:12:15,178] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6077
[2023-07-24 17:12:15,178] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:12:15,178] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [12160/40201]  eta: 4:30:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0359 (3.4235)  loss_scale: 65536.0000 (37170.8484)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0005  max mem: 6186
Epoch: [25]  [12170/40201]  eta: 4:30:10  lr: 0.000003  min_lr: 0.000000  loss: 3.6655 (3.4237)  loss_scale: 32768.0000 (37167.2310)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0005  max mem: 6186
Epoch: [25]  [12180/40201]  eta: 4:30:03  lr: 0.000003  min_lr: 0.000000  loss: 3.5735 (3.4238)  loss_scale: 32768.0000 (37163.6194)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0005  max mem: 6186
Epoch: [25]  [12190/40201]  eta: 4:29:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3371 (3.4236)  loss_scale: 32768.0000 (37160.0138)  weight_decay: 0.0500 (0.0500)  time: 0.4326  data: 0.0005  max mem: 6186
Epoch: [25]  [12200/40201]  eta: 4:29:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2122 (3.4233)  loss_scale: 32768.0000 (37156.4141)  weight_decay: 0.0500 (0.0500)  time: 0.4576  data: 0.0005  max mem: 6186
Epoch: [25]  [12210/40201]  eta: 4:29:34  lr: 0.000003  min_lr: 0.000000  loss: 2.9011 (3.4230)  loss_scale: 32768.0000 (37152.8202)  weight_decay: 0.0500 (0.0500)  time: 0.4265  data: 0.0004  max mem: 6186
Epoch: [25]  [12220/40201]  eta: 4:29:22  lr: 0.000003  min_lr: 0.000000  loss: 2.7847 (3.4226)  loss_scale: 32768.0000 (37149.2323)  weight_decay: 0.0500 (0.0500)  time: 0.3243  data: 0.0097  max mem: 6186
Epoch: [25]  [12230/40201]  eta: 4:29:11  lr: 0.000003  min_lr: 0.000000  loss: 3.0528 (3.4228)  loss_scale: 32768.0000 (37145.6502)  weight_decay: 0.0500 (0.0500)  time: 0.3292  data: 0.0103  max mem: 6186
Epoch: [25]  [12240/40201]  eta: 4:29:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3568 (3.4227)  loss_scale: 32768.0000 (37142.0740)  weight_decay: 0.0500 (0.0500)  time: 0.4911  data: 0.0016  max mem: 6186
Epoch: [25]  [12250/40201]  eta: 4:28:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1092 (3.4225)  loss_scale: 32768.0000 (37138.5036)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0010  max mem: 6186
Epoch: [25]  [12260/40201]  eta: 4:28:51  lr: 0.000003  min_lr: 0.000000  loss: 3.2903 (3.4228)  loss_scale: 32768.0000 (37134.9391)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0011  max mem: 6186
Epoch: [25]  [12270/40201]  eta: 4:28:44  lr: 0.000003  min_lr: 0.000000  loss: 3.7455 (3.4230)  loss_scale: 32768.0000 (37131.3803)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0011  max mem: 6186
Epoch: [25]  [12280/40201]  eta: 4:28:36  lr: 0.000003  min_lr: 0.000000  loss: 3.3011 (3.4230)  loss_scale: 32768.0000 (37127.8274)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0005  max mem: 6186
Epoch: [25]  [12290/40201]  eta: 4:28:29  lr: 0.000003  min_lr: 0.000000  loss: 3.3874 (3.4232)  loss_scale: 32768.0000 (37124.2802)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0013  max mem: 6186
Epoch: [25]  [12300/40201]  eta: 4:28:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4646 (3.4230)  loss_scale: 32768.0000 (37120.7388)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0013  max mem: 6186
Epoch: [25]  [12310/40201]  eta: 4:28:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2133 (3.4231)  loss_scale: 32768.0000 (37117.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0004  max mem: 6186
Epoch: [25]  [12320/40201]  eta: 4:28:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5468 (3.4234)  loss_scale: 32768.0000 (37113.6732)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0004  max mem: 6186
Epoch: [25]  [12330/40201]  eta: 4:27:59  lr: 0.000003  min_lr: 0.000000  loss: 3.9069 (3.4238)  loss_scale: 32768.0000 (37110.1491)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0004  max mem: 6186
Epoch: [25]  [12340/40201]  eta: 4:27:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5347 (3.4237)  loss_scale: 32768.0000 (37106.6306)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [12350/40201]  eta: 4:27:44  lr: 0.000003  min_lr: 0.000000  loss: 3.5347 (3.4241)  loss_scale: 32768.0000 (37103.1178)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0004  max mem: 6186
Epoch: [25]  [12360/40201]  eta: 4:27:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2554 (3.4239)  loss_scale: 32768.0000 (37099.6107)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0004  max mem: 6186
Epoch: [25]  [12370/40201]  eta: 4:27:29  lr: 0.000003  min_lr: 0.000000  loss: 2.9101 (3.4235)  loss_scale: 32768.0000 (37096.1093)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0013  max mem: 6186
Epoch: [25]  [12380/40201]  eta: 4:27:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1708 (3.4237)  loss_scale: 32768.0000 (37092.6135)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0012  max mem: 6186
Epoch: [25]  [12390/40201]  eta: 4:27:15  lr: 0.000003  min_lr: 0.000000  loss: 3.5408 (3.4237)  loss_scale: 32768.0000 (37089.1234)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0004  max mem: 6186
Epoch: [25]  [12400/40201]  eta: 4:27:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3875 (3.4233)  loss_scale: 32768.0000 (37085.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0004  max mem: 6186
Epoch: [25]  [12410/40201]  eta: 4:26:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1838 (3.4234)  loss_scale: 32768.0000 (37082.1600)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0004  max mem: 6186
[2023-07-24 17:14:19,804] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:14:19,805] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:14:19,804] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:14:19,805] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [12420/40201]  eta: 4:26:53  lr: 0.000003  min_lr: 0.000000  loss: 3.1921 (3.4231)  loss_scale: 32768.0000 (37099.7916)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0011  max mem: 6186
Epoch: [25]  [12430/40201]  eta: 4:26:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2522 (3.4231)  loss_scale: 65536.0000 (37122.6669)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0011  max mem: 6186
Epoch: [25]  [12440/40201]  eta: 4:26:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5291 (3.4231)  loss_scale: 65536.0000 (37145.5053)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0012  max mem: 6186
Epoch: [25]  [12450/40201]  eta: 4:26:30  lr: 0.000003  min_lr: 0.000000  loss: 3.2711 (3.4230)  loss_scale: 65536.0000 (37168.3071)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0012  max mem: 6186
Epoch: [25]  [12460/40201]  eta: 4:26:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3809 (3.4227)  loss_scale: 65536.0000 (37191.0723)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0005  max mem: 6186
Epoch: [25]  [12470/40201]  eta: 4:26:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3809 (3.4228)  loss_scale: 65536.0000 (37213.8010)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0004  max mem: 6186
Epoch: [25]  [12480/40201]  eta: 4:26:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2867 (3.4230)  loss_scale: 65536.0000 (37236.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0012  max mem: 6186
Epoch: [25]  [12490/40201]  eta: 4:26:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6234 (3.4231)  loss_scale: 65536.0000 (37259.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0019  max mem: 6186
Epoch: [25]  [12500/40201]  eta: 4:25:54  lr: 0.000003  min_lr: 0.000000  loss: 3.5775 (3.4230)  loss_scale: 65536.0000 (37281.7688)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0012  max mem: 6186
[2023-07-24 17:15:05,042] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6251
[2023-07-24 17:15:05,043] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:15:05,043] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:15:05,053] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6251
[2023-07-24 17:15:05,053] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [12510/40201]  eta: 4:25:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4836 (3.4230)  loss_scale: 65536.0000 (37283.3992)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0005  max mem: 6186
Epoch: [25]  [12520/40201]  eta: 4:25:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4451 (3.4229)  loss_scale: 32768.0000 (37279.7930)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0011  max mem: 6186
Epoch: [25]  [12530/40201]  eta: 4:25:31  lr: 0.000003  min_lr: 0.000000  loss: 3.1818 (3.4231)  loss_scale: 32768.0000 (37276.1925)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0011  max mem: 6186
Epoch: [25]  [12540/40201]  eta: 4:25:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1363 (3.4227)  loss_scale: 32768.0000 (37272.5977)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0004  max mem: 6186
Epoch: [25]  [12550/40201]  eta: 4:25:16  lr: 0.000003  min_lr: 0.000000  loss: 2.8455 (3.4228)  loss_scale: 32768.0000 (37269.0087)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0004  max mem: 6186
Epoch: [25]  [12560/40201]  eta: 4:25:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1534 (3.4227)  loss_scale: 32768.0000 (37265.4254)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0005  max mem: 6186
Epoch: [25]  [12570/40201]  eta: 4:25:01  lr: 0.000003  min_lr: 0.000000  loss: 3.2282 (3.4226)  loss_scale: 32768.0000 (37261.8477)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0005  max mem: 6186
Epoch: [25]  [12580/40201]  eta: 4:24:54  lr: 0.000003  min_lr: 0.000000  loss: 3.2981 (3.4226)  loss_scale: 32768.0000 (37258.2758)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0005  max mem: 6186
Epoch: [25]  [12590/40201]  eta: 4:24:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2652 (3.4229)  loss_scale: 32768.0000 (37254.7096)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
Epoch: [25]  [12600/40201]  eta: 4:24:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3370 (3.4227)  loss_scale: 32768.0000 (37251.1490)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0004  max mem: 6186
Epoch: [25]  [12610/40201]  eta: 4:24:32  lr: 0.000003  min_lr: 0.000000  loss: 3.3370 (3.4224)  loss_scale: 32768.0000 (37247.5940)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0004  max mem: 6186
Epoch: [25]  [12620/40201]  eta: 4:24:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5396 (3.4227)  loss_scale: 32768.0000 (37244.0447)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0004  max mem: 6186
Epoch: [25]  [12630/40201]  eta: 4:24:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5396 (3.4227)  loss_scale: 32768.0000 (37240.5010)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [12640/40201]  eta: 4:24:09  lr: 0.000003  min_lr: 0.000000  loss: 3.3011 (3.4225)  loss_scale: 32768.0000 (37236.9629)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0004  max mem: 6186
Epoch: [25]  [12650/40201]  eta: 4:24:02  lr: 0.000003  min_lr: 0.000000  loss: 3.5928 (3.4229)  loss_scale: 32768.0000 (37233.4304)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0009  max mem: 6186
Epoch: [25]  [12660/40201]  eta: 4:23:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6608 (3.4229)  loss_scale: 32768.0000 (37229.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0009  max mem: 6186
Epoch: [25]  [12670/40201]  eta: 4:23:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4785 (3.4232)  loss_scale: 32768.0000 (37226.3821)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0005  max mem: 6186
Epoch: [25]  [12680/40201]  eta: 4:23:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5377 (3.4231)  loss_scale: 32768.0000 (37222.8663)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0005  max mem: 6186
Epoch: [25]  [12690/40201]  eta: 4:23:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3819 (3.4230)  loss_scale: 32768.0000 (37219.3561)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0005  max mem: 6186
Epoch: [25]  [12700/40201]  eta: 4:23:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6217 (3.4231)  loss_scale: 32768.0000 (37215.8514)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0005  max mem: 6186
Epoch: [25]  [12710/40201]  eta: 4:23:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6217 (3.4232)  loss_scale: 32768.0000 (37212.3521)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0012  max mem: 6186
Epoch: [25]  [12720/40201]  eta: 4:23:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4719 (3.4232)  loss_scale: 32768.0000 (37208.8584)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0011  max mem: 6186
Epoch: [25]  [12730/40201]  eta: 4:23:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3879 (3.4233)  loss_scale: 32768.0000 (37205.3702)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0005  max mem: 6186
Epoch: [25]  [12740/40201]  eta: 4:22:57  lr: 0.000003  min_lr: 0.000000  loss: 3.0968 (3.4231)  loss_scale: 32768.0000 (37201.8874)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0012  max mem: 6186
Epoch: [25]  [12750/40201]  eta: 4:22:49  lr: 0.000003  min_lr: 0.000000  loss: 3.1881 (3.4231)  loss_scale: 32768.0000 (37198.4102)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0020  max mem: 6186
Epoch: [25]  [12760/40201]  eta: 4:22:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3541 (3.4232)  loss_scale: 32768.0000 (37194.9383)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0013  max mem: 6186
[2023-07-24 17:17:14,639] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:17:14,640] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:17:14,649] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:17:14,649] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [12770/40201]  eta: 4:22:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3541 (3.4234)  loss_scale: 32768.0000 (37217.1301)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0004  max mem: 6186
Epoch: [25]  [12780/40201]  eta: 4:22:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2147 (3.4232)  loss_scale: 65536.0000 (37239.2871)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0004  max mem: 6186
Epoch: [25]  [12790/40201]  eta: 4:22:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2711 (3.4233)  loss_scale: 65536.0000 (37261.4094)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [12800/40201]  eta: 4:22:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6211 (3.4235)  loss_scale: 65536.0000 (37283.4972)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0005  max mem: 6186
Epoch: [25]  [12810/40201]  eta: 4:22:06  lr: 0.000003  min_lr: 0.000000  loss: 3.4362 (3.4236)  loss_scale: 65536.0000 (37305.5505)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0022  max mem: 6186
Epoch: [25]  [12820/40201]  eta: 4:21:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4456 (3.4237)  loss_scale: 65536.0000 (37327.5695)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0022  max mem: 6186
Epoch: [25]  [12830/40201]  eta: 4:21:51  lr: 0.000003  min_lr: 0.000000  loss: 3.8878 (3.4240)  loss_scale: 65536.0000 (37349.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
Epoch: [25]  [12840/40201]  eta: 4:21:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4394 (3.4239)  loss_scale: 65536.0000 (37371.5044)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0004  max mem: 6186
Epoch: [25]  [12850/40201]  eta: 4:21:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2106 (3.4241)  loss_scale: 65536.0000 (37393.4206)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0004  max mem: 6186
Epoch: [25]  [12860/40201]  eta: 4:21:29  lr: 0.000003  min_lr: 0.000000  loss: 3.0528 (3.4238)  loss_scale: 65536.0000 (37415.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0005  max mem: 6186
Epoch: [25]  [12870/40201]  eta: 4:21:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1669 (3.4237)  loss_scale: 65536.0000 (37437.1508)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0005  max mem: 6186
[2023-07-24 17:18:13,718] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6439
[2023-07-24 17:18:13,718] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:18:13,718] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:18:13,720] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6439
[2023-07-24 17:18:13,720] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [12880/40201]  eta: 4:21:14  lr: 0.000003  min_lr: 0.000000  loss: 3.3682 (3.4239)  loss_scale: 65536.0000 (37453.8772)  weight_decay: 0.0500 (0.0500)  time: 0.4878  data: 0.0012  max mem: 6186
Epoch: [25]  [12890/40201]  eta: 4:21:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4746 (3.4239)  loss_scale: 32768.0000 (37450.2422)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0012  max mem: 6186
Epoch: [25]  [12900/40201]  eta: 4:21:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5504 (3.4239)  loss_scale: 32768.0000 (37446.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0012  max mem: 6186
Epoch: [25]  [12910/40201]  eta: 4:20:53  lr: 0.000003  min_lr: 0.000000  loss: 3.6643 (3.4242)  loss_scale: 32768.0000 (37442.9891)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0020  max mem: 6186
Epoch: [25]  [12920/40201]  eta: 4:20:45  lr: 0.000003  min_lr: 0.000000  loss: 3.8477 (3.4243)  loss_scale: 32768.0000 (37439.3709)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0013  max mem: 6186
Epoch: [25]  [12930/40201]  eta: 4:20:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6314 (3.4244)  loss_scale: 32768.0000 (37435.7584)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0004  max mem: 6186
Epoch: [25]  [12940/40201]  eta: 4:20:31  lr: 0.000003  min_lr: 0.000000  loss: 3.0837 (3.4241)  loss_scale: 32768.0000 (37432.1515)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0004  max mem: 6186
Epoch: [25]  [12950/40201]  eta: 4:20:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1065 (3.4241)  loss_scale: 32768.0000 (37428.5501)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0005  max mem: 6186
Epoch: [25]  [12960/40201]  eta: 4:20:17  lr: 0.000003  min_lr: 0.000000  loss: 3.9716 (3.4246)  loss_scale: 32768.0000 (37424.9542)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [12970/40201]  eta: 4:20:10  lr: 0.000003  min_lr: 0.000000  loss: 3.4659 (3.4245)  loss_scale: 32768.0000 (37421.3640)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0004  max mem: 6186
Epoch: [25]  [12980/40201]  eta: 4:20:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4659 (3.4244)  loss_scale: 32768.0000 (37417.7792)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0004  max mem: 6186
Epoch: [25]  [12990/40201]  eta: 4:19:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5036 (3.4244)  loss_scale: 32768.0000 (37414.2000)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0005  max mem: 6186
[2023-07-24 17:19:14,332] [INFO] [timer.py:181:stop] 0/13000, SamplesPerSec=12.67200925732114
Epoch: [25]  [13000/40201]  eta: 4:19:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6042 (3.4250)  loss_scale: 32768.0000 (37410.6263)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0012  max mem: 6186
Epoch: [25]  [13010/40201]  eta: 4:19:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6735 (3.4250)  loss_scale: 32768.0000 (37407.0580)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0021  max mem: 6186
Epoch: [25]  [13020/40201]  eta: 4:19:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6735 (3.4251)  loss_scale: 32768.0000 (37403.4953)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0013  max mem: 6186
Epoch: [25]  [13030/40201]  eta: 4:19:26  lr: 0.000003  min_lr: 0.000000  loss: 3.3610 (3.4249)  loss_scale: 32768.0000 (37399.9380)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0004  max mem: 6186
Epoch: [25]  [13040/40201]  eta: 4:19:19  lr: 0.000003  min_lr: 0.000000  loss: 3.3610 (3.4250)  loss_scale: 32768.0000 (37396.3862)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0004  max mem: 6186
Epoch: [25]  [13050/40201]  eta: 4:19:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4251)  loss_scale: 32768.0000 (37392.8398)  weight_decay: 0.0500 (0.0500)  time: 0.3739  data: 0.0004  max mem: 6186
Epoch: [25]  [13060/40201]  eta: 4:18:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3743 (3.4250)  loss_scale: 32768.0000 (37389.2988)  weight_decay: 0.0500 (0.0500)  time: 0.2183  data: 0.0010  max mem: 6186
Epoch: [25]  [13070/40201]  eta: 4:18:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3743 (3.4251)  loss_scale: 32768.0000 (37385.7633)  weight_decay: 0.0500 (0.0500)  time: 0.2130  data: 0.0018  max mem: 6186
Epoch: [25]  [13080/40201]  eta: 4:18:32  lr: 0.000003  min_lr: 0.000000  loss: 3.5166 (3.4252)  loss_scale: 32768.0000 (37382.2332)  weight_decay: 0.0500 (0.0500)  time: 0.3415  data: 0.1263  max mem: 6186
Epoch: [25]  [13090/40201]  eta: 4:18:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6050 (3.4252)  loss_scale: 32768.0000 (37378.7084)  weight_decay: 0.0500 (0.0500)  time: 0.3376  data: 0.1537  max mem: 6186
Epoch: [25]  [13100/40201]  eta: 4:18:08  lr: 0.000003  min_lr: 0.000000  loss: 3.5485 (3.4252)  loss_scale: 32768.0000 (37375.1891)  weight_decay: 0.0500 (0.0500)  time: 0.2734  data: 0.0729  max mem: 6186
Epoch: [25]  [13110/40201]  eta: 4:17:56  lr: 0.000003  min_lr: 0.000000  loss: 3.2026 (3.4252)  loss_scale: 32768.0000 (37371.6751)  weight_decay: 0.0500 (0.0500)  time: 0.2887  data: 0.0497  max mem: 6186
Epoch: [25]  [13120/40201]  eta: 4:17:48  lr: 0.000003  min_lr: 0.000000  loss: 3.0995 (3.4248)  loss_scale: 32768.0000 (37368.1665)  weight_decay: 0.0500 (0.0500)  time: 0.3594  data: 0.0168  max mem: 6186
Epoch: [25]  [13130/40201]  eta: 4:17:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2740 (3.4252)  loss_scale: 32768.0000 (37364.6632)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1306  max mem: 6186
[2023-07-24 17:20:09,602] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:20:09,603] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:20:09,609] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:20:09,610] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [13140/40201]  eta: 4:17:39  lr: 0.000003  min_lr: 0.000000  loss: 3.6519 (3.4252)  loss_scale: 32768.0000 (37371.1395)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1192  max mem: 6186
Epoch: [25]  [13150/40201]  eta: 4:17:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6519 (3.4255)  loss_scale: 65536.0000 (37392.5560)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [13160/40201]  eta: 4:17:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4401 (3.4253)  loss_scale: 65536.0000 (37413.9400)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0005  max mem: 6186
[2023-07-24 17:20:23,545] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6582
[2023-07-24 17:20:23,545] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:20:23,545] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:20:23,547] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6582
[2023-07-24 17:20:23,547] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [13170/40201]  eta: 4:17:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3466 (3.4251)  loss_scale: 65536.0000 (37420.3641)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0005  max mem: 6186
Epoch: [25]  [13180/40201]  eta: 4:17:09  lr: 0.000003  min_lr: 0.000000  loss: 3.3561 (3.4250)  loss_scale: 32768.0000 (37416.8345)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0004  max mem: 6186
Epoch: [25]  [13190/40201]  eta: 4:17:02  lr: 0.000003  min_lr: 0.000000  loss: 3.0975 (3.4248)  loss_scale: 32768.0000 (37413.3103)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0004  max mem: 6186
Epoch: [25]  [13200/40201]  eta: 4:16:55  lr: 0.000003  min_lr: 0.000000  loss: 3.0975 (3.4247)  loss_scale: 32768.0000 (37409.7914)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0019  max mem: 6186
Epoch: [25]  [13210/40201]  eta: 4:16:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2920 (3.4245)  loss_scale: 32768.0000 (37406.2778)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0018  max mem: 6186
Epoch: [25]  [13220/40201]  eta: 4:16:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6017 (3.4247)  loss_scale: 32768.0000 (37402.7695)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0013  max mem: 6186
Epoch: [25]  [13230/40201]  eta: 4:16:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5209 (3.4247)  loss_scale: 32768.0000 (37399.2666)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0014  max mem: 6186
Epoch: [25]  [13240/40201]  eta: 4:16:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2720 (3.4246)  loss_scale: 32768.0000 (37395.7689)  weight_decay: 0.0500 (0.0500)  time: 0.4493  data: 0.0005  max mem: 6186
Epoch: [25]  [13250/40201]  eta: 4:16:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2720 (3.4246)  loss_scale: 32768.0000 (37392.2765)  weight_decay: 0.0500 (0.0500)  time: 0.2950  data: 0.0004  max mem: 6186
Epoch: [25]  [13260/40201]  eta: 4:15:58  lr: 0.000003  min_lr: 0.000000  loss: 2.8650 (3.4246)  loss_scale: 32768.0000 (37388.7894)  weight_decay: 0.0500 (0.0500)  time: 0.1896  data: 0.0081  max mem: 6186
Epoch: [25]  [13270/40201]  eta: 4:15:46  lr: 0.000003  min_lr: 0.000000  loss: 3.3247 (3.4247)  loss_scale: 32768.0000 (37385.3075)  weight_decay: 0.0500 (0.0500)  time: 0.2435  data: 0.0449  max mem: 6186
Epoch: [25]  [13280/40201]  eta: 4:15:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4444 (3.4249)  loss_scale: 32768.0000 (37381.8309)  weight_decay: 0.0500 (0.0500)  time: 0.2821  data: 0.0504  max mem: 6186
Epoch: [25]  [13290/40201]  eta: 4:15:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3009 (3.4246)  loss_scale: 32768.0000 (37378.3595)  weight_decay: 0.0500 (0.0500)  time: 0.2730  data: 0.0356  max mem: 6186
Epoch: [25]  [13300/40201]  eta: 4:15:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1532 (3.4246)  loss_scale: 32768.0000 (37374.8933)  weight_decay: 0.0500 (0.0500)  time: 0.2898  data: 0.0861  max mem: 6186
Epoch: [25]  [13310/40201]  eta: 4:15:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3535 (3.4243)  loss_scale: 32768.0000 (37371.4323)  weight_decay: 0.0500 (0.0500)  time: 0.2962  data: 0.0867  max mem: 6186
Epoch: [25]  [13320/40201]  eta: 4:14:52  lr: 0.000003  min_lr: 0.000000  loss: 3.0066 (3.4241)  loss_scale: 32768.0000 (37367.9766)  weight_decay: 0.0500 (0.0500)  time: 0.3633  data: 0.0583  max mem: 6186
Epoch: [25]  [13330/40201]  eta: 4:14:45  lr: 0.000003  min_lr: 0.000000  loss: 3.4284 (3.4245)  loss_scale: 32768.0000 (37364.5260)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0357  max mem: 6186
Epoch: [25]  [13340/40201]  eta: 4:14:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4466 (3.4242)  loss_scale: 32768.0000 (37361.0806)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0012  max mem: 6186
Epoch: [25]  [13350/40201]  eta: 4:14:31  lr: 0.000003  min_lr: 0.000000  loss: 3.2984 (3.4244)  loss_scale: 32768.0000 (37357.6403)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0011  max mem: 6186
Epoch: [25]  [13360/40201]  eta: 4:14:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5101 (3.4244)  loss_scale: 32768.0000 (37354.2052)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0004  max mem: 6186
Epoch: [25]  [13370/40201]  eta: 4:14:17  lr: 0.000003  min_lr: 0.000000  loss: 3.5101 (3.4247)  loss_scale: 32768.0000 (37350.7753)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0005  max mem: 6186
Epoch: [25]  [13380/40201]  eta: 4:14:10  lr: 0.000003  min_lr: 0.000000  loss: 3.8771 (3.4248)  loss_scale: 32768.0000 (37347.3504)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0004  max mem: 6186
Epoch: [25]  [13390/40201]  eta: 4:14:04  lr: 0.000003  min_lr: 0.000000  loss: 3.6346 (3.4250)  loss_scale: 32768.0000 (37343.9307)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0005  max mem: 6186
Epoch: [25]  [13400/40201]  eta: 4:13:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1514 (3.4246)  loss_scale: 32768.0000 (37340.5161)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0005  max mem: 6186
Epoch: [25]  [13410/40201]  eta: 4:13:49  lr: 0.000003  min_lr: 0.000000  loss: 3.0768 (3.4243)  loss_scale: 32768.0000 (37337.1066)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0004  max mem: 6186
Epoch: [25]  [13420/40201]  eta: 4:13:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3100 (3.4246)  loss_scale: 32768.0000 (37333.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0005  max mem: 6186
[2023-07-24 17:22:14,411] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:22:14,412] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:22:14,413] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:22:14,413] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [13430/40201]  eta: 4:13:35  lr: 0.000003  min_lr: 0.000000  loss: 4.0347 (3.4249)  loss_scale: 32768.0000 (37349.8206)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0005  max mem: 6186
[2023-07-24 17:22:18,170] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6715
[2023-07-24 17:22:18,170] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:22:18,177] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6715
[2023-07-24 17:22:18,177] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:22:18,177] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [13440/40201]  eta: 4:13:28  lr: 0.000003  min_lr: 0.000000  loss: 4.0347 (3.4253)  loss_scale: 32768.0000 (37346.4117)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0004  max mem: 6186
Epoch: [25]  [13450/40201]  eta: 4:13:20  lr: 0.000003  min_lr: 0.000000  loss: 3.8398 (3.4256)  loss_scale: 32768.0000 (37343.0080)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0005  max mem: 6186
Epoch: [25]  [13460/40201]  eta: 4:13:14  lr: 0.000003  min_lr: 0.000000  loss: 3.4922 (3.4255)  loss_scale: 32768.0000 (37339.6092)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0012  max mem: 6186
Epoch: [25]  [13470/40201]  eta: 4:13:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3962 (3.4257)  loss_scale: 32768.0000 (37336.2156)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0012  max mem: 6186
Epoch: [25]  [13480/40201]  eta: 4:12:59  lr: 0.000003  min_lr: 0.000000  loss: 3.0048 (3.4252)  loss_scale: 32768.0000 (37332.8269)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0005  max mem: 6186
Epoch: [25]  [13490/40201]  eta: 4:12:53  lr: 0.000003  min_lr: 0.000000  loss: 2.9703 (3.4250)  loss_scale: 32768.0000 (37329.4433)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0005  max mem: 6186
Epoch: [25]  [13500/40201]  eta: 4:12:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2054 (3.4251)  loss_scale: 32768.0000 (37326.0647)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0004  max mem: 6186
Epoch: [25]  [13510/40201]  eta: 4:12:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7590 (3.4255)  loss_scale: 32768.0000 (37322.6911)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0005  max mem: 6186
Epoch: [25]  [13520/40201]  eta: 4:12:32  lr: 0.000003  min_lr: 0.000000  loss: 3.5436 (3.4255)  loss_scale: 32768.0000 (37319.3225)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0005  max mem: 6186
Epoch: [25]  [13530/40201]  eta: 4:12:25  lr: 0.000003  min_lr: 0.000000  loss: 3.3910 (3.4256)  loss_scale: 32768.0000 (37315.9589)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0004  max mem: 6186
Epoch: [25]  [13540/40201]  eta: 4:12:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3150 (3.4255)  loss_scale: 32768.0000 (37312.6003)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0005  max mem: 6186
Epoch: [25]  [13550/40201]  eta: 4:12:11  lr: 0.000003  min_lr: 0.000000  loss: 2.9337 (3.4253)  loss_scale: 32768.0000 (37309.2466)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0005  max mem: 6186
Epoch: [25]  [13560/40201]  eta: 4:12:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5538 (3.4255)  loss_scale: 32768.0000 (37305.8978)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [13570/40201]  eta: 4:11:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5538 (3.4254)  loss_scale: 32768.0000 (37302.5540)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0005  max mem: 6186
Epoch: [25]  [13580/40201]  eta: 4:11:50  lr: 0.000003  min_lr: 0.000000  loss: 2.7738 (3.4251)  loss_scale: 32768.0000 (37299.2151)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0005  max mem: 6186
Epoch: [25]  [13590/40201]  eta: 4:11:44  lr: 0.000003  min_lr: 0.000000  loss: 2.7738 (3.4249)  loss_scale: 32768.0000 (37295.8811)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0005  max mem: 6186
Epoch: [25]  [13600/40201]  eta: 4:11:37  lr: 0.000003  min_lr: 0.000000  loss: 2.9083 (3.4245)  loss_scale: 32768.0000 (37292.5520)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0012  max mem: 6186
Epoch: [25]  [13610/40201]  eta: 4:11:29  lr: 0.000003  min_lr: 0.000000  loss: 3.0422 (3.4243)  loss_scale: 32768.0000 (37289.2278)  weight_decay: 0.0500 (0.0500)  time: 0.4947  data: 0.0013  max mem: 6186
Epoch: [25]  [13620/40201]  eta: 4:11:20  lr: 0.000003  min_lr: 0.000000  loss: 3.0422 (3.4241)  loss_scale: 32768.0000 (37285.9085)  weight_decay: 0.0500 (0.0500)  time: 0.4398  data: 0.0005  max mem: 6186
Epoch: [25]  [13630/40201]  eta: 4:11:07  lr: 0.000003  min_lr: 0.000000  loss: 3.1235 (3.4242)  loss_scale: 32768.0000 (37282.5941)  weight_decay: 0.0500 (0.0500)  time: 0.2831  data: 0.0004  max mem: 6186
Epoch: [25]  [13640/40201]  eta: 4:10:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2975 (3.4240)  loss_scale: 32768.0000 (37279.2845)  weight_decay: 0.0500 (0.0500)  time: 0.2188  data: 0.0323  max mem: 6186
Epoch: [25]  [13650/40201]  eta: 4:10:44  lr: 0.000003  min_lr: 0.000000  loss: 3.2366 (3.4239)  loss_scale: 32768.0000 (37275.9798)  weight_decay: 0.0500 (0.0500)  time: 0.2566  data: 0.0345  max mem: 6186
Epoch: [25]  [13660/40201]  eta: 4:10:33  lr: 0.000003  min_lr: 0.000000  loss: 3.2366 (3.4239)  loss_scale: 32768.0000 (37272.6799)  weight_decay: 0.0500 (0.0500)  time: 0.2794  data: 0.0403  max mem: 6186
Epoch: [25]  [13670/40201]  eta: 4:10:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3011 (3.4237)  loss_scale: 32768.0000 (37269.3848)  weight_decay: 0.0500 (0.0500)  time: 0.4354  data: 0.1037  max mem: 6186
Epoch: [25]  [13680/40201]  eta: 4:10:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4232 (3.4239)  loss_scale: 32768.0000 (37266.0946)  weight_decay: 0.0500 (0.0500)  time: 0.3980  data: 0.0918  max mem: 6186
[2023-07-24 17:24:14,345] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:24:14,345] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:24:14,349] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:24:14,350] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [13690/40201]  eta: 4:10:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4232 (3.4238)  loss_scale: 32768.0000 (37267.5959)  weight_decay: 0.0500 (0.0500)  time: 0.3525  data: 0.0262  max mem: 6186
Epoch: [25]  [13700/40201]  eta: 4:10:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3740 (3.4240)  loss_scale: 65536.0000 (37288.2283)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0093  max mem: 6186
Epoch: [25]  [13710/40201]  eta: 4:09:54  lr: 0.000003  min_lr: 0.000000  loss: 3.3702 (3.4239)  loss_scale: 65536.0000 (37308.8306)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0101  max mem: 6186
Epoch: [25]  [13720/40201]  eta: 4:09:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3702 (3.4239)  loss_scale: 65536.0000 (37329.4028)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0012  max mem: 6186
Epoch: [25]  [13730/40201]  eta: 4:09:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2771 (3.4238)  loss_scale: 65536.0000 (37349.9451)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0004  max mem: 6186
Epoch: [25]  [13740/40201]  eta: 4:09:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1372 (3.4238)  loss_scale: 65536.0000 (37370.4575)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0004  max mem: 6186
[2023-07-24 17:24:40,063] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6870
[2023-07-24 17:24:40,064] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:24:40,064] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:24:40,066] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 6870
[2023-07-24 17:24:40,066] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [13750/40201]  eta: 4:09:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0401 (3.4236)  loss_scale: 32768.0000 (37367.1105)  weight_decay: 0.0500 (0.0500)  time: 0.4730  data: 0.0004  max mem: 6186
Epoch: [25]  [13760/40201]  eta: 4:09:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1944 (3.4237)  loss_scale: 32768.0000 (37363.7683)  weight_decay: 0.0500 (0.0500)  time: 0.4767  data: 0.0004  max mem: 6186
Epoch: [25]  [13770/40201]  eta: 4:09:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4196 (3.4237)  loss_scale: 32768.0000 (37360.4311)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0004  max mem: 6186
Epoch: [25]  [13780/40201]  eta: 4:09:05  lr: 0.000003  min_lr: 0.000000  loss: 3.6582 (3.4240)  loss_scale: 32768.0000 (37357.0986)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0004  max mem: 6186
Epoch: [25]  [13790/40201]  eta: 4:08:58  lr: 0.000003  min_lr: 0.000000  loss: 3.5775 (3.4240)  loss_scale: 32768.0000 (37353.7710)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0004  max mem: 6186
Epoch: [25]  [13800/40201]  eta: 4:08:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6075 (3.4243)  loss_scale: 32768.0000 (37350.4482)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0004  max mem: 6186
Epoch: [25]  [13810/40201]  eta: 4:08:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3533 (3.4240)  loss_scale: 32768.0000 (37347.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [13820/40201]  eta: 4:08:37  lr: 0.000003  min_lr: 0.000000  loss: 3.0852 (3.4240)  loss_scale: 32768.0000 (37343.8171)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0004  max mem: 6186
Epoch: [25]  [13830/40201]  eta: 4:08:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6891 (3.4244)  loss_scale: 32768.0000 (37340.5087)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0005  max mem: 6186
Epoch: [25]  [13840/40201]  eta: 4:08:23  lr: 0.000003  min_lr: 0.000000  loss: 3.8812 (3.4244)  loss_scale: 32768.0000 (37337.2051)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0012  max mem: 6186
Epoch: [25]  [13850/40201]  eta: 4:08:16  lr: 0.000003  min_lr: 0.000000  loss: 3.4866 (3.4243)  loss_scale: 32768.0000 (37333.9063)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0012  max mem: 6186
Epoch: [25]  [13860/40201]  eta: 4:08:09  lr: 0.000003  min_lr: 0.000000  loss: 3.4866 (3.4243)  loss_scale: 32768.0000 (37330.6122)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0004  max mem: 6186
Epoch: [25]  [13870/40201]  eta: 4:08:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2859 (3.4241)  loss_scale: 32768.0000 (37327.3229)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0005  max mem: 6186
Epoch: [25]  [13880/40201]  eta: 4:07:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1429 (3.4240)  loss_scale: 32768.0000 (37324.0383)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0018  max mem: 6186
Epoch: [25]  [13890/40201]  eta: 4:07:49  lr: 0.000003  min_lr: 0.000000  loss: 3.2093 (3.4238)  loss_scale: 32768.0000 (37320.7585)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0018  max mem: 6186
Epoch: [25]  [13900/40201]  eta: 4:07:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4506 (3.4239)  loss_scale: 32768.0000 (37317.4833)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0004  max mem: 6186
Epoch: [25]  [13910/40201]  eta: 4:07:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4842 (3.4239)  loss_scale: 32768.0000 (37314.2129)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
Epoch: [25]  [13920/40201]  eta: 4:07:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4376 (3.4240)  loss_scale: 32768.0000 (37310.9472)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0013  max mem: 6186
Epoch: [25]  [13930/40201]  eta: 4:07:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2623 (3.4240)  loss_scale: 32768.0000 (37307.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0013  max mem: 6186
Epoch: [25]  [13940/40201]  eta: 4:07:15  lr: 0.000003  min_lr: 0.000000  loss: 3.0941 (3.4238)  loss_scale: 32768.0000 (37304.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [13950/40201]  eta: 4:07:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2126 (3.4237)  loss_scale: 32768.0000 (37301.1781)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0004  max mem: 6186
Epoch: [25]  [13960/40201]  eta: 4:07:01  lr: 0.000003  min_lr: 0.000000  loss: 3.1765 (3.4236)  loss_scale: 32768.0000 (37297.9311)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0004  max mem: 6186
Epoch: [25]  [13970/40201]  eta: 4:06:54  lr: 0.000003  min_lr: 0.000000  loss: 3.1232 (3.4234)  loss_scale: 32768.0000 (37294.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0004  max mem: 6186
Epoch: [25]  [13980/40201]  eta: 4:06:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1302 (3.4232)  loss_scale: 32768.0000 (37291.4510)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0004  max mem: 6186
Epoch: [25]  [13990/40201]  eta: 4:06:40  lr: 0.000003  min_lr: 0.000000  loss: 3.4523 (3.4234)  loss_scale: 32768.0000 (37288.2179)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0011  max mem: 6186
[2023-07-24 17:26:49,187] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:26:49,187] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:26:49,194] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:26:49,194] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:26:49,285] [INFO] [logging.py:69:log_dist] [Rank 0] step=7000, skipped=34, lr=[6.581829011146297e-08, 6.581829011146297e-08, 8.775772014861728e-08, 8.775772014861728e-08, 1.1701029353148971e-07, 1.1701029353148971e-07, 1.5601372470865295e-07, 1.5601372470865295e-07, 2.0801829961153728e-07, 2.0801829961153728e-07, 2.77357732815383e-07, 2.77357732815383e-07, 3.698103104205107e-07, 3.698103104205107e-07, 4.930804138940142e-07, 4.930804138940142e-07, 6.574405518586857e-07, 6.574405518586857e-07, 8.765874024782475e-07, 8.765874024782475e-07, 1.1687832033043301e-06, 1.1687832033043301e-06, 1.5583776044057735e-06, 1.5583776044057735e-06, 2.0778368058743644e-06, 2.0778368058743644e-06, 2.770449074499153e-06, 2.770449074499153e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 17:26:49,289] [INFO] [timer.py:181:stop] 0/14000, SamplesPerSec=12.804635302174042
Epoch: [25]  [14000/40201]  eta: 4:06:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3079 (3.4234)  loss_scale: 32768.0000 (37289.6702)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0011  max mem: 6186
Epoch: [25]  [14010/40201]  eta: 4:06:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2814 (3.4234)  loss_scale: 65536.0000 (37309.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0005  max mem: 6186
Epoch: [25]  [14020/40201]  eta: 4:06:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2115 (3.4231)  loss_scale: 65536.0000 (37329.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0005  max mem: 6186
[2023-07-24 17:27:03,192] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7013
[2023-07-24 17:27:03,192] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:27:03,192] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:27:03,193] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7013
[2023-07-24 17:27:03,193] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [14030/40201]  eta: 4:06:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2224 (3.4232)  loss_scale: 65536.0000 (37340.7227)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0005  max mem: 6186
Epoch: [25]  [14040/40201]  eta: 4:06:06  lr: 0.000003  min_lr: 0.000000  loss: 3.4802 (3.4233)  loss_scale: 32768.0000 (37337.4660)  weight_decay: 0.0500 (0.0500)  time: 0.4872  data: 0.0004  max mem: 6186
Epoch: [25]  [14050/40201]  eta: 4:05:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5372 (3.4235)  loss_scale: 32768.0000 (37334.2139)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0005  max mem: 6186
Epoch: [25]  [14060/40201]  eta: 4:05:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5372 (3.4235)  loss_scale: 32768.0000 (37330.9665)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0005  max mem: 6186
Epoch: [25]  [14070/40201]  eta: 4:05:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2340 (3.4234)  loss_scale: 32768.0000 (37327.7237)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0004  max mem: 6186
Epoch: [25]  [14080/40201]  eta: 4:05:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2340 (3.4234)  loss_scale: 32768.0000 (37324.4855)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0012  max mem: 6186
Epoch: [25]  [14090/40201]  eta: 4:05:31  lr: 0.000003  min_lr: 0.000000  loss: 3.5240 (3.4235)  loss_scale: 32768.0000 (37321.2519)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0021  max mem: 6186
Epoch: [25]  [14100/40201]  eta: 4:05:18  lr: 0.000003  min_lr: 0.000000  loss: 3.5240 (3.4234)  loss_scale: 32768.0000 (37318.0228)  weight_decay: 0.0500 (0.0500)  time: 0.3136  data: 0.0014  max mem: 6186
Epoch: [25]  [14110/40201]  eta: 4:05:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3888 (3.4234)  loss_scale: 32768.0000 (37314.7984)  weight_decay: 0.0500 (0.0500)  time: 0.1916  data: 0.0005  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/5_gyoV_sQXU_000001_000011.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [14120/40201]  eta: 4:04:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3888 (3.4232)  loss_scale: 32768.0000 (37311.5785)  weight_decay: 0.0500 (0.0500)  time: 0.3129  data: 0.0717  max mem: 6186
Epoch: [25]  [14130/40201]  eta: 4:04:46  lr: 0.000003  min_lr: 0.000000  loss: 3.3421 (3.4234)  loss_scale: 32768.0000 (37308.3632)  weight_decay: 0.0500 (0.0500)  time: 0.3218  data: 0.0782  max mem: 6186
Epoch: [25]  [14140/40201]  eta: 4:04:38  lr: 0.000003  min_lr: 0.000000  loss: 3.2472 (3.4234)  loss_scale: 32768.0000 (37305.1524)  weight_decay: 0.0500 (0.0500)  time: 0.3271  data: 0.1305  max mem: 6186
Epoch: [25]  [14150/40201]  eta: 4:04:27  lr: 0.000003  min_lr: 0.000000  loss: 3.0810 (3.4232)  loss_scale: 32768.0000 (37301.9462)  weight_decay: 0.0500 (0.0500)  time: 0.3548  data: 0.1728  max mem: 6186
Epoch: [25]  [14160/40201]  eta: 4:04:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5615 (3.4236)  loss_scale: 32768.0000 (37298.7444)  weight_decay: 0.0500 (0.0500)  time: 0.3971  data: 0.1230  max mem: 6186
Epoch: [25]  [14170/40201]  eta: 4:04:14  lr: 0.000003  min_lr: 0.000000  loss: 3.3504 (3.4233)  loss_scale: 32768.0000 (37295.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0751  max mem: 6186
Epoch: [25]  [14180/40201]  eta: 4:04:07  lr: 0.000003  min_lr: 0.000000  loss: 2.8396 (3.4229)  loss_scale: 32768.0000 (37292.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0013  max mem: 6186
Epoch: [25]  [14190/40201]  eta: 4:04:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1624 (3.4229)  loss_scale: 32768.0000 (37289.1664)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0004  max mem: 6186
Epoch: [25]  [14200/40201]  eta: 4:03:54  lr: 0.000003  min_lr: 0.000000  loss: 3.3858 (3.4230)  loss_scale: 32768.0000 (37285.9827)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0012  max mem: 6186
Epoch: [25]  [14210/40201]  eta: 4:03:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4478 (3.4231)  loss_scale: 32768.0000 (37282.8035)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0020  max mem: 6186
Epoch: [25]  [14220/40201]  eta: 4:03:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3771 (3.4230)  loss_scale: 32768.0000 (37279.6287)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0019  max mem: 6186
Epoch: [25]  [14230/40201]  eta: 4:03:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3453 (3.4231)  loss_scale: 32768.0000 (37276.4584)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0011  max mem: 6186
Epoch: [25]  [14240/40201]  eta: 4:03:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3770 (3.4233)  loss_scale: 32768.0000 (37273.2926)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0004  max mem: 6186
Epoch: [25]  [14250/40201]  eta: 4:03:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5109 (3.4234)  loss_scale: 32768.0000 (37270.1312)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0005  max mem: 6186
Epoch: [25]  [14260/40201]  eta: 4:03:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5109 (3.4233)  loss_scale: 32768.0000 (37266.9743)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0005  max mem: 6186
Epoch: [25]  [14270/40201]  eta: 4:03:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5497 (3.4236)  loss_scale: 32768.0000 (37263.8217)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0004  max mem: 6186
Epoch: [25]  [14280/40201]  eta: 4:02:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5505 (3.4236)  loss_scale: 32768.0000 (37260.6736)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0005  max mem: 6186
[2023-07-24 17:28:59,615] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:28:59,615] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:28:59,624] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:28:59,624] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:29:00,286] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7143
[2023-07-24 17:29:00,286] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:29:00,287] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:29:00,298] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7143
[2023-07-24 17:29:00,298] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [14290/40201]  eta: 4:02:52  lr: 0.000003  min_lr: 0.000000  loss: 3.5505 (3.4235)  loss_scale: 32768.0000 (37262.1157)  weight_decay: 0.0500 (0.0500)  time: 0.4755  data: 0.0005  max mem: 6186
Epoch: [25]  [14300/40201]  eta: 4:02:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6865 (3.4237)  loss_scale: 32768.0000 (37258.9732)  weight_decay: 0.0500 (0.0500)  time: 0.4752  data: 0.0005  max mem: 6186
Epoch: [25]  [14310/40201]  eta: 4:02:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4283 (3.4237)  loss_scale: 32768.0000 (37255.8351)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0004  max mem: 6186
Epoch: [25]  [14320/40201]  eta: 4:02:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2770 (3.4237)  loss_scale: 32768.0000 (37252.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0004  max mem: 6186
Epoch: [25]  [14330/40201]  eta: 4:02:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2287 (3.4238)  loss_scale: 32768.0000 (37249.5720)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0004  max mem: 6186
Epoch: [25]  [14340/40201]  eta: 4:02:18  lr: 0.000003  min_lr: 0.000000  loss: 3.2287 (3.4239)  loss_scale: 32768.0000 (37246.4470)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0004  max mem: 6186
Epoch: [25]  [14350/40201]  eta: 4:02:11  lr: 0.000003  min_lr: 0.000000  loss: 3.1002 (3.4238)  loss_scale: 32768.0000 (37243.3263)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0005  max mem: 6186
Epoch: [25]  [14360/40201]  eta: 4:02:05  lr: 0.000003  min_lr: 0.000000  loss: 3.1002 (3.4237)  loss_scale: 32768.0000 (37240.2100)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0004  max mem: 6186
Epoch: [25]  [14370/40201]  eta: 4:01:58  lr: 0.000003  min_lr: 0.000000  loss: 3.1738 (3.4238)  loss_scale: 32768.0000 (37237.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0004  max mem: 6186
Epoch: [25]  [14380/40201]  eta: 4:01:52  lr: 0.000003  min_lr: 0.000000  loss: 3.4498 (3.4238)  loss_scale: 32768.0000 (37233.9904)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0004  max mem: 6186
Epoch: [25]  [14390/40201]  eta: 4:01:46  lr: 0.000003  min_lr: 0.000000  loss: 3.1438 (3.4236)  loss_scale: 32768.0000 (37230.8871)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0004  max mem: 6186
Epoch: [25]  [14400/40201]  eta: 4:01:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3155 (3.4241)  loss_scale: 32768.0000 (37227.7881)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0005  max mem: 6186
Epoch: [25]  [14410/40201]  eta: 4:01:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7701 (3.4242)  loss_scale: 32768.0000 (37224.6934)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0009  max mem: 6186
Epoch: [25]  [14420/40201]  eta: 4:01:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2672 (3.4241)  loss_scale: 32768.0000 (37221.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0009  max mem: 6186
Epoch: [25]  [14430/40201]  eta: 4:01:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2672 (3.4243)  loss_scale: 32768.0000 (37218.5168)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [14440/40201]  eta: 4:01:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2797 (3.4243)  loss_scale: 32768.0000 (37215.4349)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0005  max mem: 6186
Epoch: [25]  [14450/40201]  eta: 4:01:06  lr: 0.000003  min_lr: 0.000000  loss: 3.2325 (3.4241)  loss_scale: 32768.0000 (37212.3573)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0005  max mem: 6186
Epoch: [25]  [14460/40201]  eta: 4:01:00  lr: 0.000003  min_lr: 0.000000  loss: 3.2325 (3.4240)  loss_scale: 32768.0000 (37209.2840)  weight_decay: 0.0500 (0.0500)  time: 0.5003  data: 0.0012  max mem: 6186
Epoch: [25]  [14470/40201]  eta: 4:00:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2519 (3.4242)  loss_scale: 32768.0000 (37206.2149)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0012  max mem: 6186
Epoch: [25]  [14480/40201]  eta: 4:00:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1929 (3.4240)  loss_scale: 32768.0000 (37203.1501)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0005  max mem: 6186
Epoch: [25]  [14490/40201]  eta: 4:00:40  lr: 0.000003  min_lr: 0.000000  loss: 3.0674 (3.4241)  loss_scale: 32768.0000 (37200.0894)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0013  max mem: 6186
Epoch: [25]  [14500/40201]  eta: 4:00:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.4244)  loss_scale: 32768.0000 (37197.0330)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0013  max mem: 6186
Epoch: [25]  [14510/40201]  eta: 4:00:27  lr: 0.000003  min_lr: 0.000000  loss: 3.6903 (3.4245)  loss_scale: 32768.0000 (37193.9808)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0005  max mem: 6186
Epoch: [25]  [14520/40201]  eta: 4:00:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3278 (3.4244)  loss_scale: 32768.0000 (37190.9329)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0011  max mem: 6186
Epoch: [25]  [14530/40201]  eta: 4:00:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5914 (3.4245)  loss_scale: 32768.0000 (37187.8891)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0011  max mem: 6186
Epoch: [25]  [14540/40201]  eta: 4:00:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6874 (3.4245)  loss_scale: 32768.0000 (37184.8495)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0004  max mem: 6186
[2023-07-24 17:31:10,962] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:31:10,962] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:31:10,969] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:31:10,969] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [14550/40201]  eta: 4:00:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4596 (3.4244)  loss_scale: 32768.0000 (37195.3257)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0067  max mem: 6186
Epoch: [25]  [14560/40201]  eta: 3:59:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3773 (3.4244)  loss_scale: 65536.0000 (37214.7891)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0067  max mem: 6186
Epoch: [25]  [14570/40201]  eta: 3:59:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3773 (3.4242)  loss_scale: 65536.0000 (37234.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0012  max mem: 6186
Epoch: [25]  [14580/40201]  eta: 3:59:40  lr: 0.000003  min_lr: 0.000000  loss: 3.6927 (3.4245)  loss_scale: 65536.0000 (37253.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0019  max mem: 6186
Epoch: [25]  [14590/40201]  eta: 3:59:34  lr: 0.000003  min_lr: 0.000000  loss: 3.8092 (3.4247)  loss_scale: 65536.0000 (37273.0193)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0012  max mem: 6186
Epoch: [25]  [14600/40201]  eta: 3:59:27  lr: 0.000003  min_lr: 0.000000  loss: 3.9039 (3.4250)  loss_scale: 65536.0000 (37292.3761)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0005  max mem: 6186
Epoch: [25]  [14610/40201]  eta: 3:59:21  lr: 0.000003  min_lr: 0.000000  loss: 3.9039 (3.4254)  loss_scale: 65536.0000 (37311.7065)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0005  max mem: 6186
Epoch: [25]  [14620/40201]  eta: 3:59:14  lr: 0.000003  min_lr: 0.000000  loss: 3.8345 (3.4255)  loss_scale: 65536.0000 (37331.0105)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0004  max mem: 6186
[2023-07-24 17:31:49,174] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7310
[2023-07-24 17:31:49,174] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:31:49,175] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:31:49,175] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7310
[2023-07-24 17:31:49,175] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [14630/40201]  eta: 3:59:07  lr: 0.000003  min_lr: 0.000000  loss: 3.1371 (3.4253)  loss_scale: 32768.0000 (37327.8917)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0005  max mem: 6186
Epoch: [25]  [14640/40201]  eta: 3:59:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5633 (3.4254)  loss_scale: 32768.0000 (37324.7773)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0004  max mem: 6186
Epoch: [25]  [14650/40201]  eta: 3:58:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5432 (3.4255)  loss_scale: 32768.0000 (37321.6671)  weight_decay: 0.0500 (0.0500)  time: 0.4334  data: 0.0005  max mem: 6186
Epoch: [25]  [14660/40201]  eta: 3:58:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0418 (3.4252)  loss_scale: 32768.0000 (37318.5611)  weight_decay: 0.0500 (0.0500)  time: 0.2705  data: 0.0005  max mem: 6186
Epoch: [25]  [14670/40201]  eta: 3:58:28  lr: 0.000003  min_lr: 0.000000  loss: 3.0418 (3.4250)  loss_scale: 32768.0000 (37315.4593)  weight_decay: 0.0500 (0.0500)  time: 0.2202  data: 0.0334  max mem: 6186
Epoch: [25]  [14680/40201]  eta: 3:58:21  lr: 0.000003  min_lr: 0.000000  loss: 3.0945 (3.4250)  loss_scale: 32768.0000 (37312.3618)  weight_decay: 0.0500 (0.0500)  time: 0.3687  data: 0.0376  max mem: 6186
Epoch: [25]  [14690/40201]  eta: 3:58:10  lr: 0.000003  min_lr: 0.000000  loss: 3.0967 (3.4249)  loss_scale: 32768.0000 (37309.2685)  weight_decay: 0.0500 (0.0500)  time: 0.3530  data: 0.0047  max mem: 6186
Epoch: [25]  [14700/40201]  eta: 3:57:59  lr: 0.000003  min_lr: 0.000000  loss: 3.0967 (3.4246)  loss_scale: 32768.0000 (37306.1794)  weight_decay: 0.0500 (0.0500)  time: 0.2408  data: 0.0004  max mem: 6186
Epoch: [25]  [14710/40201]  eta: 3:57:48  lr: 0.000003  min_lr: 0.000000  loss: 3.2881 (3.4245)  loss_scale: 32768.0000 (37303.0946)  weight_decay: 0.0500 (0.0500)  time: 0.2524  data: 0.0005  max mem: 6186
Epoch: [25]  [14720/40201]  eta: 3:57:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2881 (3.4246)  loss_scale: 32768.0000 (37300.0139)  weight_decay: 0.0500 (0.0500)  time: 0.3442  data: 0.0039  max mem: 6186
Epoch: [25]  [14730/40201]  eta: 3:57:33  lr: 0.000003  min_lr: 0.000000  loss: 3.0941 (3.4245)  loss_scale: 32768.0000 (37296.9373)  weight_decay: 0.0500 (0.0500)  time: 0.4715  data: 0.0038  max mem: 6186
Epoch: [25]  [14740/40201]  eta: 3:57:27  lr: 0.000003  min_lr: 0.000000  loss: 3.5765 (3.4248)  loss_scale: 32768.0000 (37293.8650)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0012  max mem: 6186
Epoch: [25]  [14750/40201]  eta: 3:57:20  lr: 0.000003  min_lr: 0.000000  loss: 3.3293 (3.4245)  loss_scale: 32768.0000 (37290.7968)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0013  max mem: 6186
Epoch: [25]  [14760/40201]  eta: 3:57:13  lr: 0.000003  min_lr: 0.000000  loss: 2.9433 (3.4243)  loss_scale: 32768.0000 (37287.7328)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0005  max mem: 6186
Epoch: [25]  [14770/40201]  eta: 3:57:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0158 (3.4241)  loss_scale: 32768.0000 (37284.6729)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0011  max mem: 6186
Epoch: [25]  [14780/40201]  eta: 3:57:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1267 (3.4240)  loss_scale: 32768.0000 (37281.6172)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0011  max mem: 6186
Epoch: [25]  [14790/40201]  eta: 3:56:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5807 (3.4243)  loss_scale: 32768.0000 (37278.5656)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0005  max mem: 6186
Epoch: [25]  [14800/40201]  eta: 3:56:47  lr: 0.000003  min_lr: 0.000000  loss: 3.6834 (3.4245)  loss_scale: 32768.0000 (37275.5181)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0005  max mem: 6186
Epoch: [25]  [14810/40201]  eta: 3:56:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5369 (3.4248)  loss_scale: 32768.0000 (37272.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [14820/40201]  eta: 3:56:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3684 (3.4247)  loss_scale: 32768.0000 (37269.4355)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0004  max mem: 6186
Epoch: [25]  [14830/40201]  eta: 3:56:27  lr: 0.000003  min_lr: 0.000000  loss: 2.8543 (3.4245)  loss_scale: 32768.0000 (37266.4004)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0004  max mem: 6186
Epoch: [25]  [14840/40201]  eta: 3:56:19  lr: 0.000003  min_lr: 0.000000  loss: 2.8543 (3.4244)  loss_scale: 32768.0000 (37263.3693)  weight_decay: 0.0500 (0.0500)  time: 0.4645  data: 0.0004  max mem: 6186
Epoch: [25]  [14850/40201]  eta: 3:56:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2792 (3.4244)  loss_scale: 32768.0000 (37260.3423)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0004  max mem: 6186
Epoch: [25]  [14860/40201]  eta: 3:55:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5731 (3.4245)  loss_scale: 32768.0000 (37257.3194)  weight_decay: 0.0500 (0.0500)  time: 0.2049  data: 0.0010  max mem: 6186
Epoch: [25]  [14870/40201]  eta: 3:55:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2983 (3.4246)  loss_scale: 32768.0000 (37254.3006)  weight_decay: 0.0500 (0.0500)  time: 0.2574  data: 0.0228  max mem: 6186
[2023-07-24 17:33:34,199] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:33:34,199] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:33:34,199] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:33:34,199] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [14880/40201]  eta: 3:55:40  lr: 0.000003  min_lr: 0.000000  loss: 3.1506 (3.4245)  loss_scale: 32768.0000 (37255.6898)  weight_decay: 0.0500 (0.0500)  time: 0.4115  data: 0.1661  max mem: 6186
Epoch: [25]  [14890/40201]  eta: 3:55:29  lr: 0.000003  min_lr: 0.000000  loss: 3.0264 (3.4244)  loss_scale: 65536.0000 (37274.6814)  weight_decay: 0.0500 (0.0500)  time: 0.4043  data: 0.1905  max mem: 6186
Epoch: [25]  [14900/40201]  eta: 3:55:19  lr: 0.000003  min_lr: 0.000000  loss: 3.0193 (3.4243)  loss_scale: 65536.0000 (37293.6474)  weight_decay: 0.0500 (0.0500)  time: 0.2921  data: 0.1090  max mem: 6186
Epoch: [25]  [14910/40201]  eta: 3:55:13  lr: 0.000003  min_lr: 0.000000  loss: 3.1027 (3.4243)  loss_scale: 65536.0000 (37312.5880)  weight_decay: 0.0500 (0.0500)  time: 0.4069  data: 0.0974  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/oyj6TFAxpiw_000229_000239.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/oyj6TFAxpiw_000229_000239.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [14920/40201]  eta: 3:55:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3296 (3.4244)  loss_scale: 65536.0000 (37331.5033)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0352  max mem: 6186
Epoch: [25]  [14930/40201]  eta: 3:55:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4603 (3.4245)  loss_scale: 65536.0000 (37350.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0004  max mem: 6186
[2023-07-24 17:33:59,740] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7469
[2023-07-24 17:33:59,740] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:33:59,741] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7469
[2023-07-24 17:33:59,742] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:33:59,742] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [14940/40201]  eta: 3:54:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4393 (3.4245)  loss_scale: 65536.0000 (37364.8714)  weight_decay: 0.0500 (0.0500)  time: 0.4814  data: 0.0008  max mem: 6186
Epoch: [25]  [14950/40201]  eta: 3:54:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4243 (3.4243)  loss_scale: 32768.0000 (37361.7968)  weight_decay: 0.0500 (0.0500)  time: 0.4864  data: 0.0016  max mem: 6186
Epoch: [25]  [14960/40201]  eta: 3:54:39  lr: 0.000003  min_lr: 0.000000  loss: 2.8631 (3.4240)  loss_scale: 32768.0000 (37358.7263)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0012  max mem: 6186
Epoch: [25]  [14970/40201]  eta: 3:54:28  lr: 0.000003  min_lr: 0.000000  loss: 2.9015 (3.4239)  loss_scale: 32768.0000 (37355.6599)  weight_decay: 0.0500 (0.0500)  time: 0.3338  data: 0.0010  max mem: 6186
Epoch: [25]  [14980/40201]  eta: 3:54:17  lr: 0.000003  min_lr: 0.000000  loss: 3.0957 (3.4237)  loss_scale: 32768.0000 (37352.5976)  weight_decay: 0.0500 (0.0500)  time: 0.2325  data: 0.0010  max mem: 6186
Epoch: [25]  [14990/40201]  eta: 3:54:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4440 (3.4240)  loss_scale: 32768.0000 (37349.5393)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0012  max mem: 6186
[2023-07-24 17:34:23,146] [INFO] [timer.py:181:stop] 0/15000, SamplesPerSec=12.924354779397289
Epoch: [25]  [15000/40201]  eta: 3:54:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7579 (3.4240)  loss_scale: 32768.0000 (37346.4852)  weight_decay: 0.0500 (0.0500)  time: 0.4355  data: 0.0019  max mem: 6186
Epoch: [25]  [15010/40201]  eta: 3:53:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2412 (3.4239)  loss_scale: 32768.0000 (37343.4351)  weight_decay: 0.0500 (0.0500)  time: 0.4502  data: 0.0012  max mem: 6186
Epoch: [25]  [15020/40201]  eta: 3:53:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2331 (3.4238)  loss_scale: 32768.0000 (37340.3891)  weight_decay: 0.0500 (0.0500)  time: 0.3745  data: 0.0005  max mem: 6186
Epoch: [25]  [15030/40201]  eta: 3:53:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0898 (3.4235)  loss_scale: 32768.0000 (37337.3471)  weight_decay: 0.0500 (0.0500)  time: 0.4390  data: 0.0009  max mem: 6186
Epoch: [25]  [15040/40201]  eta: 3:53:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1452 (3.4235)  loss_scale: 32768.0000 (37334.3092)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0015  max mem: 6186
Epoch: [25]  [15050/40201]  eta: 3:53:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5133 (3.4235)  loss_scale: 32768.0000 (37331.2753)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0011  max mem: 6186
Epoch: [25]  [15060/40201]  eta: 3:53:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0622 (3.4234)  loss_scale: 32768.0000 (37328.2454)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0004  max mem: 6186
Epoch: [25]  [15070/40201]  eta: 3:53:12  lr: 0.000003  min_lr: 0.000000  loss: 2.9517 (3.4233)  loss_scale: 32768.0000 (37325.2196)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0005  max mem: 6186
Epoch: [25]  [15080/40201]  eta: 3:53:06  lr: 0.000003  min_lr: 0.000000  loss: 3.4168 (3.4236)  loss_scale: 32768.0000 (37322.1977)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0005  max mem: 6186
Epoch: [25]  [15090/40201]  eta: 3:52:59  lr: 0.000003  min_lr: 0.000000  loss: 3.6740 (3.4238)  loss_scale: 32768.0000 (37319.1799)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0004  max mem: 6186
Epoch: [25]  [15100/40201]  eta: 3:52:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3173 (3.4239)  loss_scale: 32768.0000 (37316.1661)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0004  max mem: 6186
Epoch: [25]  [15110/40201]  eta: 3:52:46  lr: 0.000003  min_lr: 0.000000  loss: 3.3125 (3.4238)  loss_scale: 32768.0000 (37313.1562)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0004  max mem: 6186
Epoch: [25]  [15120/40201]  eta: 3:52:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0342 (3.4236)  loss_scale: 32768.0000 (37310.1504)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0004  max mem: 6186
Epoch: [25]  [15130/40201]  eta: 3:52:33  lr: 0.000003  min_lr: 0.000000  loss: 3.0605 (3.4236)  loss_scale: 32768.0000 (37307.1485)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0004  max mem: 6186
Epoch: [25]  [15140/40201]  eta: 3:52:26  lr: 0.000003  min_lr: 0.000000  loss: 3.4643 (3.4235)  loss_scale: 32768.0000 (37304.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0013  max mem: 6186
Epoch: [25]  [15150/40201]  eta: 3:52:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4643 (3.4238)  loss_scale: 32768.0000 (37301.1566)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0013  max mem: 6186
Epoch: [25]  [15160/40201]  eta: 3:52:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2946 (3.4236)  loss_scale: 32768.0000 (37298.1666)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [15170/40201]  eta: 3:52:06  lr: 0.000003  min_lr: 0.000000  loss: 2.9418 (3.4233)  loss_scale: 32768.0000 (37295.1805)  weight_decay: 0.0500 (0.0500)  time: 0.4912  data: 0.0004  max mem: 6186
Epoch: [25]  [15180/40201]  eta: 3:52:00  lr: 0.000003  min_lr: 0.000000  loss: 2.8657 (3.4232)  loss_scale: 32768.0000 (37292.1984)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0004  max mem: 6186
Epoch: [25]  [15190/40201]  eta: 3:51:53  lr: 0.000003  min_lr: 0.000000  loss: 3.1864 (3.4230)  loss_scale: 32768.0000 (37289.2202)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0009  max mem: 6186
[2023-07-24 17:35:58,971] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:35:58,971] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:35:58,980] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:35:58,980] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [15200/40201]  eta: 3:51:47  lr: 0.000003  min_lr: 0.000000  loss: 3.0614 (3.4228)  loss_scale: 32768.0000 (37294.8685)  weight_decay: 0.0500 (0.0500)  time: 0.4915  data: 0.0009  max mem: 6186
[2023-07-24 17:36:04,724] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7604
[2023-07-24 17:36:04,724] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:36:04,724] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7604
[2023-07-24 17:36:04,724] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:36:04,724] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [15210/40201]  eta: 3:51:40  lr: 0.000003  min_lr: 0.000000  loss: 3.6552 (3.4230)  loss_scale: 65536.0000 (37309.1263)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0013  max mem: 6186
Epoch: [25]  [15220/40201]  eta: 3:51:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7206 (3.4232)  loss_scale: 32768.0000 (37306.1428)  weight_decay: 0.0500 (0.0500)  time: 0.4775  data: 0.0013  max mem: 6186
Epoch: [25]  [15230/40201]  eta: 3:51:27  lr: 0.000003  min_lr: 0.000000  loss: 3.4271 (3.4232)  loss_scale: 32768.0000 (37303.1633)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0005  max mem: 6186
Epoch: [25]  [15240/40201]  eta: 3:51:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1405 (3.4231)  loss_scale: 32768.0000 (37300.1877)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0008  max mem: 6186
Epoch: [25]  [15250/40201]  eta: 3:51:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5002 (3.4232)  loss_scale: 32768.0000 (37297.2159)  weight_decay: 0.0500 (0.0500)  time: 0.4925  data: 0.0008  max mem: 6186
Epoch: [25]  [15260/40201]  eta: 3:51:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2374 (3.4230)  loss_scale: 32768.0000 (37294.2481)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0005  max mem: 6186
Epoch: [25]  [15270/40201]  eta: 3:51:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1781 (3.4228)  loss_scale: 32768.0000 (37291.2841)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0004  max mem: 6186
Epoch: [25]  [15280/40201]  eta: 3:50:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4293 (3.4232)  loss_scale: 32768.0000 (37288.3241)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0004  max mem: 6186
Epoch: [25]  [15290/40201]  eta: 3:50:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4293 (3.4229)  loss_scale: 32768.0000 (37285.3679)  weight_decay: 0.0500 (0.0500)  time: 0.4919  data: 0.0004  max mem: 6186
Epoch: [25]  [15300/40201]  eta: 3:50:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1111 (3.4229)  loss_scale: 32768.0000 (37282.4155)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0004  max mem: 6186
Epoch: [25]  [15310/40201]  eta: 3:50:30  lr: 0.000003  min_lr: 0.000000  loss: 3.0457 (3.4226)  loss_scale: 32768.0000 (37279.4670)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0005  max mem: 6186
Epoch: [25]  [15320/40201]  eta: 3:50:19  lr: 0.000003  min_lr: 0.000000  loss: 3.1609 (3.4226)  loss_scale: 32768.0000 (37276.5224)  weight_decay: 0.0500 (0.0500)  time: 0.2097  data: 0.0011  max mem: 6186
Epoch: [25]  [15330/40201]  eta: 3:50:09  lr: 0.000003  min_lr: 0.000000  loss: 3.3353 (3.4225)  loss_scale: 32768.0000 (37273.5816)  weight_decay: 0.0500 (0.0500)  time: 0.2425  data: 0.0675  max mem: 6186
Epoch: [25]  [15340/40201]  eta: 3:49:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5708 (3.4227)  loss_scale: 32768.0000 (37270.6447)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.1342  max mem: 6186
Epoch: [25]  [15350/40201]  eta: 3:49:50  lr: 0.000003  min_lr: 0.000000  loss: 3.3588 (3.4226)  loss_scale: 32768.0000 (37267.7115)  weight_decay: 0.0500 (0.0500)  time: 0.3005  data: 0.1021  max mem: 6186
Epoch: [25]  [15360/40201]  eta: 3:49:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1859 (3.4225)  loss_scale: 32768.0000 (37264.7822)  weight_decay: 0.0500 (0.0500)  time: 0.3669  data: 0.0698  max mem: 6186
Epoch: [25]  [15370/40201]  eta: 3:49:32  lr: 0.000003  min_lr: 0.000000  loss: 3.4246 (3.4227)  loss_scale: 32768.0000 (37261.8567)  weight_decay: 0.0500 (0.0500)  time: 0.3490  data: 0.0369  max mem: 6186
Epoch: [25]  [15380/40201]  eta: 3:49:27  lr: 0.000003  min_lr: 0.000000  loss: 3.4769 (3.4227)  loss_scale: 32768.0000 (37258.9350)  weight_decay: 0.0500 (0.0500)  time: 0.4199  data: 0.0019  max mem: 6186
Epoch: [25]  [15390/40201]  eta: 3:49:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4769 (3.4226)  loss_scale: 32768.0000 (37256.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0004  max mem: 6186
Epoch: [25]  [15400/40201]  eta: 3:49:14  lr: 0.000003  min_lr: 0.000000  loss: 3.0677 (3.4227)  loss_scale: 32768.0000 (37253.1030)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0004  max mem: 6186
Epoch: [25]  [15410/40201]  eta: 3:49:07  lr: 0.000003  min_lr: 0.000000  loss: 2.8952 (3.4224)  loss_scale: 32768.0000 (37250.1927)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0004  max mem: 6186
Epoch: [25]  [15420/40201]  eta: 3:49:01  lr: 0.000003  min_lr: 0.000000  loss: 3.1520 (3.4223)  loss_scale: 32768.0000 (37247.2862)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0004  max mem: 6186
Epoch: [25]  [15430/40201]  eta: 3:48:54  lr: 0.000003  min_lr: 0.000000  loss: 3.1520 (3.4222)  loss_scale: 32768.0000 (37244.3834)  weight_decay: 0.0500 (0.0500)  time: 0.4939  data: 0.0013  max mem: 6186
Epoch: [25]  [15440/40201]  eta: 3:48:48  lr: 0.000003  min_lr: 0.000000  loss: 3.1227 (3.4221)  loss_scale: 32768.0000 (37241.4844)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0012  max mem: 6186
Epoch: [25]  [15450/40201]  eta: 3:48:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4562 (3.4222)  loss_scale: 32768.0000 (37238.5891)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0004  max mem: 6186
Epoch: [25]  [15460/40201]  eta: 3:48:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4562 (3.4221)  loss_scale: 32768.0000 (37235.6976)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0005  max mem: 6186
[2023-07-24 17:37:59,292] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:37:59,292] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:37:59,307] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:37:59,308] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [15470/40201]  eta: 3:48:29  lr: 0.000003  min_lr: 0.000000  loss: 3.0489 (3.4221)  loss_scale: 32768.0000 (37241.2819)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0004  max mem: 6186
[2023-07-24 17:38:00,957] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7735
[2023-07-24 17:38:00,957] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:38:00,957] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:38:00,959] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7735
[2023-07-24 17:38:00,959] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [15480/40201]  eta: 3:48:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2389 (3.4221)  loss_scale: 32768.0000 (37238.3924)  weight_decay: 0.0500 (0.0500)  time: 0.4718  data: 0.0005  max mem: 6186
Epoch: [25]  [15490/40201]  eta: 3:48:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4430 (3.4223)  loss_scale: 32768.0000 (37235.5066)  weight_decay: 0.0500 (0.0500)  time: 0.4737  data: 0.0004  max mem: 6186
Epoch: [25]  [15500/40201]  eta: 3:48:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7101 (3.4223)  loss_scale: 32768.0000 (37232.6245)  weight_decay: 0.0500 (0.0500)  time: 0.4088  data: 0.0004  max mem: 6186
Epoch: [25]  [15510/40201]  eta: 3:47:56  lr: 0.000003  min_lr: 0.000000  loss: 3.4113 (3.4224)  loss_scale: 32768.0000 (37229.7461)  weight_decay: 0.0500 (0.0500)  time: 0.2999  data: 0.0443  max mem: 6186
Epoch: [25]  [15520/40201]  eta: 3:47:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4113 (3.4225)  loss_scale: 32768.0000 (37226.8715)  weight_decay: 0.0500 (0.0500)  time: 0.2831  data: 0.0956  max mem: 6186
Epoch: [25]  [15530/40201]  eta: 3:47:35  lr: 0.000003  min_lr: 0.000000  loss: 3.4496 (3.4225)  loss_scale: 32768.0000 (37224.0005)  weight_decay: 0.0500 (0.0500)  time: 0.2368  data: 0.0517  max mem: 6186
Epoch: [25]  [15540/40201]  eta: 3:47:26  lr: 0.000003  min_lr: 0.000000  loss: 3.3154 (3.4224)  loss_scale: 32768.0000 (37221.1333)  weight_decay: 0.0500 (0.0500)  time: 0.2655  data: 0.0843  max mem: 6186
Epoch: [25]  [15550/40201]  eta: 3:47:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2457 (3.4224)  loss_scale: 32768.0000 (37218.2697)  weight_decay: 0.0500 (0.0500)  time: 0.2943  data: 0.1060  max mem: 6186
Epoch: [25]  [15560/40201]  eta: 3:47:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5007 (3.4227)  loss_scale: 32768.0000 (37215.4098)  weight_decay: 0.0500 (0.0500)  time: 0.2965  data: 0.0346  max mem: 6186
Epoch: [25]  [15570/40201]  eta: 3:47:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8559 (3.4229)  loss_scale: 32768.0000 (37212.5536)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0129  max mem: 6186
Epoch: [25]  [15580/40201]  eta: 3:46:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1212 (3.4227)  loss_scale: 32768.0000 (37209.7010)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.0012  max mem: 6186
Epoch: [25]  [15590/40201]  eta: 3:46:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3068 (3.4228)  loss_scale: 32768.0000 (37206.8522)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0012  max mem: 6186
Epoch: [25]  [15600/40201]  eta: 3:46:46  lr: 0.000003  min_lr: 0.000000  loss: 3.3068 (3.4226)  loss_scale: 32768.0000 (37204.0069)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0005  max mem: 6186
Epoch: [25]  [15610/40201]  eta: 3:46:40  lr: 0.000003  min_lr: 0.000000  loss: 2.9663 (3.4224)  loss_scale: 32768.0000 (37201.1653)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0004  max mem: 6186
Epoch: [25]  [15620/40201]  eta: 3:46:34  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4226)  loss_scale: 32768.0000 (37198.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [15630/40201]  eta: 3:46:27  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4226)  loss_scale: 32768.0000 (37195.4931)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0005  max mem: 6186
Epoch: [25]  [15640/40201]  eta: 3:46:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4203 (3.4226)  loss_scale: 32768.0000 (37192.6624)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0005  max mem: 6186
Epoch: [25]  [15650/40201]  eta: 3:46:15  lr: 0.000003  min_lr: 0.000000  loss: 3.1057 (3.4227)  loss_scale: 32768.0000 (37189.8353)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0005  max mem: 6186
Epoch: [25]  [15660/40201]  eta: 3:46:08  lr: 0.000003  min_lr: 0.000000  loss: 3.2777 (3.4228)  loss_scale: 32768.0000 (37187.0118)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0004  max mem: 6186
Epoch: [25]  [15670/40201]  eta: 3:46:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2777 (3.4226)  loss_scale: 32768.0000 (37184.1919)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0005  max mem: 6186
Epoch: [25]  [15680/40201]  eta: 3:45:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1425 (3.4226)  loss_scale: 32768.0000 (37181.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0004  max mem: 6186
Epoch: [25]  [15690/40201]  eta: 3:45:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3403 (3.4225)  loss_scale: 32768.0000 (37178.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0004  max mem: 6186
Epoch: [25]  [15700/40201]  eta: 3:45:43  lr: 0.000003  min_lr: 0.000000  loss: 3.4366 (3.4225)  loss_scale: 32768.0000 (37175.7539)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0005  max mem: 6186
Epoch: [25]  [15710/40201]  eta: 3:45:37  lr: 0.000003  min_lr: 0.000000  loss: 3.4366 (3.4226)  loss_scale: 32768.0000 (37172.9484)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0013  max mem: 6186
Epoch: [25]  [15720/40201]  eta: 3:45:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6301 (3.4227)  loss_scale: 32768.0000 (37170.1464)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0012  max mem: 6186
[2023-07-24 17:39:58,425] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:39:58,425] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:39:58,432] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:39:58,432] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [15730/40201]  eta: 3:45:24  lr: 0.000003  min_lr: 0.000000  loss: 2.9109 (3.4223)  loss_scale: 32768.0000 (37171.5141)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0005  max mem: 6186
Epoch: [25]  [15740/40201]  eta: 3:45:18  lr: 0.000003  min_lr: 0.000000  loss: 2.9267 (3.4225)  loss_scale: 65536.0000 (37189.5336)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0005  max mem: 6186
[2023-07-24 17:40:08,271] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7874
[2023-07-24 17:40:08,271] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:40:08,275] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 7874
[2023-07-24 17:40:08,275] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:40:08,275] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [15750/40201]  eta: 3:45:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2683 (3.4222)  loss_scale: 65536.0000 (37203.3694)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0005  max mem: 6186
Epoch: [25]  [15760/40201]  eta: 3:45:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2346 (3.4223)  loss_scale: 32768.0000 (37200.5553)  weight_decay: 0.0500 (0.0500)  time: 0.4901  data: 0.0005  max mem: 6186
Epoch: [25]  [15770/40201]  eta: 3:44:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2346 (3.4221)  loss_scale: 32768.0000 (37197.7447)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0004  max mem: 6186
Epoch: [25]  [15780/40201]  eta: 3:44:52  lr: 0.000003  min_lr: 0.000000  loss: 3.1733 (3.4221)  loss_scale: 32768.0000 (37194.9377)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0004  max mem: 6186
Epoch: [25]  [15790/40201]  eta: 3:44:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2500 (3.4219)  loss_scale: 32768.0000 (37192.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0004  max mem: 6186
Epoch: [25]  [15800/40201]  eta: 3:44:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2500 (3.4218)  loss_scale: 32768.0000 (37189.3343)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [15810/40201]  eta: 3:44:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5113 (3.4219)  loss_scale: 32768.0000 (37186.5380)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
Epoch: [25]  [15820/40201]  eta: 3:44:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2710 (3.4218)  loss_scale: 32768.0000 (37183.7451)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0005  max mem: 6186
Epoch: [25]  [15830/40201]  eta: 3:44:21  lr: 0.000003  min_lr: 0.000000  loss: 3.2026 (3.4216)  loss_scale: 32768.0000 (37180.9558)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0012  max mem: 6186
Epoch: [25]  [15840/40201]  eta: 3:44:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2026 (3.4216)  loss_scale: 32768.0000 (37178.1701)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0016  max mem: 6186
Epoch: [25]  [15850/40201]  eta: 3:44:05  lr: 0.000003  min_lr: 0.000000  loss: 3.1040 (3.4215)  loss_scale: 32768.0000 (37175.3878)  weight_decay: 0.0500 (0.0500)  time: 0.3995  data: 0.0009  max mem: 6186
Epoch: [25]  [15860/40201]  eta: 3:43:59  lr: 0.000003  min_lr: 0.000000  loss: 3.0731 (3.4214)  loss_scale: 32768.0000 (37172.6090)  weight_decay: 0.0500 (0.0500)  time: 0.4152  data: 0.0004  max mem: 6186
Epoch: [25]  [15870/40201]  eta: 3:43:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1768 (3.4214)  loss_scale: 32768.0000 (37169.8338)  weight_decay: 0.0500 (0.0500)  time: 0.4011  data: 0.0464  max mem: 6186
Epoch: [25]  [15880/40201]  eta: 3:43:40  lr: 0.000003  min_lr: 0.000000  loss: 3.1768 (3.4213)  loss_scale: 32768.0000 (37167.0620)  weight_decay: 0.0500 (0.0500)  time: 0.2866  data: 0.0848  max mem: 6186
Epoch: [25]  [15890/40201]  eta: 3:43:30  lr: 0.000003  min_lr: 0.000000  loss: 3.2628 (3.4214)  loss_scale: 32768.0000 (37164.2938)  weight_decay: 0.0500 (0.0500)  time: 0.2673  data: 0.0433  max mem: 6186
Epoch: [25]  [15900/40201]  eta: 3:43:29  lr: 0.000003  min_lr: 0.000000  loss: 3.1561 (3.4212)  loss_scale: 32768.0000 (37161.5290)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0465  max mem: 6186
Epoch: [25]  [15910/40201]  eta: 3:43:22  lr: 0.000003  min_lr: 0.000000  loss: 2.7891 (3.4209)  loss_scale: 32768.0000 (37158.7676)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.0420  max mem: 6186
Epoch: [25]  [15920/40201]  eta: 3:43:16  lr: 0.000003  min_lr: 0.000000  loss: 2.9323 (3.4206)  loss_scale: 32768.0000 (37156.0098)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0004  max mem: 6186
Epoch: [25]  [15930/40201]  eta: 3:43:10  lr: 0.000003  min_lr: 0.000000  loss: 2.9809 (3.4202)  loss_scale: 32768.0000 (37153.2554)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0004  max mem: 6186
Epoch: [25]  [15940/40201]  eta: 3:43:03  lr: 0.000003  min_lr: 0.000000  loss: 2.9792 (3.4200)  loss_scale: 32768.0000 (37150.5045)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
Epoch: [25]  [15950/40201]  eta: 3:42:57  lr: 0.000003  min_lr: 0.000000  loss: 2.9792 (3.4197)  loss_scale: 32768.0000 (37147.7570)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0004  max mem: 6186
Epoch: [25]  [15960/40201]  eta: 3:42:51  lr: 0.000003  min_lr: 0.000000  loss: 3.1518 (3.4197)  loss_scale: 32768.0000 (37145.0130)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0004  max mem: 6186
Epoch: [25]  [15970/40201]  eta: 3:42:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1601 (3.4197)  loss_scale: 32768.0000 (37142.2724)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0005  max mem: 6186
Epoch: [25]  [15980/40201]  eta: 3:42:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4035 (3.4197)  loss_scale: 32768.0000 (37139.5352)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0004  max mem: 6186
Epoch: [25]  [15990/40201]  eta: 3:42:32  lr: 0.000003  min_lr: 0.000000  loss: 3.3294 (3.4197)  loss_scale: 32768.0000 (37136.8015)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0004  max mem: 6186
[2023-07-24 17:42:08,455] [INFO] [logging.py:69:log_dist] [Rank 0] step=8000, skipped=41, lr=[6.446840342369407e-08, 6.446840342369407e-08, 8.595787123159209e-08, 8.595787123159209e-08, 1.1461049497545612e-07, 1.1461049497545612e-07, 1.5281399330060816e-07, 1.5281399330060816e-07, 2.0375199106747756e-07, 2.0375199106747756e-07, 2.716693214233034e-07, 2.716693214233034e-07, 3.622257618977379e-07, 3.622257618977379e-07, 4.829676825303171e-07, 4.829676825303171e-07, 6.439569100404229e-07, 6.439569100404229e-07, 8.586092133872305e-07, 8.586092133872305e-07, 1.1448122845163074e-06, 1.1448122845163074e-06, 1.5264163793550763e-06, 1.5264163793550763e-06, 2.035221839140102e-06, 2.035221839140102e-06, 2.7136291188534692e-06, 2.7136291188534692e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 17:42:08,460] [INFO] [timer.py:181:stop] 0/16000, SamplesPerSec=13.003464022630023
Epoch: [25]  [16000/40201]  eta: 3:42:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5613 (3.4200)  loss_scale: 32768.0000 (37134.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0004  max mem: 6186
[2023-07-24 17:42:12,125] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:42:12,125] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:42:12,171] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:42:12,171] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [16010/40201]  eta: 3:42:18  lr: 0.000003  min_lr: 0.000000  loss: 3.7496 (3.4199)  loss_scale: 32768.0000 (37139.5306)  weight_decay: 0.0500 (0.0500)  time: 0.4508  data: 0.0004  max mem: 6186
Epoch: [25]  [16020/40201]  eta: 3:42:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0633 (3.4199)  loss_scale: 65536.0000 (37157.2551)  weight_decay: 0.0500 (0.0500)  time: 0.2918  data: 0.0005  max mem: 6186
Epoch: [25]  [16030/40201]  eta: 3:42:00  lr: 0.000003  min_lr: 0.000000  loss: 3.1971 (3.4198)  loss_scale: 65536.0000 (37174.9575)  weight_decay: 0.0500 (0.0500)  time: 0.3393  data: 0.0030  max mem: 6186
[2023-07-24 17:42:20,342] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8017
[2023-07-24 17:42:20,342] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8017
[2023-07-24 17:42:20,342] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:42:20,342] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:42:20,342] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [16040/40201]  eta: 3:41:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2730 (3.4199)  loss_scale: 65536.0000 (37180.3813)  weight_decay: 0.0500 (0.0500)  time: 0.3673  data: 0.0029  max mem: 6186
Epoch: [25]  [16050/40201]  eta: 3:41:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5513 (3.4198)  loss_scale: 32768.0000 (37177.6323)  weight_decay: 0.0500 (0.0500)  time: 0.2406  data: 0.0005  max mem: 6186
Epoch: [25]  [16060/40201]  eta: 3:41:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5394 (3.4198)  loss_scale: 32768.0000 (37174.8867)  weight_decay: 0.0500 (0.0500)  time: 0.2445  data: 0.0004  max mem: 6186
Epoch: [25]  [16070/40201]  eta: 3:41:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5394 (3.4198)  loss_scale: 32768.0000 (37172.1446)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0137  max mem: 6186
Epoch: [25]  [16080/40201]  eta: 3:41:14  lr: 0.000003  min_lr: 0.000000  loss: 3.7161 (3.4200)  loss_scale: 32768.0000 (37169.4059)  weight_decay: 0.0500 (0.0500)  time: 0.3842  data: 0.0137  max mem: 6186
Epoch: [25]  [16090/40201]  eta: 3:41:08  lr: 0.000003  min_lr: 0.000000  loss: 3.5431 (3.4197)  loss_scale: 32768.0000 (37166.6706)  weight_decay: 0.0500 (0.0500)  time: 0.4527  data: 0.0005  max mem: 6186
Epoch: [25]  [16100/40201]  eta: 3:41:01  lr: 0.000003  min_lr: 0.000000  loss: 3.1092 (3.4197)  loss_scale: 32768.0000 (37163.9386)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0004  max mem: 6186
Epoch: [25]  [16110/40201]  eta: 3:40:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2170 (3.4196)  loss_scale: 32768.0000 (37161.2101)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0013  max mem: 6186
Epoch: [25]  [16120/40201]  eta: 3:40:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3813 (3.4196)  loss_scale: 32768.0000 (37158.4850)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0013  max mem: 6186
Epoch: [25]  [16130/40201]  eta: 3:40:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6967 (3.4198)  loss_scale: 32768.0000 (37155.7632)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0011  max mem: 6186
Epoch: [25]  [16140/40201]  eta: 3:40:36  lr: 0.000003  min_lr: 0.000000  loss: 3.5785 (3.4197)  loss_scale: 32768.0000 (37153.0448)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0011  max mem: 6186
Epoch: [25]  [16150/40201]  eta: 3:40:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1327 (3.4197)  loss_scale: 32768.0000 (37150.3298)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0004  max mem: 6186
Epoch: [25]  [16160/40201]  eta: 3:40:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4535 (3.4197)  loss_scale: 32768.0000 (37147.6181)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0005  max mem: 6186
Epoch: [25]  [16170/40201]  eta: 3:40:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6708 (3.4200)  loss_scale: 32768.0000 (37144.9098)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0004  max mem: 6186
Epoch: [25]  [16180/40201]  eta: 3:40:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4869 (3.4199)  loss_scale: 32768.0000 (37142.2048)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
Epoch: [25]  [16190/40201]  eta: 3:40:05  lr: 0.000003  min_lr: 0.000000  loss: 3.3628 (3.4201)  loss_scale: 32768.0000 (37139.5032)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0005  max mem: 6186
Epoch: [25]  [16200/40201]  eta: 3:39:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5473 (3.4203)  loss_scale: 32768.0000 (37136.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0005  max mem: 6186
Epoch: [25]  [16210/40201]  eta: 3:39:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4301 (3.4204)  loss_scale: 32768.0000 (37134.1099)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0013  max mem: 6186
Epoch: [25]  [16220/40201]  eta: 3:39:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4316 (3.4205)  loss_scale: 32768.0000 (37131.4183)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0013  max mem: 6186
Epoch: [25]  [16230/40201]  eta: 3:39:41  lr: 0.000003  min_lr: 0.000000  loss: 3.5270 (3.4207)  loss_scale: 32768.0000 (37128.7300)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0005  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/DSNcuU-e8bU_000021_000031.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [16240/40201]  eta: 3:39:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1124 (3.4203)  loss_scale: 32768.0000 (37126.0449)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0005  max mem: 6186
Epoch: [25]  [16250/40201]  eta: 3:39:29  lr: 0.000003  min_lr: 0.000000  loss: 2.9244 (3.4202)  loss_scale: 32768.0000 (37123.3632)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0012  max mem: 6186
Epoch: [25]  [16260/40201]  eta: 3:39:22  lr: 0.000003  min_lr: 0.000000  loss: 3.1081 (3.4202)  loss_scale: 32768.0000 (37120.6848)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0011  max mem: 6186
Epoch: [25]  [16270/40201]  eta: 3:39:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1611 (3.4202)  loss_scale: 32768.0000 (37118.0097)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0004  max mem: 6186
Epoch: [25]  [16280/40201]  eta: 3:39:10  lr: 0.000003  min_lr: 0.000000  loss: 3.3277 (3.4203)  loss_scale: 32768.0000 (37115.3379)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0004  max mem: 6186
Epoch: [25]  [16290/40201]  eta: 3:39:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3277 (3.4203)  loss_scale: 32768.0000 (37112.6693)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0004  max mem: 6186
[2023-07-24 17:44:21,753] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:44:21,753] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:44:21,756] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:44:21,756] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [16300/40201]  eta: 3:38:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5031 (3.4204)  loss_scale: 32768.0000 (37126.0855)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0004  max mem: 6186
Epoch: [25]  [16310/40201]  eta: 3:38:51  lr: 0.000003  min_lr: 0.000000  loss: 3.0364 (3.4201)  loss_scale: 65536.0000 (37143.5032)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0004  max mem: 6186
Epoch: [25]  [16320/40201]  eta: 3:38:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3115 (3.4201)  loss_scale: 65536.0000 (37160.8995)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0004  max mem: 6186
Epoch: [25]  [16330/40201]  eta: 3:38:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3697 (3.4200)  loss_scale: 65536.0000 (37178.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0004  max mem: 6186
[2023-07-24 17:44:41,506] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8166
[2023-07-24 17:44:41,506] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:44:41,506] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:44:41,508] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8166
[2023-07-24 17:44:41,508] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [16340/40201]  eta: 3:38:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1618 (3.4201)  loss_scale: 65536.0000 (37179.5861)  weight_decay: 0.0500 (0.0500)  time: 0.4746  data: 0.0012  max mem: 6186
Epoch: [25]  [16350/40201]  eta: 3:38:26  lr: 0.000003  min_lr: 0.000000  loss: 3.1635 (3.4200)  loss_scale: 32768.0000 (37176.8880)  weight_decay: 0.0500 (0.0500)  time: 0.4763  data: 0.0018  max mem: 6186
Epoch: [25]  [16360/40201]  eta: 3:38:19  lr: 0.000003  min_lr: 0.000000  loss: 2.8496 (3.4196)  loss_scale: 32768.0000 (37174.1933)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0010  max mem: 6186
Epoch: [25]  [16370/40201]  eta: 3:38:13  lr: 0.000003  min_lr: 0.000000  loss: 2.8496 (3.4196)  loss_scale: 32768.0000 (37171.5018)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [16380/40201]  eta: 3:38:07  lr: 0.000003  min_lr: 0.000000  loss: 3.8486 (3.4198)  loss_scale: 32768.0000 (37168.8136)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
Epoch: [25]  [16390/40201]  eta: 3:38:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5346 (3.4199)  loss_scale: 32768.0000 (37166.1287)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0004  max mem: 6186
Epoch: [25]  [16400/40201]  eta: 3:37:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1574 (3.4197)  loss_scale: 32768.0000 (37163.4471)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0004  max mem: 6186
Epoch: [25]  [16410/40201]  eta: 3:37:48  lr: 0.000003  min_lr: 0.000000  loss: 2.9809 (3.4197)  loss_scale: 32768.0000 (37160.7688)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0004  max mem: 6186
Epoch: [25]  [16420/40201]  eta: 3:37:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1814 (3.4197)  loss_scale: 32768.0000 (37158.0937)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0004  max mem: 6186
Epoch: [25]  [16430/40201]  eta: 3:37:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4715 (3.4198)  loss_scale: 32768.0000 (37155.4218)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0004  max mem: 6186
Epoch: [25]  [16440/40201]  eta: 3:37:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4715 (3.4200)  loss_scale: 32768.0000 (37152.7532)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0013  max mem: 6186
Epoch: [25]  [16450/40201]  eta: 3:37:24  lr: 0.000003  min_lr: 0.000000  loss: 3.8137 (3.4203)  loss_scale: 32768.0000 (37150.0879)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0013  max mem: 6186
Epoch: [25]  [16460/40201]  eta: 3:37:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6050 (3.4202)  loss_scale: 32768.0000 (37147.4258)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0011  max mem: 6186
Epoch: [25]  [16470/40201]  eta: 3:37:11  lr: 0.000003  min_lr: 0.000000  loss: 3.0600 (3.4200)  loss_scale: 32768.0000 (37144.7669)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0018  max mem: 6186
Epoch: [25]  [16480/40201]  eta: 3:37:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2601 (3.4201)  loss_scale: 32768.0000 (37142.1113)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0011  max mem: 6186
Epoch: [25]  [16490/40201]  eta: 3:36:59  lr: 0.000003  min_lr: 0.000000  loss: 3.4024 (3.4199)  loss_scale: 32768.0000 (37139.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0012  max mem: 6186
Epoch: [25]  [16500/40201]  eta: 3:36:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4024 (3.4200)  loss_scale: 32768.0000 (37136.8096)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0013  max mem: 6186
Epoch: [25]  [16510/40201]  eta: 3:36:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5352 (3.4200)  loss_scale: 32768.0000 (37134.1636)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0005  max mem: 6186
Epoch: [25]  [16520/40201]  eta: 3:36:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6490 (3.4202)  loss_scale: 32768.0000 (37131.5209)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0005  max mem: 6186
Epoch: [25]  [16530/40201]  eta: 3:36:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6462 (3.4202)  loss_scale: 32768.0000 (37128.8813)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0005  max mem: 6186
Epoch: [25]  [16540/40201]  eta: 3:36:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3232 (3.4202)  loss_scale: 32768.0000 (37126.2448)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0005  max mem: 6186
Epoch: [25]  [16550/40201]  eta: 3:36:22  lr: 0.000003  min_lr: 0.000000  loss: 3.0868 (3.4198)  loss_scale: 32768.0000 (37123.6116)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0004  max mem: 6186
Epoch: [25]  [16560/40201]  eta: 3:36:16  lr: 0.000003  min_lr: 0.000000  loss: 2.8129 (3.4196)  loss_scale: 32768.0000 (37120.9816)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0005  max mem: 6186
Epoch: [25]  [16570/40201]  eta: 3:36:10  lr: 0.000003  min_lr: 0.000000  loss: 2.8187 (3.4193)  loss_scale: 32768.0000 (37118.3547)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0014  max mem: 6186
Epoch: [25]  [16580/40201]  eta: 3:36:04  lr: 0.000003  min_lr: 0.000000  loss: 2.9377 (3.4192)  loss_scale: 32768.0000 (37115.7310)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0013  max mem: 6186
Epoch: [25]  [16590/40201]  eta: 3:35:57  lr: 0.000003  min_lr: 0.000000  loss: 3.5348 (3.4194)  loss_scale: 32768.0000 (37113.1105)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0004  max mem: 6186
[2023-07-24 17:46:50,795] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:46:50,795] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:46:50,795] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:46:50,795] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [16600/40201]  eta: 3:35:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5735 (3.4195)  loss_scale: 32768.0000 (37130.2317)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0004  max mem: 6186
[2023-07-24 17:46:57,667] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8303
[2023-07-24 17:46:57,667] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:46:57,667] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8303
[2023-07-24 17:46:57,667] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:46:57,667] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [16610/40201]  eta: 3:35:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6709 (3.4196)  loss_scale: 65536.0000 (37139.4416)  weight_decay: 0.0500 (0.0500)  time: 0.4038  data: 0.0004  max mem: 6186
Epoch: [25]  [16620/40201]  eta: 3:35:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3131 (3.4196)  loss_scale: 32768.0000 (37136.8115)  weight_decay: 0.0500 (0.0500)  time: 0.3719  data: 0.0004  max mem: 6186
Epoch: [25]  [16630/40201]  eta: 3:35:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3131 (3.4196)  loss_scale: 32768.0000 (37134.1846)  weight_decay: 0.0500 (0.0500)  time: 0.4103  data: 0.0011  max mem: 6186
Epoch: [25]  [16640/40201]  eta: 3:35:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4089 (3.4196)  loss_scale: 32768.0000 (37131.5608)  weight_decay: 0.0500 (0.0500)  time: 0.2958  data: 0.0011  max mem: 6186
Epoch: [25]  [16650/40201]  eta: 3:35:08  lr: 0.000003  min_lr: 0.000000  loss: 3.1332 (3.4193)  loss_scale: 32768.0000 (37128.9402)  weight_decay: 0.0500 (0.0500)  time: 0.2404  data: 0.0012  max mem: 6186
Epoch: [25]  [16660/40201]  eta: 3:34:58  lr: 0.000003  min_lr: 0.000000  loss: 3.1332 (3.4191)  loss_scale: 32768.0000 (37126.3228)  weight_decay: 0.0500 (0.0500)  time: 0.2410  data: 0.0014  max mem: 6186
Epoch: [25]  [16670/40201]  eta: 3:34:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4040 (3.4192)  loss_scale: 32768.0000 (37123.7085)  weight_decay: 0.0500 (0.0500)  time: 0.2398  data: 0.0069  max mem: 6186
Epoch: [25]  [16680/40201]  eta: 3:34:44  lr: 0.000003  min_lr: 0.000000  loss: 3.5447 (3.4192)  loss_scale: 32768.0000 (37121.0973)  weight_decay: 0.0500 (0.0500)  time: 0.4571  data: 0.0218  max mem: 6186
Epoch: [25]  [16690/40201]  eta: 3:34:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5447 (3.4190)  loss_scale: 32768.0000 (37118.4892)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0156  max mem: 6186
Epoch: [25]  [16700/40201]  eta: 3:34:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6518 (3.4192)  loss_scale: 32768.0000 (37115.8843)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0011  max mem: 6186
Epoch: [25]  [16710/40201]  eta: 3:34:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6328 (3.4191)  loss_scale: 32768.0000 (37113.2825)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0019  max mem: 6186
Epoch: [25]  [16720/40201]  eta: 3:34:19  lr: 0.000003  min_lr: 0.000000  loss: 3.2284 (3.4189)  loss_scale: 32768.0000 (37110.6838)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0013  max mem: 6186
Epoch: [25]  [16730/40201]  eta: 3:34:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2284 (3.4191)  loss_scale: 32768.0000 (37108.0882)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0004  max mem: 6186
Epoch: [25]  [16740/40201]  eta: 3:34:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2203 (3.4190)  loss_scale: 32768.0000 (37105.4957)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0004  max mem: 6186
Epoch: [25]  [16750/40201]  eta: 3:34:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5607 (3.4191)  loss_scale: 32768.0000 (37102.9063)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0013  max mem: 6186
Epoch: [25]  [16760/40201]  eta: 3:33:55  lr: 0.000003  min_lr: 0.000000  loss: 3.9012 (3.4195)  loss_scale: 32768.0000 (37100.3200)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0013  max mem: 6186
Epoch: [25]  [16770/40201]  eta: 3:33:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5877 (3.4196)  loss_scale: 32768.0000 (37097.7368)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0005  max mem: 6186
Epoch: [25]  [16780/40201]  eta: 3:33:42  lr: 0.000003  min_lr: 0.000000  loss: 3.7097 (3.4198)  loss_scale: 32768.0000 (37095.1567)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0005  max mem: 6186
Epoch: [25]  [16790/40201]  eta: 3:33:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4121 (3.4196)  loss_scale: 32768.0000 (37092.5796)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0004  max mem: 6186
Epoch: [25]  [16800/40201]  eta: 3:33:30  lr: 0.000003  min_lr: 0.000000  loss: 3.1356 (3.4196)  loss_scale: 32768.0000 (37090.0056)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0004  max mem: 6186
Epoch: [25]  [16810/40201]  eta: 3:33:24  lr: 0.000003  min_lr: 0.000000  loss: 3.3140 (3.4197)  loss_scale: 32768.0000 (37087.4347)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0005  max mem: 6186
Epoch: [25]  [16820/40201]  eta: 3:33:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1757 (3.4194)  loss_scale: 32768.0000 (37084.8668)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0005  max mem: 6186
Epoch: [25]  [16830/40201]  eta: 3:33:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1917 (3.4195)  loss_scale: 32768.0000 (37082.3019)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0004  max mem: 6186
Epoch: [25]  [16840/40201]  eta: 3:33:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5008 (3.4197)  loss_scale: 32768.0000 (37079.7402)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
Epoch: [25]  [16850/40201]  eta: 3:33:00  lr: 0.000003  min_lr: 0.000000  loss: 3.7175 (3.4198)  loss_scale: 32768.0000 (37077.1814)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0004  max mem: 6186
Epoch: [25]  [16860/40201]  eta: 3:32:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4833 (3.4198)  loss_scale: 32768.0000 (37074.6257)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0004  max mem: 6186
[2023-07-24 17:48:55,241] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:48:55,242] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:48:55,244] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:48:55,244] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [16870/40201]  eta: 3:32:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3725 (3.4199)  loss_scale: 32768.0000 (37083.7266)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0012  max mem: 6186
Epoch: [25]  [16880/40201]  eta: 3:32:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3797 (3.4198)  loss_scale: 65536.0000 (37100.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0011  max mem: 6186
Epoch: [25]  [16890/40201]  eta: 3:32:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2475 (3.4196)  loss_scale: 65536.0000 (37117.4159)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [16900/40201]  eta: 3:32:29  lr: 0.000003  min_lr: 0.000000  loss: 2.8254 (3.4195)  loss_scale: 65536.0000 (37134.2306)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0004  max mem: 6186
Epoch: [25]  [16910/40201]  eta: 3:32:23  lr: 0.000003  min_lr: 0.000000  loss: 2.8732 (3.4191)  loss_scale: 65536.0000 (37151.0255)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0004  max mem: 6186
Epoch: [25]  [16920/40201]  eta: 3:32:17  lr: 0.000003  min_lr: 0.000000  loss: 3.0918 (3.4190)  loss_scale: 65536.0000 (37167.8005)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0011  max mem: 6186
[2023-07-24 17:49:26,999] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8464
[2023-07-24 17:49:26,999] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:49:26,999] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:49:27,001] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8464
[2023-07-24 17:49:27,001] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [16930/40201]  eta: 3:32:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1719 (3.4188)  loss_scale: 65536.0000 (37180.6849)  weight_decay: 0.0500 (0.0500)  time: 0.4754  data: 0.0012  max mem: 6186
Epoch: [25]  [16940/40201]  eta: 3:32:04  lr: 0.000003  min_lr: 0.000000  loss: 3.2896 (3.4187)  loss_scale: 32768.0000 (37178.0802)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0005  max mem: 6186
Epoch: [25]  [16950/40201]  eta: 3:31:58  lr: 0.000003  min_lr: 0.000000  loss: 3.6124 (3.4188)  loss_scale: 32768.0000 (37175.4785)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0009  max mem: 6186
Epoch: [25]  [16960/40201]  eta: 3:31:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7548 (3.4191)  loss_scale: 32768.0000 (37172.8799)  weight_decay: 0.0500 (0.0500)  time: 0.4737  data: 0.0017  max mem: 6186
Epoch: [25]  [16970/40201]  eta: 3:31:41  lr: 0.000003  min_lr: 0.000000  loss: 3.7209 (3.4192)  loss_scale: 32768.0000 (37170.2844)  weight_decay: 0.0500 (0.0500)  time: 0.3354  data: 0.0219  max mem: 6186
Epoch: [25]  [16980/40201]  eta: 3:31:32  lr: 0.000003  min_lr: 0.000000  loss: 3.4534 (3.4195)  loss_scale: 32768.0000 (37167.6919)  weight_decay: 0.0500 (0.0500)  time: 0.2608  data: 0.0818  max mem: 6186
Epoch: [25]  [16990/40201]  eta: 3:31:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4588 (3.4194)  loss_scale: 32768.0000 (37165.1025)  weight_decay: 0.0500 (0.0500)  time: 0.3241  data: 0.1423  max mem: 6186
[2023-07-24 17:49:53,322] [INFO] [timer.py:181:stop] 0/17000, SamplesPerSec=13.066429756508292
Epoch: [25]  [17000/40201]  eta: 3:31:16  lr: 0.000003  min_lr: 0.000000  loss: 3.2603 (3.4192)  loss_scale: 32768.0000 (37162.5161)  weight_decay: 0.0500 (0.0500)  time: 0.3566  data: 0.1741  max mem: 6186
Epoch: [25]  [17010/40201]  eta: 3:31:07  lr: 0.000003  min_lr: 0.000000  loss: 3.0192 (3.4191)  loss_scale: 32768.0000 (37159.9327)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.1228  max mem: 6186
Epoch: [25]  [17020/40201]  eta: 3:30:58  lr: 0.000003  min_lr: 0.000000  loss: 2.8906 (3.4190)  loss_scale: 32768.0000 (37157.3524)  weight_decay: 0.0500 (0.0500)  time: 0.2876  data: 0.0987  max mem: 6186
Epoch: [25]  [17030/40201]  eta: 3:30:52  lr: 0.000003  min_lr: 0.000000  loss: 3.0851 (3.4191)  loss_scale: 32768.0000 (37154.7752)  weight_decay: 0.0500 (0.0500)  time: 0.3986  data: 0.1160  max mem: 6186
Epoch: [25]  [17040/40201]  eta: 3:30:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5697 (3.4190)  loss_scale: 32768.0000 (37152.2009)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0477  max mem: 6186
Epoch: [25]  [17050/40201]  eta: 3:30:40  lr: 0.000003  min_lr: 0.000000  loss: 3.0809 (3.4187)  loss_scale: 32768.0000 (37149.6297)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0005  max mem: 6186
Epoch: [25]  [17060/40201]  eta: 3:30:34  lr: 0.000003  min_lr: 0.000000  loss: 3.0809 (3.4188)  loss_scale: 32768.0000 (37147.0615)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0005  max mem: 6186
Epoch: [25]  [17070/40201]  eta: 3:30:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3042 (3.4185)  loss_scale: 32768.0000 (37144.4963)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0005  max mem: 6186
Epoch: [25]  [17080/40201]  eta: 3:30:21  lr: 0.000003  min_lr: 0.000000  loss: 3.3031 (3.4186)  loss_scale: 32768.0000 (37141.9341)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0005  max mem: 6186
Epoch: [25]  [17090/40201]  eta: 3:30:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3684 (3.4188)  loss_scale: 32768.0000 (37139.3749)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0012  max mem: 6186
Epoch: [25]  [17100/40201]  eta: 3:30:09  lr: 0.000003  min_lr: 0.000000  loss: 3.8425 (3.4192)  loss_scale: 32768.0000 (37136.8187)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0011  max mem: 6186
Epoch: [25]  [17110/40201]  eta: 3:30:03  lr: 0.000003  min_lr: 0.000000  loss: 3.4680 (3.4192)  loss_scale: 32768.0000 (37134.2654)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0004  max mem: 6186
Epoch: [25]  [17120/40201]  eta: 3:29:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4472 (3.4193)  loss_scale: 32768.0000 (37131.7152)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0004  max mem: 6186
Epoch: [25]  [17130/40201]  eta: 3:29:51  lr: 0.000003  min_lr: 0.000000  loss: 2.8331 (3.4189)  loss_scale: 32768.0000 (37129.1679)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0004  max mem: 6186
Epoch: [25]  [17140/40201]  eta: 3:29:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2421 (3.4191)  loss_scale: 32768.0000 (37126.6237)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0004  max mem: 6186
Epoch: [25]  [17150/40201]  eta: 3:29:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5921 (3.4191)  loss_scale: 32768.0000 (37124.0823)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0004  max mem: 6186
Epoch: [25]  [17160/40201]  eta: 3:29:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4782 (3.4191)  loss_scale: 32768.0000 (37121.5440)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0012  max mem: 6186
Epoch: [25]  [17170/40201]  eta: 3:29:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7001 (3.4193)  loss_scale: 32768.0000 (37119.0086)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0021  max mem: 6186
Epoch: [25]  [17180/40201]  eta: 3:29:21  lr: 0.000003  min_lr: 0.000000  loss: 3.3300 (3.4191)  loss_scale: 32768.0000 (37116.4761)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0013  max mem: 6186
[2023-07-24 17:51:21,948] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:51:21,948] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:51:21,948] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:51:21,949] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [17190/40201]  eta: 3:29:12  lr: 0.000003  min_lr: 0.000000  loss: 2.9190 (3.4187)  loss_scale: 32768.0000 (37121.5711)  weight_decay: 0.0500 (0.0500)  time: 0.3816  data: 0.0005  max mem: 6186
Epoch: [25]  [17200/40201]  eta: 3:29:02  lr: 0.000003  min_lr: 0.000000  loss: 3.0585 (3.4190)  loss_scale: 65536.0000 (37138.0901)  weight_decay: 0.0500 (0.0500)  time: 0.2211  data: 0.0005  max mem: 6186
Epoch: [25]  [17210/40201]  eta: 3:28:52  lr: 0.000003  min_lr: 0.000000  loss: 3.4652 (3.4190)  loss_scale: 65536.0000 (37154.5900)  weight_decay: 0.0500 (0.0500)  time: 0.2222  data: 0.0130  max mem: 6186
Epoch: [25]  [17220/40201]  eta: 3:28:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1246 (3.4188)  loss_scale: 65536.0000 (37171.0707)  weight_decay: 0.0500 (0.0500)  time: 0.2797  data: 0.0319  max mem: 6186
Epoch: [25]  [17230/40201]  eta: 3:28:41  lr: 0.000003  min_lr: 0.000000  loss: 2.9366 (3.4186)  loss_scale: 65536.0000 (37187.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.3130  max mem: 6186
Epoch: [25]  [17240/40201]  eta: 3:28:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0947 (3.4185)  loss_scale: 65536.0000 (37203.9747)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.3449  max mem: 6186
Epoch: [25]  [17250/40201]  eta: 3:28:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3956 (3.4185)  loss_scale: 65536.0000 (37220.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0513  max mem: 6186
Epoch: [25]  [17260/40201]  eta: 3:28:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2645 (3.4185)  loss_scale: 65536.0000 (37236.8025)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0013  max mem: 6186
Epoch: [25]  [17270/40201]  eta: 3:28:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1227 (3.4184)  loss_scale: 65536.0000 (37253.1879)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0013  max mem: 6186
[2023-07-24 17:52:00,120] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8636
[2023-07-24 17:52:00,120] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:52:00,127] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8636
[2023-07-24 17:52:00,127] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:52:00,127] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [17280/40201]  eta: 3:28:12  lr: 0.000003  min_lr: 0.000000  loss: 2.8771 (3.4184)  loss_scale: 65536.0000 (37254.3848)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0004  max mem: 6186
Epoch: [25]  [17290/40201]  eta: 3:28:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3315 (3.4182)  loss_scale: 32768.0000 (37251.7902)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0004  max mem: 6186
Epoch: [25]  [17300/40201]  eta: 3:28:00  lr: 0.000003  min_lr: 0.000000  loss: 3.0934 (3.4182)  loss_scale: 32768.0000 (37249.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0004  max mem: 6186
Epoch: [25]  [17310/40201]  eta: 3:27:54  lr: 0.000003  min_lr: 0.000000  loss: 3.5697 (3.4184)  loss_scale: 32768.0000 (37246.6099)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0004  max mem: 6186
Epoch: [25]  [17320/40201]  eta: 3:27:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4536 (3.4184)  loss_scale: 32768.0000 (37244.0242)  weight_decay: 0.0500 (0.0500)  time: 0.5003  data: 0.0004  max mem: 6186
Epoch: [25]  [17330/40201]  eta: 3:27:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4171 (3.4185)  loss_scale: 32768.0000 (37241.4416)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0004  max mem: 6186
Epoch: [25]  [17340/40201]  eta: 3:27:36  lr: 0.000003  min_lr: 0.000000  loss: 3.3646 (3.4185)  loss_scale: 32768.0000 (37238.8619)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0004  max mem: 6186
Epoch: [25]  [17350/40201]  eta: 3:27:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3031 (3.4184)  loss_scale: 32768.0000 (37236.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0004  max mem: 6186
Epoch: [25]  [17360/40201]  eta: 3:27:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1722 (3.4183)  loss_scale: 32768.0000 (37233.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0004  max mem: 6186
Epoch: [25]  [17370/40201]  eta: 3:27:18  lr: 0.000003  min_lr: 0.000000  loss: 3.1858 (3.4184)  loss_scale: 32768.0000 (37231.1406)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0005  max mem: 6186
Epoch: [25]  [17380/40201]  eta: 3:27:13  lr: 0.000003  min_lr: 0.000000  loss: 3.1858 (3.4180)  loss_scale: 32768.0000 (37228.5728)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0004  max mem: 6186
Epoch: [25]  [17390/40201]  eta: 3:27:07  lr: 0.000003  min_lr: 0.000000  loss: 3.1443 (3.4180)  loss_scale: 32768.0000 (37226.0079)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0004  max mem: 6186
Epoch: [25]  [17400/40201]  eta: 3:27:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4578 (3.4181)  loss_scale: 32768.0000 (37223.4460)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0005  max mem: 6186
Epoch: [25]  [17410/40201]  eta: 3:26:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2946 (3.4179)  loss_scale: 32768.0000 (37220.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0004  max mem: 6186
Epoch: [25]  [17420/40201]  eta: 3:26:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3616 (3.4181)  loss_scale: 32768.0000 (37218.3310)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0004  max mem: 6186
Epoch: [25]  [17430/40201]  eta: 3:26:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6645 (3.4180)  loss_scale: 32768.0000 (37215.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [17440/40201]  eta: 3:26:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2541 (3.4181)  loss_scale: 32768.0000 (37213.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0004  max mem: 6186
Epoch: [25]  [17450/40201]  eta: 3:26:31  lr: 0.000003  min_lr: 0.000000  loss: 3.3903 (3.4183)  loss_scale: 32768.0000 (37210.6804)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0004  max mem: 6186
Epoch: [25]  [17460/40201]  eta: 3:26:25  lr: 0.000003  min_lr: 0.000000  loss: 3.8139 (3.4184)  loss_scale: 32768.0000 (37208.1361)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0009  max mem: 6186
Epoch: [25]  [17470/40201]  eta: 3:26:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6016 (3.4185)  loss_scale: 32768.0000 (37205.5946)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0009  max mem: 6186
Epoch: [25]  [17480/40201]  eta: 3:26:13  lr: 0.000003  min_lr: 0.000000  loss: 3.4877 (3.4186)  loss_scale: 32768.0000 (37203.0561)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0005  max mem: 6186
Epoch: [25]  [17490/40201]  eta: 3:26:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5330 (3.4187)  loss_scale: 32768.0000 (37200.5205)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0004  max mem: 6186
Epoch: [25]  [17500/40201]  eta: 3:26:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4645 (3.4188)  loss_scale: 32768.0000 (37197.9878)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0004  max mem: 6186
Epoch: [25]  [17510/40201]  eta: 3:25:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5772 (3.4190)  loss_scale: 32768.0000 (37195.4579)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
Epoch: [25]  [17520/40201]  eta: 3:25:50  lr: 0.000003  min_lr: 0.000000  loss: 3.8253 (3.4193)  loss_scale: 32768.0000 (37192.9310)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0004  max mem: 6186
Epoch: [25]  [17530/40201]  eta: 3:25:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6210 (3.4193)  loss_scale: 32768.0000 (37190.4069)  weight_decay: 0.0500 (0.0500)  time: 0.4353  data: 0.0011  max mem: 6186
[2023-07-24 17:54:09,338] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:54:09,338] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:54:09,338] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:54:09,339] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [17540/40201]  eta: 3:25:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1003 (3.4193)  loss_scale: 32768.0000 (37206.5666)  weight_decay: 0.0500 (0.0500)  time: 0.2785  data: 0.0012  max mem: 6186
Epoch: [25]  [17550/40201]  eta: 3:25:27  lr: 0.000003  min_lr: 0.000000  loss: 3.1509 (3.4190)  loss_scale: 65536.0000 (37222.7078)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.2168  max mem: 6186
[2023-07-24 17:54:17,670] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8776
[2023-07-24 17:54:17,670] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8776
[2023-07-24 17:54:17,670] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:54:17,670] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:54:17,670] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [17560/40201]  eta: 3:25:17  lr: 0.000003  min_lr: 0.000000  loss: 2.9038 (3.4189)  loss_scale: 65536.0000 (37223.9030)  weight_decay: 0.0500 (0.0500)  time: 0.4085  data: 0.2318  max mem: 6186
Epoch: [25]  [17570/40201]  eta: 3:25:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4794 (3.4191)  loss_scale: 32768.0000 (37221.3670)  weight_decay: 0.0500 (0.0500)  time: 0.2408  data: 0.0610  max mem: 6186
Epoch: [25]  [17580/40201]  eta: 3:24:59  lr: 0.000003  min_lr: 0.000000  loss: 3.4794 (3.4193)  loss_scale: 32768.0000 (37218.8340)  weight_decay: 0.0500 (0.0500)  time: 0.2558  data: 0.0694  max mem: 6186
Epoch: [25]  [17590/40201]  eta: 3:24:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3508 (3.4193)  loss_scale: 32768.0000 (37216.3038)  weight_decay: 0.0500 (0.0500)  time: 0.3266  data: 0.1335  max mem: 6186
Epoch: [25]  [17600/40201]  eta: 3:24:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4535 (3.4195)  loss_scale: 32768.0000 (37213.7765)  weight_decay: 0.0500 (0.0500)  time: 0.4600  data: 0.1102  max mem: 6186
Epoch: [25]  [17610/40201]  eta: 3:24:40  lr: 0.000003  min_lr: 0.000000  loss: 3.4360 (3.4195)  loss_scale: 32768.0000 (37211.2521)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0005  max mem: 6186
Epoch: [25]  [17620/40201]  eta: 3:24:34  lr: 0.000003  min_lr: 0.000000  loss: 3.3858 (3.4196)  loss_scale: 32768.0000 (37208.7305)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0004  max mem: 6186
Epoch: [25]  [17630/40201]  eta: 3:24:28  lr: 0.000003  min_lr: 0.000000  loss: 3.1330 (3.4195)  loss_scale: 32768.0000 (37206.2118)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0004  max mem: 6186
Epoch: [25]  [17640/40201]  eta: 3:24:23  lr: 0.000003  min_lr: 0.000000  loss: 3.3303 (3.4196)  loss_scale: 32768.0000 (37203.6959)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0005  max mem: 6186
Epoch: [25]  [17650/40201]  eta: 3:24:17  lr: 0.000003  min_lr: 0.000000  loss: 3.0712 (3.4194)  loss_scale: 32768.0000 (37201.1829)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0018  max mem: 6186
Epoch: [25]  [17660/40201]  eta: 3:24:17  lr: 0.000003  min_lr: 0.000000  loss: 3.1593 (3.4196)  loss_scale: 32768.0000 (37198.6728)  weight_decay: 0.0500 (0.0500)  time: 0.7459  data: 0.0018  max mem: 6186
Epoch: [25]  [17670/40201]  eta: 3:24:12  lr: 0.000003  min_lr: 0.000000  loss: 3.5547 (3.4197)  loss_scale: 32768.0000 (37196.1655)  weight_decay: 0.0500 (0.0500)  time: 0.7586  data: 0.0013  max mem: 6186
Epoch: [25]  [17680/40201]  eta: 3:24:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1899 (3.4195)  loss_scale: 32768.0000 (37193.6610)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0020  max mem: 6186
Epoch: [25]  [17690/40201]  eta: 3:24:00  lr: 0.000003  min_lr: 0.000000  loss: 3.0595 (3.4194)  loss_scale: 32768.0000 (37191.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0012  max mem: 6186
Epoch: [25]  [17700/40201]  eta: 3:23:54  lr: 0.000003  min_lr: 0.000000  loss: 2.9803 (3.4193)  loss_scale: 32768.0000 (37188.6605)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0005  max mem: 6186
Epoch: [25]  [17710/40201]  eta: 3:23:48  lr: 0.000003  min_lr: 0.000000  loss: 3.0235 (3.4191)  loss_scale: 32768.0000 (37186.1645)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0010  max mem: 6186
Epoch: [25]  [17720/40201]  eta: 3:23:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3736 (3.4192)  loss_scale: 32768.0000 (37183.6714)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0010  max mem: 6186
Epoch: [25]  [17730/40201]  eta: 3:23:36  lr: 0.000003  min_lr: 0.000000  loss: 3.2636 (3.4191)  loss_scale: 32768.0000 (37181.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0005  max mem: 6186
Epoch: [25]  [17740/40201]  eta: 3:23:30  lr: 0.000003  min_lr: 0.000000  loss: 2.8011 (3.4189)  loss_scale: 32768.0000 (37178.6934)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0004  max mem: 6186
Epoch: [25]  [17750/40201]  eta: 3:23:25  lr: 0.000003  min_lr: 0.000000  loss: 3.2603 (3.4191)  loss_scale: 32768.0000 (37176.2087)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0005  max mem: 6186
Epoch: [25]  [17760/40201]  eta: 3:23:19  lr: 0.000003  min_lr: 0.000000  loss: 3.7724 (3.4194)  loss_scale: 32768.0000 (37173.7267)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0005  max mem: 6186
Epoch: [25]  [17770/40201]  eta: 3:23:14  lr: 0.000003  min_lr: 0.000000  loss: 3.5207 (3.4193)  loss_scale: 32768.0000 (37171.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0006  max mem: 6186
Epoch: [25]  [17780/40201]  eta: 3:23:08  lr: 0.000003  min_lr: 0.000000  loss: 3.0622 (3.4192)  loss_scale: 32768.0000 (37168.7712)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0005  max mem: 6186
Epoch: [25]  [17790/40201]  eta: 3:23:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2858 (3.4192)  loss_scale: 32768.0000 (37166.2976)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0004  max mem: 6186
Epoch: [25]  [17800/40201]  eta: 3:22:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3290 (3.4193)  loss_scale: 32768.0000 (37163.8268)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0004  max mem: 6186
Epoch: [25]  [17810/40201]  eta: 3:22:50  lr: 0.000003  min_lr: 0.000000  loss: 3.0935 (3.4191)  loss_scale: 32768.0000 (37161.3587)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0005  max mem: 6186
[2023-07-24 17:56:26,879] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:56:26,880] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:56:26,907] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:56:26,907] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [17820/40201]  eta: 3:22:44  lr: 0.000003  min_lr: 0.000000  loss: 3.0990 (3.4190)  loss_scale: 32768.0000 (37177.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0005  max mem: 6186
Epoch: [25]  [17830/40201]  eta: 3:22:38  lr: 0.000003  min_lr: 0.000000  loss: 3.1465 (3.4189)  loss_scale: 65536.0000 (37193.1849)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0005  max mem: 6186
Epoch: [25]  [17840/40201]  eta: 3:22:33  lr: 0.000003  min_lr: 0.000000  loss: 3.0221 (3.4187)  loss_scale: 65536.0000 (37209.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0005  max mem: 6186
Epoch: [25]  [17850/40201]  eta: 3:22:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2457 (3.4188)  loss_scale: 65536.0000 (37224.9398)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0005  max mem: 6186
Epoch: [25]  [17860/40201]  eta: 3:22:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5103 (3.4187)  loss_scale: 65536.0000 (37240.7905)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0005  max mem: 6186
Epoch: [25]  [17870/40201]  eta: 3:22:15  lr: 0.000003  min_lr: 0.000000  loss: 3.3766 (3.4188)  loss_scale: 65536.0000 (37256.6236)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0005  max mem: 6186
Epoch: [25]  [17880/40201]  eta: 3:22:09  lr: 0.000003  min_lr: 0.000000  loss: 3.1005 (3.4186)  loss_scale: 65536.0000 (37272.4389)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0005  max mem: 6186
Epoch: [25]  [17890/40201]  eta: 3:22:03  lr: 0.000003  min_lr: 0.000000  loss: 3.0828 (3.4185)  loss_scale: 65536.0000 (37288.2365)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0005  max mem: 6186
Epoch: [25]  [17900/40201]  eta: 3:21:59  lr: 0.000003  min_lr: 0.000000  loss: 3.4032 (3.4184)  loss_scale: 65536.0000 (37304.0165)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0326  max mem: 6186
Epoch: [25]  [17910/40201]  eta: 3:21:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3304 (3.4185)  loss_scale: 65536.0000 (37319.7789)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0326  max mem: 6186
Epoch: [25]  [17920/40201]  eta: 3:21:47  lr: 0.000003  min_lr: 0.000000  loss: 3.3304 (3.4185)  loss_scale: 65536.0000 (37335.5237)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0005  max mem: 6186
Epoch: [25]  [17930/40201]  eta: 3:21:41  lr: 0.000003  min_lr: 0.000000  loss: 3.0160 (3.4183)  loss_scale: 65536.0000 (37351.2509)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0009  max mem: 6186
[2023-07-24 17:57:31,160] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8967
[2023-07-24 17:57:31,161] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:57:31,167] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 8967
[2023-07-24 17:57:31,168] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:57:31,168] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [17940/40201]  eta: 3:21:35  lr: 0.000003  min_lr: 0.000000  loss: 2.8934 (3.4180)  loss_scale: 65536.0000 (37356.0020)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0010  max mem: 6186
Epoch: [25]  [17950/40201]  eta: 3:21:29  lr: 0.000003  min_lr: 0.000000  loss: 2.8259 (3.4179)  loss_scale: 32768.0000 (37353.4462)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0005  max mem: 6186
Epoch: [25]  [17960/40201]  eta: 3:21:24  lr: 0.000003  min_lr: 0.000000  loss: 2.8481 (3.4177)  loss_scale: 32768.0000 (37350.8932)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0005  max mem: 6186
Epoch: [25]  [17970/40201]  eta: 3:21:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0576 (3.4178)  loss_scale: 32768.0000 (37348.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0004  max mem: 6186
Epoch: [25]  [17980/40201]  eta: 3:21:12  lr: 0.000003  min_lr: 0.000000  loss: 3.5180 (3.4177)  loss_scale: 32768.0000 (37345.7957)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0006  max mem: 6186
Epoch: [25]  [17990/40201]  eta: 3:21:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2665 (3.4176)  loss_scale: 32768.0000 (37343.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0006  max mem: 6186
[2023-07-24 17:58:04,778] [INFO] [logging.py:69:log_dist] [Rank 0] step=9000, skipped=48, lr=[6.31318956949933e-08, 6.31318956949933e-08, 8.417586092665773e-08, 8.417586092665773e-08, 1.1223448123554365e-07, 1.1223448123554365e-07, 1.4964597498072487e-07, 1.4964597498072487e-07, 1.9952796664096647e-07, 1.9952796664096647e-07, 2.6603728885462197e-07, 2.6603728885462197e-07, 3.5471638513949596e-07, 3.5471638513949596e-07, 4.729551801859946e-07, 4.729551801859946e-07, 6.306069069146595e-07, 6.306069069146595e-07, 8.40809209219546e-07, 8.40809209219546e-07, 1.1210789456260612e-06, 1.1210789456260612e-06, 1.4947719275014151e-06, 1.4947719275014151e-06, 1.9930292366685533e-06, 1.9930292366685533e-06, 2.6573723155580713e-06, 2.6573723155580713e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 17:58:04,782] [INFO] [timer.py:181:stop] 0/18000, SamplesPerSec=13.11452082894301
Epoch: [25]  [18000/40201]  eta: 3:21:01  lr: 0.000003  min_lr: 0.000000  loss: 3.2665 (3.4178)  loss_scale: 32768.0000 (37340.7095)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0012  max mem: 6186
Epoch: [25]  [18010/40201]  eta: 3:20:55  lr: 0.000003  min_lr: 0.000000  loss: 3.8457 (3.4180)  loss_scale: 32768.0000 (37338.1707)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0011  max mem: 6186
Epoch: [25]  [18020/40201]  eta: 3:20:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5421 (3.4180)  loss_scale: 32768.0000 (37335.6346)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0005  max mem: 6186
Epoch: [25]  [18030/40201]  eta: 3:20:44  lr: 0.000003  min_lr: 0.000000  loss: 3.0368 (3.4179)  loss_scale: 32768.0000 (37333.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0006  max mem: 6186
Epoch: [25]  [18040/40201]  eta: 3:20:38  lr: 0.000003  min_lr: 0.000000  loss: 3.1395 (3.4178)  loss_scale: 32768.0000 (37330.5710)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0005  max mem: 6186
Epoch: [25]  [18050/40201]  eta: 3:20:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6447 (3.4180)  loss_scale: 32768.0000 (37328.0434)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0004  max mem: 6186
Epoch: [25]  [18060/40201]  eta: 3:20:27  lr: 0.000003  min_lr: 0.000000  loss: 3.6447 (3.4177)  loss_scale: 32768.0000 (37325.5186)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0005  max mem: 6186
Epoch: [25]  [18070/40201]  eta: 3:20:21  lr: 0.000003  min_lr: 0.000000  loss: 3.0765 (3.4178)  loss_scale: 32768.0000 (37322.9966)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0005  max mem: 6186
Epoch: [25]  [18080/40201]  eta: 3:20:15  lr: 0.000003  min_lr: 0.000000  loss: 3.5714 (3.4179)  loss_scale: 32768.0000 (37320.4774)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0005  max mem: 6186
Epoch: [25]  [18090/40201]  eta: 3:20:10  lr: 0.000003  min_lr: 0.000000  loss: 3.5451 (3.4180)  loss_scale: 32768.0000 (37317.9610)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0005  max mem: 6186
Epoch: [25]  [18100/40201]  eta: 3:20:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3739 (3.4180)  loss_scale: 32768.0000 (37315.4473)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0005  max mem: 6186
Epoch: [25]  [18110/40201]  eta: 3:19:58  lr: 0.000003  min_lr: 0.000000  loss: 3.6644 (3.4181)  loss_scale: 32768.0000 (37312.9364)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0005  max mem: 6186
Epoch: [25]  [18120/40201]  eta: 3:19:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3572 (3.4178)  loss_scale: 32768.0000 (37310.4283)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0005  max mem: 6186
Epoch: [25]  [18130/40201]  eta: 3:19:46  lr: 0.000003  min_lr: 0.000000  loss: 2.9990 (3.4178)  loss_scale: 32768.0000 (37307.9230)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [18140/40201]  eta: 3:19:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3395 (3.4178)  loss_scale: 32768.0000 (37305.4204)  weight_decay: 0.0500 (0.0500)  time: 0.4534  data: 0.0005  max mem: 6186
Epoch: [25]  [18150/40201]  eta: 3:19:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4000 (3.4178)  loss_scale: 32768.0000 (37302.9206)  weight_decay: 0.0500 (0.0500)  time: 0.3035  data: 0.0005  max mem: 6186
Epoch: [25]  [18160/40201]  eta: 3:19:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4000 (3.4178)  loss_scale: 32768.0000 (37300.4235)  weight_decay: 0.0500 (0.0500)  time: 0.2279  data: 0.0057  max mem: 6186
Epoch: [25]  [18170/40201]  eta: 3:19:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6206 (3.4179)  loss_scale: 32768.0000 (37297.9292)  weight_decay: 0.0500 (0.0500)  time: 0.4427  data: 0.2140  max mem: 6186
Epoch: [25]  [18180/40201]  eta: 3:19:08  lr: 0.000003  min_lr: 0.000000  loss: 3.3243 (3.4178)  loss_scale: 32768.0000 (37295.4377)  weight_decay: 0.0500 (0.0500)  time: 0.4487  data: 0.2474  max mem: 6186
Epoch: [25]  [18190/40201]  eta: 3:19:00  lr: 0.000003  min_lr: 0.000000  loss: 3.3243 (3.4178)  loss_scale: 32768.0000 (37292.9488)  weight_decay: 0.0500 (0.0500)  time: 0.2957  data: 0.0940  max mem: 6186
[2023-07-24 17:59:33,701] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:59:33,701] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 17:59:33,702] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 17:59:33,702] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [18200/40201]  eta: 3:18:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2715 (3.4178)  loss_scale: 32768.0000 (37304.8654)  weight_decay: 0.0500 (0.0500)  time: 0.4594  data: 0.1111  max mem: 6186
Epoch: [25]  [18210/40201]  eta: 3:18:49  lr: 0.000003  min_lr: 0.000000  loss: 2.9624 (3.4176)  loss_scale: 65536.0000 (37320.3677)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0561  max mem: 6186
Epoch: [25]  [18220/40201]  eta: 3:18:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1456 (3.4176)  loss_scale: 65536.0000 (37335.8529)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0004  max mem: 6186
[2023-07-24 17:59:53,474] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9112
[2023-07-24 17:59:53,474] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 17:59:53,474] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 17:59:53,476] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9112
[2023-07-24 17:59:53,477] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [18230/40201]  eta: 3:18:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5747 (3.4176)  loss_scale: 65536.0000 (37340.5369)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0004  max mem: 6186
Epoch: [25]  [18240/40201]  eta: 3:18:33  lr: 0.000003  min_lr: 0.000000  loss: 3.8441 (3.4177)  loss_scale: 32768.0000 (37338.0302)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0011  max mem: 6186
Epoch: [25]  [18250/40201]  eta: 3:18:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7609 (3.4177)  loss_scale: 32768.0000 (37335.5262)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0011  max mem: 6186
Epoch: [25]  [18260/40201]  eta: 3:18:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5075 (3.4178)  loss_scale: 32768.0000 (37333.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0012  max mem: 6186
Epoch: [25]  [18270/40201]  eta: 3:18:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5075 (3.4180)  loss_scale: 32768.0000 (37330.5264)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0012  max mem: 6186
Epoch: [25]  [18280/40201]  eta: 3:18:10  lr: 0.000003  min_lr: 0.000000  loss: 3.6601 (3.4180)  loss_scale: 32768.0000 (37328.0306)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0004  max mem: 6186
Epoch: [25]  [18290/40201]  eta: 3:18:04  lr: 0.000003  min_lr: 0.000000  loss: 3.7705 (3.4181)  loss_scale: 32768.0000 (37325.5376)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0004  max mem: 6186
Epoch: [25]  [18300/40201]  eta: 3:17:58  lr: 0.000003  min_lr: 0.000000  loss: 3.0966 (3.4180)  loss_scale: 32768.0000 (37323.0473)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0005  max mem: 6186
Epoch: [25]  [18310/40201]  eta: 3:17:52  lr: 0.000003  min_lr: 0.000000  loss: 3.0991 (3.4182)  loss_scale: 32768.0000 (37320.5597)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0005  max mem: 6186
Epoch: [25]  [18320/40201]  eta: 3:17:47  lr: 0.000003  min_lr: 0.000000  loss: 3.7906 (3.4183)  loss_scale: 32768.0000 (37318.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0005  max mem: 6186
Epoch: [25]  [18330/40201]  eta: 3:17:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3346 (3.4182)  loss_scale: 32768.0000 (37315.5926)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0011  max mem: 6186
Epoch: [25]  [18340/40201]  eta: 3:17:35  lr: 0.000003  min_lr: 0.000000  loss: 3.1932 (3.4182)  loss_scale: 32768.0000 (37313.1131)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0011  max mem: 6186
Epoch: [25]  [18350/40201]  eta: 3:17:30  lr: 0.000003  min_lr: 0.000000  loss: 3.2865 (3.4182)  loss_scale: 32768.0000 (37310.6364)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0012  max mem: 6186
Epoch: [25]  [18360/40201]  eta: 3:17:24  lr: 0.000003  min_lr: 0.000000  loss: 3.3129 (3.4181)  loss_scale: 32768.0000 (37308.1623)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0012  max mem: 6186
Epoch: [25]  [18370/40201]  eta: 3:17:19  lr: 0.000003  min_lr: 0.000000  loss: 3.0981 (3.4180)  loss_scale: 32768.0000 (37305.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0004  max mem: 6186
Epoch: [25]  [18380/40201]  eta: 3:17:13  lr: 0.000003  min_lr: 0.000000  loss: 3.0981 (3.4179)  loss_scale: 32768.0000 (37303.2222)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0004  max mem: 6186
Epoch: [25]  [18390/40201]  eta: 3:17:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5616 (3.4180)  loss_scale: 32768.0000 (37300.7562)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0012  max mem: 6186
Epoch: [25]  [18400/40201]  eta: 3:17:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5992 (3.4181)  loss_scale: 32768.0000 (37298.2929)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0012  max mem: 6186
Epoch: [25]  [18410/40201]  eta: 3:16:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5236 (3.4182)  loss_scale: 32768.0000 (37295.8323)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0005  max mem: 6186
Epoch: [25]  [18420/40201]  eta: 3:16:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2324 (3.4181)  loss_scale: 32768.0000 (37293.3743)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0012  max mem: 6186
Epoch: [25]  [18430/40201]  eta: 3:16:44  lr: 0.000003  min_lr: 0.000000  loss: 3.2111 (3.4180)  loss_scale: 32768.0000 (37290.9190)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0012  max mem: 6186
Epoch: [25]  [18440/40201]  eta: 3:16:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4784 (3.4181)  loss_scale: 32768.0000 (37288.4664)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0005  max mem: 6186
Epoch: [25]  [18450/40201]  eta: 3:16:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2383 (3.4179)  loss_scale: 32768.0000 (37286.0164)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0005  max mem: 6186
Epoch: [25]  [18460/40201]  eta: 3:16:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1714 (3.4177)  loss_scale: 32768.0000 (37283.5690)  weight_decay: 0.0500 (0.0500)  time: 0.3909  data: 0.0005  max mem: 6186
Epoch: [25]  [18470/40201]  eta: 3:16:16  lr: 0.000003  min_lr: 0.000000  loss: 3.2102 (3.4176)  loss_scale: 32768.0000 (37281.1244)  weight_decay: 0.0500 (0.0500)  time: 0.2968  data: 0.0005  max mem: 6186
Epoch: [25]  [18480/40201]  eta: 3:16:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5565 (3.4179)  loss_scale: 32768.0000 (37278.6823)  weight_decay: 0.0500 (0.0500)  time: 0.2791  data: 0.0005  max mem: 6186
[2023-07-24 18:01:59,121] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:01:59,121] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:01:59,121] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:01:59,121] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [18490/40201]  eta: 3:15:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8147 (3.4179)  loss_scale: 32768.0000 (37290.4198)  weight_decay: 0.0500 (0.0500)  time: 0.2928  data: 0.0735  max mem: 6186
Epoch: [25]  [18500/40201]  eta: 3:15:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3892 (3.4178)  loss_scale: 65536.0000 (37305.6868)  weight_decay: 0.0500 (0.0500)  time: 0.3333  data: 0.1436  max mem: 6186
Epoch: [25]  [18510/40201]  eta: 3:15:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3328 (3.4178)  loss_scale: 65536.0000 (37320.9374)  weight_decay: 0.0500 (0.0500)  time: 0.3587  data: 0.1733  max mem: 6186
Epoch: [25]  [18520/40201]  eta: 3:15:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7621 (3.4180)  loss_scale: 65536.0000 (37336.1715)  weight_decay: 0.0500 (0.0500)  time: 0.4216  data: 0.1386  max mem: 6186
Epoch: [25]  [18530/40201]  eta: 3:15:32  lr: 0.000003  min_lr: 0.000000  loss: 3.8671 (3.4180)  loss_scale: 65536.0000 (37351.3891)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0358  max mem: 6186
Epoch: [25]  [18540/40201]  eta: 3:15:26  lr: 0.000003  min_lr: 0.000000  loss: 3.3649 (3.4180)  loss_scale: 65536.0000 (37366.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0004  max mem: 6186
Epoch: [25]  [18550/40201]  eta: 3:15:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5150 (3.4182)  loss_scale: 65536.0000 (37381.7752)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0004  max mem: 6186
Epoch: [25]  [18560/40201]  eta: 3:15:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4778 (3.4182)  loss_scale: 65536.0000 (37396.9437)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0004  max mem: 6186
Epoch: [25]  [18570/40201]  eta: 3:15:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2766 (3.4180)  loss_scale: 65536.0000 (37412.0958)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0004  max mem: 6186
Epoch: [25]  [18580/40201]  eta: 3:15:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3028 (3.4182)  loss_scale: 65536.0000 (37427.2317)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0005  max mem: 6186
Epoch: [25]  [18590/40201]  eta: 3:14:57  lr: 0.000003  min_lr: 0.000000  loss: 3.6414 (3.4183)  loss_scale: 65536.0000 (37442.3512)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0005  max mem: 6186
Epoch: [25]  [18600/40201]  eta: 3:14:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6154 (3.4184)  loss_scale: 65536.0000 (37457.4545)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0004  max mem: 6186
Epoch: [25]  [18610/40201]  eta: 3:14:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6013 (3.4185)  loss_scale: 65536.0000 (37472.5416)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0004  max mem: 6186
Epoch: [25]  [18620/40201]  eta: 3:14:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5232 (3.4187)  loss_scale: 65536.0000 (37487.6125)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0004  max mem: 6186
Epoch: [25]  [18630/40201]  eta: 3:14:34  lr: 0.000003  min_lr: 0.000000  loss: 2.8044 (3.4184)  loss_scale: 65536.0000 (37502.6672)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0004  max mem: 6186
Epoch: [25]  [18640/40201]  eta: 3:14:28  lr: 0.000003  min_lr: 0.000000  loss: 2.7800 (3.4182)  loss_scale: 65536.0000 (37517.7057)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0004  max mem: 6186
Epoch: [25]  [18650/40201]  eta: 3:14:22  lr: 0.000003  min_lr: 0.000000  loss: 3.0735 (3.4182)  loss_scale: 65536.0000 (37532.7281)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0014  max mem: 6186
Epoch: [25]  [18660/40201]  eta: 3:14:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3597 (3.4183)  loss_scale: 65536.0000 (37547.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0014  max mem: 6186
Epoch: [25]  [18670/40201]  eta: 3:14:10  lr: 0.000003  min_lr: 0.000000  loss: 3.4244 (3.4186)  loss_scale: 65536.0000 (37562.7247)  weight_decay: 0.0500 (0.0500)  time: 0.4919  data: 0.0008  max mem: 6186
Epoch: [25]  [18680/40201]  eta: 3:14:05  lr: 0.000003  min_lr: 0.000000  loss: 3.3978 (3.4187)  loss_scale: 65536.0000 (37577.6988)  weight_decay: 0.0500 (0.0500)  time: 0.5003  data: 0.0008  max mem: 6186
Epoch: [25]  [18690/40201]  eta: 3:13:59  lr: 0.000003  min_lr: 0.000000  loss: 3.3404 (3.4187)  loss_scale: 65536.0000 (37592.6570)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0004  max mem: 6186
Epoch: [25]  [18700/40201]  eta: 3:13:53  lr: 0.000003  min_lr: 0.000000  loss: 3.3481 (3.4187)  loss_scale: 65536.0000 (37607.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
[2023-07-24 18:03:47,069] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9353
[2023-07-24 18:03:47,069] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:03:47,079] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9353
[2023-07-24 18:03:47,079] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:03:47,079] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [18710/40201]  eta: 3:13:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4944 (3.4187)  loss_scale: 65536.0000 (37615.5203)  weight_decay: 0.0500 (0.0500)  time: 0.4758  data: 0.0004  max mem: 6186
Epoch: [25]  [18720/40201]  eta: 3:13:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2450 (3.4185)  loss_scale: 32768.0000 (37612.9309)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0004  max mem: 6186
Epoch: [25]  [18730/40201]  eta: 3:13:35  lr: 0.000003  min_lr: 0.000000  loss: 3.3640 (3.4187)  loss_scale: 32768.0000 (37610.3443)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0005  max mem: 6186
Epoch: [25]  [18740/40201]  eta: 3:13:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4559 (3.4186)  loss_scale: 32768.0000 (37607.7605)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0012  max mem: 6186
Epoch: [25]  [18750/40201]  eta: 3:13:23  lr: 0.000003  min_lr: 0.000000  loss: 3.2373 (3.4185)  loss_scale: 32768.0000 (37605.1795)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0025  max mem: 6186
Epoch: [25]  [18760/40201]  eta: 3:13:17  lr: 0.000003  min_lr: 0.000000  loss: 3.2890 (3.4185)  loss_scale: 32768.0000 (37602.6011)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0022  max mem: 6186
Epoch: [25]  [18770/40201]  eta: 3:13:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2840 (3.4186)  loss_scale: 32768.0000 (37600.0256)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0009  max mem: 6186
Epoch: [25]  [18780/40201]  eta: 3:13:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1634 (3.4186)  loss_scale: 32768.0000 (37597.4527)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0004  max mem: 6186
Epoch: [25]  [18790/40201]  eta: 3:13:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5172 (3.4187)  loss_scale: 32768.0000 (37594.8827)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0004  max mem: 6186
Epoch: [25]  [18800/40201]  eta: 3:12:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6641 (3.4187)  loss_scale: 32768.0000 (37592.3153)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0004  max mem: 6186
Epoch: [25]  [18810/40201]  eta: 3:12:48  lr: 0.000003  min_lr: 0.000000  loss: 3.5515 (3.4189)  loss_scale: 32768.0000 (37589.7507)  weight_decay: 0.0500 (0.0500)  time: 0.4930  data: 0.0004  max mem: 6186
Epoch: [25]  [18820/40201]  eta: 3:12:42  lr: 0.000003  min_lr: 0.000000  loss: 3.1048 (3.4187)  loss_scale: 32768.0000 (37587.1888)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0004  max mem: 6186
Epoch: [25]  [18830/40201]  eta: 3:12:36  lr: 0.000003  min_lr: 0.000000  loss: 3.1048 (3.4187)  loss_scale: 32768.0000 (37584.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0004  max mem: 6186
Epoch: [25]  [18840/40201]  eta: 3:12:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4111 (3.4187)  loss_scale: 32768.0000 (37582.0731)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0004  max mem: 6186
Epoch: [25]  [18850/40201]  eta: 3:12:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0496 (3.4183)  loss_scale: 32768.0000 (37579.5194)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0012  max mem: 6186
Epoch: [25]  [18860/40201]  eta: 3:12:19  lr: 0.000003  min_lr: 0.000000  loss: 3.0496 (3.4184)  loss_scale: 32768.0000 (37576.9683)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0019  max mem: 6186
Epoch: [25]  [18870/40201]  eta: 3:12:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5041 (3.4186)  loss_scale: 32768.0000 (37574.4200)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0011  max mem: 6186
Epoch: [25]  [18880/40201]  eta: 3:12:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6597 (3.4187)  loss_scale: 32768.0000 (37571.8744)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0004  max mem: 6186
Epoch: [25]  [18890/40201]  eta: 3:12:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6597 (3.4190)  loss_scale: 32768.0000 (37569.3314)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0004  max mem: 6186
Epoch: [25]  [18900/40201]  eta: 3:11:55  lr: 0.000003  min_lr: 0.000000  loss: 3.9179 (3.4192)  loss_scale: 32768.0000 (37566.7912)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [18910/40201]  eta: 3:11:49  lr: 0.000003  min_lr: 0.000000  loss: 3.1464 (3.4190)  loss_scale: 32768.0000 (37564.2536)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0004  max mem: 6186
Epoch: [25]  [18920/40201]  eta: 3:11:43  lr: 0.000003  min_lr: 0.000000  loss: 2.6053 (3.4186)  loss_scale: 32768.0000 (37561.7187)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0012  max mem: 6186
Epoch: [25]  [18930/40201]  eta: 3:11:38  lr: 0.000003  min_lr: 0.000000  loss: 3.0521 (3.4187)  loss_scale: 32768.0000 (37559.1865)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0012  max mem: 6186
Epoch: [25]  [18940/40201]  eta: 3:11:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6598 (3.4188)  loss_scale: 32768.0000 (37556.6570)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0005  max mem: 6186
Epoch: [25]  [18950/40201]  eta: 3:11:26  lr: 0.000003  min_lr: 0.000000  loss: 3.0353 (3.4188)  loss_scale: 32768.0000 (37554.1301)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0005  max mem: 6186
Epoch: [25]  [18960/40201]  eta: 3:11:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2024 (3.4186)  loss_scale: 32768.0000 (37551.6059)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0005  max mem: 6186
[2023-07-24 18:05:56,457] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:05:56,457] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:05:56,461] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:05:56,462] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [18970/40201]  eta: 3:11:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2654 (3.4188)  loss_scale: 32768.0000 (37559.4480)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0004  max mem: 6186
Epoch: [25]  [18980/40201]  eta: 3:11:09  lr: 0.000003  min_lr: 0.000000  loss: 3.2841 (3.4188)  loss_scale: 65536.0000 (37574.1872)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0004  max mem: 6186
Epoch: [25]  [18990/40201]  eta: 3:11:03  lr: 0.000003  min_lr: 0.000000  loss: 2.8413 (3.4185)  loss_scale: 65536.0000 (37588.9110)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0011  max mem: 6186
[2023-07-24 18:06:13,787] [INFO] [timer.py:181:stop] 0/19000, SamplesPerSec=13.14931922605194
Epoch: [25]  [19000/40201]  eta: 3:10:57  lr: 0.000003  min_lr: 0.000000  loss: 2.9650 (3.4185)  loss_scale: 65536.0000 (37603.6192)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0011  max mem: 6186
Epoch: [25]  [19010/40201]  eta: 3:10:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7543 (3.4186)  loss_scale: 65536.0000 (37618.3119)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0004  max mem: 6186
Epoch: [25]  [19020/40201]  eta: 3:10:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6522 (3.4185)  loss_scale: 65536.0000 (37632.9892)  weight_decay: 0.0500 (0.0500)  time: 0.4126  data: 0.0004  max mem: 6186
Epoch: [25]  [19030/40201]  eta: 3:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5795 (3.4186)  loss_scale: 65536.0000 (37647.6511)  weight_decay: 0.0500 (0.0500)  time: 0.2557  data: 0.0012  max mem: 6186
Epoch: [25]  [19040/40201]  eta: 3:10:26  lr: 0.000003  min_lr: 0.000000  loss: 3.4272 (3.4186)  loss_scale: 65536.0000 (37662.2976)  weight_decay: 0.0500 (0.0500)  time: 0.2391  data: 0.0100  max mem: 6186
Epoch: [25]  [19050/40201]  eta: 3:10:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4272 (3.4187)  loss_scale: 65536.0000 (37676.9287)  weight_decay: 0.0500 (0.0500)  time: 0.2878  data: 0.0100  max mem: 6186
Epoch: [25]  [19060/40201]  eta: 3:10:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6727 (3.4187)  loss_scale: 65536.0000 (37691.5444)  weight_decay: 0.0500 (0.0500)  time: 0.2668  data: 0.0094  max mem: 6186
Epoch: [25]  [19070/40201]  eta: 3:10:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5961 (3.4189)  loss_scale: 65536.0000 (37706.1448)  weight_decay: 0.0500 (0.0500)  time: 0.2895  data: 0.0106  max mem: 6186
Epoch: [25]  [19080/40201]  eta: 3:09:54  lr: 0.000003  min_lr: 0.000000  loss: 3.2460 (3.4188)  loss_scale: 65536.0000 (37720.7299)  weight_decay: 0.0500 (0.0500)  time: 0.3349  data: 0.0245  max mem: 6186
Epoch: [25]  [19090/40201]  eta: 3:09:46  lr: 0.000003  min_lr: 0.000000  loss: 2.9897 (3.4185)  loss_scale: 65536.0000 (37735.2998)  weight_decay: 0.0500 (0.0500)  time: 0.3487  data: 0.0305  max mem: 6186
Epoch: [25]  [19100/40201]  eta: 3:09:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4931 (3.4188)  loss_scale: 65536.0000 (37749.8544)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0084  max mem: 6186
[2023-07-24 18:06:52,292] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9553
[2023-07-24 18:06:52,292] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:06:52,292] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:06:52,298] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9553
[2023-07-24 18:06:52,298] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [19110/40201]  eta: 3:09:35  lr: 0.000003  min_lr: 0.000000  loss: 3.7976 (3.4190)  loss_scale: 65536.0000 (37757.5352)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0004  max mem: 6186
Epoch: [25]  [19120/40201]  eta: 3:09:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4936 (3.4189)  loss_scale: 32768.0000 (37754.9258)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0004  max mem: 6186
Epoch: [25]  [19130/40201]  eta: 3:09:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1448 (3.4189)  loss_scale: 32768.0000 (37752.3191)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0004  max mem: 6186
Epoch: [25]  [19140/40201]  eta: 3:09:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3289 (3.4188)  loss_scale: 32768.0000 (37749.7151)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0004  max mem: 6186
Epoch: [25]  [19150/40201]  eta: 3:09:12  lr: 0.000003  min_lr: 0.000000  loss: 3.1610 (3.4186)  loss_scale: 32768.0000 (37747.1138)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0004  max mem: 6186
Epoch: [25]  [19160/40201]  eta: 3:09:06  lr: 0.000003  min_lr: 0.000000  loss: 3.1135 (3.4187)  loss_scale: 32768.0000 (37744.5152)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0004  max mem: 6186
Epoch: [25]  [19170/40201]  eta: 3:09:00  lr: 0.000003  min_lr: 0.000000  loss: 3.4886 (3.4188)  loss_scale: 32768.0000 (37741.9194)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0004  max mem: 6186
Epoch: [25]  [19180/40201]  eta: 3:08:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7834 (3.4190)  loss_scale: 32768.0000 (37739.3262)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0004  max mem: 6186
Epoch: [25]  [19190/40201]  eta: 3:08:49  lr: 0.000003  min_lr: 0.000000  loss: 3.1552 (3.4189)  loss_scale: 32768.0000 (37736.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0005  max mem: 6186
Epoch: [25]  [19200/40201]  eta: 3:08:43  lr: 0.000003  min_lr: 0.000000  loss: 3.2101 (3.4190)  loss_scale: 32768.0000 (37734.1480)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0004  max mem: 6186
Epoch: [25]  [19210/40201]  eta: 3:08:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5936 (3.4192)  loss_scale: 32768.0000 (37731.5630)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0004  max mem: 6186
Epoch: [25]  [19220/40201]  eta: 3:08:31  lr: 0.000003  min_lr: 0.000000  loss: 3.7370 (3.4195)  loss_scale: 32768.0000 (37728.9806)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0004  max mem: 6186
Epoch: [25]  [19230/40201]  eta: 3:08:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5616 (3.4195)  loss_scale: 32768.0000 (37726.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0013  max mem: 6186
Epoch: [25]  [19240/40201]  eta: 3:08:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2677 (3.4195)  loss_scale: 32768.0000 (37723.8239)  weight_decay: 0.0500 (0.0500)  time: 0.4947  data: 0.0013  max mem: 6186
Epoch: [25]  [19250/40201]  eta: 3:08:14  lr: 0.000003  min_lr: 0.000000  loss: 3.6982 (3.4196)  loss_scale: 32768.0000 (37721.2496)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0004  max mem: 6186
Epoch: [25]  [19260/40201]  eta: 3:08:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6982 (3.4196)  loss_scale: 32768.0000 (37718.6780)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0004  max mem: 6186
Epoch: [25]  [19270/40201]  eta: 3:08:03  lr: 0.000003  min_lr: 0.000000  loss: 3.4607 (3.4197)  loss_scale: 32768.0000 (37716.1090)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0004  max mem: 6186
Epoch: [25]  [19280/40201]  eta: 3:07:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3548 (3.4197)  loss_scale: 32768.0000 (37713.5427)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0004  max mem: 6186
Epoch: [25]  [19290/40201]  eta: 3:07:51  lr: 0.000003  min_lr: 0.000000  loss: 3.2746 (3.4196)  loss_scale: 32768.0000 (37710.9790)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0004  max mem: 6186
Epoch: [25]  [19300/40201]  eta: 3:07:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1467 (3.4196)  loss_scale: 32768.0000 (37708.4180)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0004  max mem: 6186
Epoch: [25]  [19310/40201]  eta: 3:07:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2337 (3.4196)  loss_scale: 32768.0000 (37705.8597)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0004  max mem: 6186
Epoch: [25]  [19320/40201]  eta: 3:07:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2382 (3.4196)  loss_scale: 32768.0000 (37703.3040)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [19330/40201]  eta: 3:07:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3662 (3.4195)  loss_scale: 32768.0000 (37700.7509)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
Epoch: [25]  [19340/40201]  eta: 3:07:22  lr: 0.000003  min_lr: 0.000000  loss: 3.3662 (3.4194)  loss_scale: 32768.0000 (37698.2005)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0016  max mem: 6186
Epoch: [25]  [19350/40201]  eta: 3:07:16  lr: 0.000003  min_lr: 0.000000  loss: 3.4785 (3.4195)  loss_scale: 32768.0000 (37695.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0016  max mem: 6186
Epoch: [25]  [19360/40201]  eta: 3:07:10  lr: 0.000003  min_lr: 0.000000  loss: 3.4426 (3.4195)  loss_scale: 32768.0000 (37693.1076)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0004  max mem: 6186
[2023-07-24 18:09:01,420] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:09:01,420] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:09:01,429] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:09:01,429] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [19370/40201]  eta: 3:07:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4426 (3.4197)  loss_scale: 32768.0000 (37700.7147)  weight_decay: 0.0500 (0.0500)  time: 0.4925  data: 0.0005  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/CUxsn4YXksI_000119_000129.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [19380/40201]  eta: 3:06:58  lr: 0.000003  min_lr: 0.000000  loss: 3.2189 (3.4194)  loss_scale: 65536.0000 (37715.0768)  weight_decay: 0.0500 (0.0500)  time: 0.4547  data: 0.0005  max mem: 6186
Epoch: [25]  [19390/40201]  eta: 3:06:49  lr: 0.000003  min_lr: 0.000000  loss: 3.1593 (3.4195)  loss_scale: 65536.0000 (37729.4242)  weight_decay: 0.0500 (0.0500)  time: 0.3000  data: 0.0005  max mem: 6186
Epoch: [25]  [19400/40201]  eta: 3:06:39  lr: 0.000003  min_lr: 0.000000  loss: 3.6717 (3.4195)  loss_scale: 65536.0000 (37743.7567)  weight_decay: 0.0500 (0.0500)  time: 0.1823  data: 0.0005  max mem: 6186
Epoch: [25]  [19410/40201]  eta: 3:06:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4933 (3.4196)  loss_scale: 65536.0000 (37758.0745)  weight_decay: 0.0500 (0.0500)  time: 0.2322  data: 0.0299  max mem: 6186
Epoch: [25]  [19420/40201]  eta: 3:06:24  lr: 0.000003  min_lr: 0.000000  loss: 3.3286 (3.4196)  loss_scale: 65536.0000 (37772.3775)  weight_decay: 0.0500 (0.0500)  time: 0.3143  data: 0.0955  max mem: 6186
Epoch: [25]  [19430/40201]  eta: 3:06:15  lr: 0.000003  min_lr: 0.000000  loss: 3.1620 (3.4195)  loss_scale: 65536.0000 (37786.6658)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.1081  max mem: 6186
Epoch: [25]  [19440/40201]  eta: 3:06:07  lr: 0.000003  min_lr: 0.000000  loss: 3.1536 (3.4194)  loss_scale: 65536.0000 (37800.9395)  weight_decay: 0.0500 (0.0500)  time: 0.2445  data: 0.0631  max mem: 6186
Epoch: [25]  [19450/40201]  eta: 3:06:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4056 (3.4195)  loss_scale: 65536.0000 (37815.1984)  weight_decay: 0.0500 (0.0500)  time: 0.4324  data: 0.0212  max mem: 6186
Epoch: [25]  [19460/40201]  eta: 3:05:57  lr: 0.000003  min_lr: 0.000000  loss: 3.9376 (3.4197)  loss_scale: 65536.0000 (37829.4427)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0004  max mem: 6186
Epoch: [25]  [19470/40201]  eta: 3:05:51  lr: 0.000003  min_lr: 0.000000  loss: 3.8748 (3.4198)  loss_scale: 65536.0000 (37843.6723)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0006  max mem: 6186
Epoch: [25]  [19480/40201]  eta: 3:05:45  lr: 0.000003  min_lr: 0.000000  loss: 3.6525 (3.4199)  loss_scale: 65536.0000 (37857.8874)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0007  max mem: 6186
Epoch: [25]  [19490/40201]  eta: 3:05:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3424 (3.4200)  loss_scale: 65536.0000 (37872.0878)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0007  max mem: 6186
Epoch: [25]  [19500/40201]  eta: 3:05:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2400 (3.4200)  loss_scale: 65536.0000 (37886.2737)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0007  max mem: 6186
Epoch: [25]  [19510/40201]  eta: 3:05:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3490 (3.4200)  loss_scale: 65536.0000 (37900.4451)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0004  max mem: 6186
Epoch: [25]  [19520/40201]  eta: 3:05:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5437 (3.4200)  loss_scale: 65536.0000 (37914.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0005  max mem: 6186
Epoch: [25]  [19530/40201]  eta: 3:05:16  lr: 0.000003  min_lr: 0.000000  loss: 3.0766 (3.4197)  loss_scale: 65536.0000 (37928.7443)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0005  max mem: 6186
Epoch: [25]  [19540/40201]  eta: 3:05:11  lr: 0.000003  min_lr: 0.000000  loss: 3.1490 (3.4197)  loss_scale: 65536.0000 (37942.8721)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0005  max mem: 6186
[2023-07-24 18:10:17,779] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9773
[2023-07-24 18:10:17,779] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9773
[2023-07-24 18:10:17,780] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:10:17,780] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:10:17,780] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [19550/40201]  eta: 3:05:04  lr: 0.000003  min_lr: 0.000000  loss: 3.3191 (3.4197)  loss_scale: 65536.0000 (37950.2814)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0004  max mem: 6186
Epoch: [25]  [19560/40201]  eta: 3:04:59  lr: 0.000003  min_lr: 0.000000  loss: 3.6685 (3.4198)  loss_scale: 32768.0000 (37947.6321)  weight_decay: 0.0500 (0.0500)  time: 0.4762  data: 0.0004  max mem: 6186
Epoch: [25]  [19570/40201]  eta: 3:04:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4924 (3.4199)  loss_scale: 32768.0000 (37944.9855)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0004  max mem: 6186
Epoch: [25]  [19580/40201]  eta: 3:04:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1867 (3.4198)  loss_scale: 32768.0000 (37942.3417)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0004  max mem: 6186
Epoch: [25]  [19590/40201]  eta: 3:04:41  lr: 0.000003  min_lr: 0.000000  loss: 3.0633 (3.4196)  loss_scale: 32768.0000 (37939.7005)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [19600/40201]  eta: 3:04:36  lr: 0.000003  min_lr: 0.000000  loss: 3.0383 (3.4195)  loss_scale: 32768.0000 (37937.0620)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0004  max mem: 6186
Epoch: [25]  [19610/40201]  eta: 3:04:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4905 (3.4196)  loss_scale: 32768.0000 (37934.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0004  max mem: 6186
Epoch: [25]  [19620/40201]  eta: 3:04:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4842 (3.4197)  loss_scale: 32768.0000 (37931.7931)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0004  max mem: 6186
Epoch: [25]  [19630/40201]  eta: 3:04:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6190 (3.4198)  loss_scale: 32768.0000 (37929.1627)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0005  max mem: 6186
Epoch: [25]  [19640/40201]  eta: 3:04:14  lr: 0.000003  min_lr: 0.000000  loss: 3.5834 (3.4197)  loss_scale: 32768.0000 (37926.5349)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0005  max mem: 6186
Epoch: [25]  [19650/40201]  eta: 3:04:08  lr: 0.000003  min_lr: 0.000000  loss: 3.3186 (3.4196)  loss_scale: 32768.0000 (37923.9098)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0010  max mem: 6186
Epoch: [25]  [19660/40201]  eta: 3:04:02  lr: 0.000003  min_lr: 0.000000  loss: 3.8031 (3.4199)  loss_scale: 32768.0000 (37921.2874)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0010  max mem: 6186
Epoch: [25]  [19670/40201]  eta: 3:03:56  lr: 0.000003  min_lr: 0.000000  loss: 3.4123 (3.4198)  loss_scale: 32768.0000 (37918.6677)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0005  max mem: 6186
Epoch: [25]  [19680/40201]  eta: 3:03:51  lr: 0.000003  min_lr: 0.000000  loss: 3.0435 (3.4196)  loss_scale: 32768.0000 (37916.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0005  max mem: 6186
Epoch: [25]  [19690/40201]  eta: 3:03:45  lr: 0.000003  min_lr: 0.000000  loss: 3.1442 (3.4197)  loss_scale: 32768.0000 (37913.4362)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [19700/40201]  eta: 3:03:39  lr: 0.000003  min_lr: 0.000000  loss: 3.4558 (3.4198)  loss_scale: 32768.0000 (37910.8244)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0004  max mem: 6186
Epoch: [25]  [19710/40201]  eta: 3:03:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4558 (3.4200)  loss_scale: 32768.0000 (37908.2153)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0013  max mem: 6186
Epoch: [25]  [19720/40201]  eta: 3:03:28  lr: 0.000003  min_lr: 0.000000  loss: 3.7458 (3.4201)  loss_scale: 32768.0000 (37905.6088)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0013  max mem: 6186
Epoch: [25]  [19730/40201]  eta: 3:03:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4423 (3.4202)  loss_scale: 32768.0000 (37903.0050)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [19740/40201]  eta: 3:03:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1184 (3.4200)  loss_scale: 32768.0000 (37900.4038)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0004  max mem: 6186
Epoch: [25]  [19750/40201]  eta: 3:03:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1101 (3.4200)  loss_scale: 32768.0000 (37897.8053)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
Epoch: [25]  [19760/40201]  eta: 3:03:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2440 (3.4199)  loss_scale: 32768.0000 (37895.2094)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0005  max mem: 6186
Epoch: [25]  [19770/40201]  eta: 3:02:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2178 (3.4198)  loss_scale: 32768.0000 (37892.6161)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0004  max mem: 6186
Epoch: [25]  [19780/40201]  eta: 3:02:53  lr: 0.000003  min_lr: 0.000000  loss: 3.0273 (3.4193)  loss_scale: 32768.0000 (37890.0254)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0005  max mem: 6186
Epoch: [25]  [19790/40201]  eta: 3:02:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1063 (3.4192)  loss_scale: 32768.0000 (37887.4373)  weight_decay: 0.0500 (0.0500)  time: 0.4997  data: 0.0004  max mem: 6186
Epoch: [25]  [19800/40201]  eta: 3:02:42  lr: 0.000003  min_lr: 0.000000  loss: 3.2740 (3.4191)  loss_scale: 32768.0000 (37884.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
[2023-07-24 18:12:27,999] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:12:27,999] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:12:28,013] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:12:28,013] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [19810/40201]  eta: 3:02:36  lr: 0.000003  min_lr: 0.000000  loss: 3.2740 (3.4189)  loss_scale: 32768.0000 (37892.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0004  max mem: 6186
Epoch: [25]  [19820/40201]  eta: 3:02:30  lr: 0.000003  min_lr: 0.000000  loss: 2.8924 (3.4187)  loss_scale: 65536.0000 (37906.1400)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/ixQrfusr6k8_000001_000011.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [19830/40201]  eta: 3:02:24  lr: 0.000003  min_lr: 0.000000  loss: 2.7855 (3.4183)  loss_scale: 65536.0000 (37920.0726)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0004  max mem: 6186
Epoch: [25]  [19840/40201]  eta: 3:02:19  lr: 0.000003  min_lr: 0.000000  loss: 3.1268 (3.4183)  loss_scale: 65536.0000 (37933.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0004  max mem: 6186
[2023-07-24 18:12:49,751] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9924
[2023-07-24 18:12:49,751] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:12:49,755] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 9924
[2023-07-24 18:12:49,755] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:12:49,755] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [19850/40201]  eta: 3:02:12  lr: 0.000003  min_lr: 0.000000  loss: 3.5621 (3.4185)  loss_scale: 65536.0000 (37944.5944)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0012  max mem: 6186
Epoch: [25]  [19860/40201]  eta: 3:02:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4253 (3.4184)  loss_scale: 32768.0000 (37941.9880)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0011  max mem: 6186
Epoch: [25]  [19870/40201]  eta: 3:02:01  lr: 0.000003  min_lr: 0.000000  loss: 2.9763 (3.4183)  loss_scale: 32768.0000 (37939.3842)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0004  max mem: 6186
Epoch: [25]  [19880/40201]  eta: 3:01:55  lr: 0.000003  min_lr: 0.000000  loss: 3.3564 (3.4183)  loss_scale: 32768.0000 (37936.7831)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0004  max mem: 6186
Epoch: [25]  [19890/40201]  eta: 3:01:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4469 (3.4183)  loss_scale: 32768.0000 (37934.1845)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0004  max mem: 6186
Epoch: [25]  [19900/40201]  eta: 3:01:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4001 (3.4184)  loss_scale: 32768.0000 (37931.5886)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0004  max mem: 6186
Epoch: [25]  [19910/40201]  eta: 3:01:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3207 (3.4184)  loss_scale: 32768.0000 (37928.9952)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0004  max mem: 6186
Epoch: [25]  [19920/40201]  eta: 3:01:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6064 (3.4185)  loss_scale: 32768.0000 (37926.4045)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0004  max mem: 6186
Epoch: [25]  [19930/40201]  eta: 3:01:27  lr: 0.000003  min_lr: 0.000000  loss: 3.1776 (3.4182)  loss_scale: 32768.0000 (37923.8164)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0011  max mem: 6186
Epoch: [25]  [19940/40201]  eta: 3:01:21  lr: 0.000003  min_lr: 0.000000  loss: 2.8159 (3.4181)  loss_scale: 32768.0000 (37921.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0011  max mem: 6186
Epoch: [25]  [19950/40201]  eta: 3:01:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2627 (3.4182)  loss_scale: 32768.0000 (37918.6479)  weight_decay: 0.0500 (0.0500)  time: 0.4422  data: 0.0004  max mem: 6186
Epoch: [25]  [19960/40201]  eta: 3:01:06  lr: 0.000003  min_lr: 0.000000  loss: 2.8963 (3.4179)  loss_scale: 32768.0000 (37916.0675)  weight_decay: 0.0500 (0.0500)  time: 0.3415  data: 0.0014  max mem: 6186
Epoch: [25]  [19970/40201]  eta: 3:01:00  lr: 0.000003  min_lr: 0.000000  loss: 2.8963 (3.4179)  loss_scale: 32768.0000 (37913.4898)  weight_decay: 0.0500 (0.0500)  time: 0.3988  data: 0.0013  max mem: 6186
Epoch: [25]  [19980/40201]  eta: 3:00:53  lr: 0.000003  min_lr: 0.000000  loss: 3.2771 (3.4178)  loss_scale: 32768.0000 (37910.9146)  weight_decay: 0.0500 (0.0500)  time: 0.3987  data: 0.0014  max mem: 6186
Epoch: [25]  [19990/40201]  eta: 3:00:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1390 (3.4178)  loss_scale: 32768.0000 (37908.3420)  weight_decay: 0.0500 (0.0500)  time: 0.2520  data: 0.0116  max mem: 6186
[2023-07-24 18:13:53,993] [INFO] [logging.py:69:log_dist] [Rank 0] step=10000, skipped=53, lr=[6.180880320271548e-08, 6.180880320271548e-08, 8.241173760362064e-08, 8.241173760362064e-08, 1.098823168048275e-07, 1.098823168048275e-07, 1.4650975573977e-07, 1.4650975573977e-07, 1.9534634098636002e-07, 1.9534634098636002e-07, 2.604617879818134e-07, 2.604617879818134e-07, 3.4728238397575113e-07, 3.4728238397575113e-07, 4.6304317863433486e-07, 4.6304317863433486e-07, 6.173909048457798e-07, 6.173909048457798e-07, 8.231878731277064e-07, 8.231878731277064e-07, 1.097583830836942e-06, 1.097583830836942e-06, 1.463445107782589e-06, 1.463445107782589e-06, 1.9512601437101188e-06, 1.9512601437101188e-06, 2.601680191613492e-06, 2.601680191613492e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 18:13:53,994] [INFO] [timer.py:181:stop] 0/20000, SamplesPerSec=13.203441653643306
Epoch: [25]  [20000/40201]  eta: 3:00:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2363 (3.4180)  loss_scale: 32768.0000 (37905.7719)  weight_decay: 0.0500 (0.0500)  time: 0.2048  data: 0.0221  max mem: 6186
Epoch: [25]  [20010/40201]  eta: 3:00:28  lr: 0.000003  min_lr: 0.000000  loss: 3.5563 (3.4181)  loss_scale: 32768.0000 (37903.2044)  weight_decay: 0.0500 (0.0500)  time: 0.2848  data: 0.0368  max mem: 6186
Epoch: [25]  [20020/40201]  eta: 3:00:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2663 (3.4181)  loss_scale: 32768.0000 (37900.6395)  weight_decay: 0.0500 (0.0500)  time: 0.4317  data: 0.0253  max mem: 6186
Epoch: [25]  [20030/40201]  eta: 3:00:17  lr: 0.000003  min_lr: 0.000000  loss: 3.1552 (3.4180)  loss_scale: 32768.0000 (37898.0772)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0005  max mem: 6186
Epoch: [25]  [20040/40201]  eta: 3:00:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4437 (3.4182)  loss_scale: 32768.0000 (37895.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0004  max mem: 6186
Epoch: [25]  [20050/40201]  eta: 3:00:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4438 (3.4183)  loss_scale: 32768.0000 (37892.9602)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [20060/40201]  eta: 2:59:59  lr: 0.000003  min_lr: 0.000000  loss: 3.3724 (3.4182)  loss_scale: 32768.0000 (37890.4055)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0004  max mem: 6186
Epoch: [25]  [20070/40201]  eta: 2:59:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6060 (3.4186)  loss_scale: 32768.0000 (37887.8533)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0004  max mem: 6186
Epoch: [25]  [20080/40201]  eta: 2:59:48  lr: 0.000003  min_lr: 0.000000  loss: 3.9313 (3.4188)  loss_scale: 32768.0000 (37885.3037)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0005  max mem: 6186
Epoch: [25]  [20090/40201]  eta: 2:59:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3389 (3.4188)  loss_scale: 32768.0000 (37882.7567)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0005  max mem: 6186
Epoch: [25]  [20100/40201]  eta: 2:59:37  lr: 0.000003  min_lr: 0.000000  loss: 3.3389 (3.4191)  loss_scale: 32768.0000 (37880.2121)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0007  max mem: 6186
[2023-07-24 18:14:46,509] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:14:46,510] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:14:46,514] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:14:46,514] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [20110/40201]  eta: 2:59:31  lr: 0.000003  min_lr: 0.000000  loss: 3.7634 (3.4192)  loss_scale: 32768.0000 (37884.1876)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0007  max mem: 6186
Epoch: [25]  [20120/40201]  eta: 2:59:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1949 (3.4190)  loss_scale: 65536.0000 (37897.9303)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [20130/40201]  eta: 2:59:19  lr: 0.000003  min_lr: 0.000000  loss: 3.1192 (3.4190)  loss_scale: 65536.0000 (37911.6594)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0004  max mem: 6186
Epoch: [25]  [20140/40201]  eta: 2:59:14  lr: 0.000003  min_lr: 0.000000  loss: 3.3380 (3.4190)  loss_scale: 65536.0000 (37925.3749)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0004  max mem: 6186
[2023-07-24 18:15:04,319] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10071
[2023-07-24 18:15:04,319] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:15:04,319] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:15:04,321] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10071
[2023-07-24 18:15:04,322] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [20150/40201]  eta: 2:59:08  lr: 0.000003  min_lr: 0.000000  loss: 3.3380 (3.4190)  loss_scale: 65536.0000 (37926.0678)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0004  max mem: 6186
Epoch: [25]  [20160/40201]  eta: 2:59:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4328 (3.4192)  loss_scale: 32768.0000 (37923.5093)  weight_decay: 0.0500 (0.0500)  time: 0.4822  data: 0.0004  max mem: 6186
Epoch: [25]  [20170/40201]  eta: 2:58:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3547 (3.4191)  loss_scale: 32768.0000 (37920.9534)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0004  max mem: 6186
Epoch: [25]  [20180/40201]  eta: 2:58:51  lr: 0.000003  min_lr: 0.000000  loss: 2.8087 (3.4189)  loss_scale: 32768.0000 (37918.4001)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0011  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/9D0o8lh8oeY_002353_002363.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/9D0o8lh8oeY_002353_002363.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [20190/40201]  eta: 2:58:45  lr: 0.000003  min_lr: 0.000000  loss: 2.8087 (3.4188)  loss_scale: 32768.0000 (37915.8492)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0012  max mem: 6186
Epoch: [25]  [20200/40201]  eta: 2:58:39  lr: 0.000003  min_lr: 0.000000  loss: 3.4662 (3.4189)  loss_scale: 32768.0000 (37913.3009)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0005  max mem: 6186
Epoch: [25]  [20210/40201]  eta: 2:58:34  lr: 0.000003  min_lr: 0.000000  loss: 3.4035 (3.4189)  loss_scale: 32768.0000 (37910.7551)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0004  max mem: 6186
Epoch: [25]  [20220/40201]  eta: 2:58:28  lr: 0.000003  min_lr: 0.000000  loss: 3.2215 (3.4190)  loss_scale: 32768.0000 (37908.2119)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0004  max mem: 6186
Epoch: [25]  [20230/40201]  eta: 2:58:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2625 (3.4190)  loss_scale: 32768.0000 (37905.6711)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0004  max mem: 6186
Epoch: [25]  [20240/40201]  eta: 2:58:16  lr: 0.000003  min_lr: 0.000000  loss: 3.2625 (3.4190)  loss_scale: 32768.0000 (37903.1328)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0004  max mem: 6186
Epoch: [25]  [20250/40201]  eta: 2:58:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3681 (3.4189)  loss_scale: 32768.0000 (37900.5971)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0004  max mem: 6186
Epoch: [25]  [20260/40201]  eta: 2:58:05  lr: 0.000003  min_lr: 0.000000  loss: 3.3264 (3.4187)  loss_scale: 32768.0000 (37898.0639)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0004  max mem: 6186
Epoch: [25]  [20270/40201]  eta: 2:57:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2543 (3.4187)  loss_scale: 32768.0000 (37895.5331)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0004  max mem: 6186
Epoch: [25]  [20280/40201]  eta: 2:57:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4713 (3.4190)  loss_scale: 32768.0000 (37893.0049)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0004  max mem: 6186
Epoch: [25]  [20290/40201]  eta: 2:57:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7365 (3.4190)  loss_scale: 32768.0000 (37890.4791)  weight_decay: 0.0500 (0.0500)  time: 0.5003  data: 0.0004  max mem: 6186
Epoch: [25]  [20300/40201]  eta: 2:57:42  lr: 0.000003  min_lr: 0.000000  loss: 3.3626 (3.4190)  loss_scale: 32768.0000 (37887.9559)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0020  max mem: 6186
Epoch: [25]  [20310/40201]  eta: 2:57:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6331 (3.4192)  loss_scale: 32768.0000 (37885.4351)  weight_decay: 0.0500 (0.0500)  time: 0.3195  data: 0.0020  max mem: 6186
Epoch: [25]  [20320/40201]  eta: 2:57:24  lr: 0.000003  min_lr: 0.000000  loss: 3.2554 (3.4192)  loss_scale: 32768.0000 (37882.9168)  weight_decay: 0.0500 (0.0500)  time: 0.1788  data: 0.0005  max mem: 6186
Epoch: [25]  [20330/40201]  eta: 2:57:19  lr: 0.000003  min_lr: 0.000000  loss: 3.1198 (3.4191)  loss_scale: 32768.0000 (37880.4010)  weight_decay: 0.0500 (0.0500)  time: 0.3651  data: 0.0004  max mem: 6186
Epoch: [25]  [20340/40201]  eta: 2:57:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1881 (3.4190)  loss_scale: 32768.0000 (37877.8876)  weight_decay: 0.0500 (0.0500)  time: 0.3788  data: 0.0004  max mem: 6186
Epoch: [25]  [20350/40201]  eta: 2:57:03  lr: 0.000003  min_lr: 0.000000  loss: 3.1576 (3.4189)  loss_scale: 32768.0000 (37875.3767)  weight_decay: 0.0500 (0.0500)  time: 0.2545  data: 0.0060  max mem: 6186
Epoch: [25]  [20360/40201]  eta: 2:56:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2064 (3.4189)  loss_scale: 32768.0000 (37872.8683)  weight_decay: 0.0500 (0.0500)  time: 0.3124  data: 0.0066  max mem: 6186
Epoch: [25]  [20370/40201]  eta: 2:56:50  lr: 0.000003  min_lr: 0.000000  loss: 3.5442 (3.4192)  loss_scale: 32768.0000 (37870.3624)  weight_decay: 0.0500 (0.0500)  time: 0.4492  data: 0.0010  max mem: 6186
Epoch: [25]  [20380/40201]  eta: 2:56:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3755 (3.4189)  loss_scale: 32768.0000 (37867.8589)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0004  max mem: 6186
Epoch: [25]  [20390/40201]  eta: 2:56:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0025 (3.4189)  loss_scale: 32768.0000 (37865.3579)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0005  max mem: 6186
Epoch: [25]  [20400/40201]  eta: 2:56:33  lr: 0.000003  min_lr: 0.000000  loss: 2.8674 (3.4185)  loss_scale: 32768.0000 (37862.8593)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0005  max mem: 6186
[2023-07-24 18:17:01,011] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:17:01,011] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:17:01,023] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:17:01,023] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [20410/40201]  eta: 2:56:27  lr: 0.000003  min_lr: 0.000000  loss: 2.6864 (3.4185)  loss_scale: 32768.0000 (37876.4172)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0004  max mem: 6186
Epoch: [25]  [20420/40201]  eta: 2:56:22  lr: 0.000003  min_lr: 0.000000  loss: 3.2328 (3.4184)  loss_scale: 65536.0000 (37889.9619)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0004  max mem: 6186
Epoch: [25]  [20430/40201]  eta: 2:56:16  lr: 0.000003  min_lr: 0.000000  loss: 3.0935 (3.4184)  loss_scale: 65536.0000 (37903.4933)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0005  max mem: 6186
Epoch: [25]  [20440/40201]  eta: 2:56:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1111 (3.4183)  loss_scale: 65536.0000 (37917.0115)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0005  max mem: 6186
[2023-07-24 18:17:23,825] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10223
[2023-07-24 18:17:23,825] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10223
[2023-07-24 18:17:23,825] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:17:23,825] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:17:23,825] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [20450/40201]  eta: 2:56:04  lr: 0.000003  min_lr: 0.000000  loss: 3.1111 (3.4183)  loss_scale: 65536.0000 (37924.1074)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0005  max mem: 6186
Epoch: [25]  [20460/40201]  eta: 2:55:59  lr: 0.000003  min_lr: 0.000000  loss: 3.5084 (3.4183)  loss_scale: 32768.0000 (37921.5874)  weight_decay: 0.0500 (0.0500)  time: 0.4838  data: 0.0004  max mem: 6186
Epoch: [25]  [20470/40201]  eta: 2:55:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5446 (3.4186)  loss_scale: 32768.0000 (37919.0699)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0004  max mem: 6186
Epoch: [25]  [20480/40201]  eta: 2:55:47  lr: 0.000003  min_lr: 0.000000  loss: 3.9911 (3.4189)  loss_scale: 32768.0000 (37916.5549)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0005  max mem: 6186
Epoch: [25]  [20490/40201]  eta: 2:55:42  lr: 0.000003  min_lr: 0.000000  loss: 3.7006 (3.4187)  loss_scale: 32768.0000 (37914.0423)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0004  max mem: 6186
Epoch: [25]  [20500/40201]  eta: 2:55:36  lr: 0.000003  min_lr: 0.000000  loss: 3.3233 (3.4187)  loss_scale: 32768.0000 (37911.5321)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0004  max mem: 6186
Epoch: [25]  [20510/40201]  eta: 2:55:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3927 (3.4189)  loss_scale: 32768.0000 (37909.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0004  max mem: 6186
Epoch: [25]  [20520/40201]  eta: 2:55:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0773 (3.4188)  loss_scale: 32768.0000 (37906.5192)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0005  max mem: 6186
Epoch: [25]  [20530/40201]  eta: 2:55:19  lr: 0.000003  min_lr: 0.000000  loss: 3.0126 (3.4187)  loss_scale: 32768.0000 (37904.0164)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0005  max mem: 6186
Epoch: [25]  [20540/40201]  eta: 2:55:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2199 (3.4187)  loss_scale: 32768.0000 (37901.5160)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0004  max mem: 6186
Epoch: [25]  [20550/40201]  eta: 2:55:08  lr: 0.000003  min_lr: 0.000000  loss: 3.7066 (3.4188)  loss_scale: 32768.0000 (37899.0181)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0005  max mem: 6186
Epoch: [25]  [20560/40201]  eta: 2:55:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4465 (3.4187)  loss_scale: 32768.0000 (37896.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0005  max mem: 6186
Epoch: [25]  [20570/40201]  eta: 2:54:57  lr: 0.000003  min_lr: 0.000000  loss: 3.4915 (3.4190)  loss_scale: 32768.0000 (37894.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0005  max mem: 6186
Epoch: [25]  [20580/40201]  eta: 2:54:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7473 (3.4189)  loss_scale: 32768.0000 (37891.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0004  max mem: 6186
Epoch: [25]  [20590/40201]  eta: 2:54:45  lr: 0.000003  min_lr: 0.000000  loss: 3.2366 (3.4189)  loss_scale: 32768.0000 (37889.0506)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0024  max mem: 6186
Epoch: [25]  [20600/40201]  eta: 2:54:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6969 (3.4191)  loss_scale: 32768.0000 (37886.5647)  weight_decay: 0.0500 (0.0500)  time: 0.4263  data: 0.0024  max mem: 6186
Epoch: [25]  [20610/40201]  eta: 2:54:29  lr: 0.000003  min_lr: 0.000000  loss: 3.6969 (3.4190)  loss_scale: 32768.0000 (37884.0813)  weight_decay: 0.0500 (0.0500)  time: 0.2694  data: 0.0005  max mem: 6186
Epoch: [25]  [20620/40201]  eta: 2:54:21  lr: 0.000003  min_lr: 0.000000  loss: 3.1986 (3.4189)  loss_scale: 32768.0000 (37881.6003)  weight_decay: 0.0500 (0.0500)  time: 0.2177  data: 0.0005  max mem: 6186
Epoch: [25]  [20630/40201]  eta: 2:54:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2103 (3.4189)  loss_scale: 32768.0000 (37879.1217)  weight_decay: 0.0500 (0.0500)  time: 0.2415  data: 0.0152  max mem: 6186
Epoch: [25]  [20640/40201]  eta: 2:54:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3201 (3.4188)  loss_scale: 32768.0000 (37876.6455)  weight_decay: 0.0500 (0.0500)  time: 0.3135  data: 0.0153  max mem: 6186
Epoch: [25]  [20650/40201]  eta: 2:53:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8612 (3.4192)  loss_scale: 32768.0000 (37874.1717)  weight_decay: 0.0500 (0.0500)  time: 0.3369  data: 0.0005  max mem: 6186
Epoch: [25]  [20660/40201]  eta: 2:53:51  lr: 0.000003  min_lr: 0.000000  loss: 3.5714 (3.4191)  loss_scale: 32768.0000 (37871.7003)  weight_decay: 0.0500 (0.0500)  time: 0.2859  data: 0.0005  max mem: 6186
Epoch: [25]  [20670/40201]  eta: 2:53:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1476 (3.4192)  loss_scale: 32768.0000 (37869.2313)  weight_decay: 0.0500 (0.0500)  time: 0.3120  data: 0.0005  max mem: 6186
Epoch: [25]  [20680/40201]  eta: 2:53:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4805 (3.4190)  loss_scale: 32768.0000 (37866.7647)  weight_decay: 0.0500 (0.0500)  time: 0.4176  data: 0.0005  max mem: 6186
Epoch: [25]  [20690/40201]  eta: 2:53:33  lr: 0.000003  min_lr: 0.000000  loss: 3.3930 (3.4191)  loss_scale: 32768.0000 (37864.3004)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0004  max mem: 6186
Epoch: [25]  [20700/40201]  eta: 2:53:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3608 (3.4192)  loss_scale: 32768.0000 (37861.8386)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0004  max mem: 6186
[2023-07-24 18:19:16,281] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:19:16,281] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:19:16,284] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:19:16,284] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [20710/40201]  eta: 2:53:21  lr: 0.000003  min_lr: 0.000000  loss: 3.1018 (3.4192)  loss_scale: 32768.0000 (37868.8720)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0013  max mem: 6186
Epoch: [25]  [20720/40201]  eta: 2:53:16  lr: 0.000003  min_lr: 0.000000  loss: 3.2352 (3.4192)  loss_scale: 65536.0000 (37882.2242)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0013  max mem: 6186
[2023-07-24 18:19:28,031] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10364
[2023-07-24 18:19:28,031] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:19:28,031] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:19:28,033] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10364
[2023-07-24 18:19:28,033] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [20730/40201]  eta: 2:53:10  lr: 0.000003  min_lr: 0.000000  loss: 3.2352 (3.4192)  loss_scale: 65536.0000 (37892.4023)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0004  max mem: 6186
Epoch: [25]  [20740/40201]  eta: 2:53:04  lr: 0.000003  min_lr: 0.000000  loss: 3.1778 (3.4190)  loss_scale: 32768.0000 (37889.9316)  weight_decay: 0.0500 (0.0500)  time: 0.4784  data: 0.0005  max mem: 6186
Epoch: [25]  [20750/40201]  eta: 2:52:58  lr: 0.000003  min_lr: 0.000000  loss: 3.2981 (3.4191)  loss_scale: 32768.0000 (37887.4634)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0005  max mem: 6186
Epoch: [25]  [20760/40201]  eta: 2:52:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8014 (3.4192)  loss_scale: 32768.0000 (37884.9974)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0012  max mem: 6186
Epoch: [25]  [20770/40201]  eta: 2:52:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4054 (3.4191)  loss_scale: 32768.0000 (37882.5339)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0012  max mem: 6186
Epoch: [25]  [20780/40201]  eta: 2:52:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3010 (3.4191)  loss_scale: 32768.0000 (37880.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0004  max mem: 6186
Epoch: [25]  [20790/40201]  eta: 2:52:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6550 (3.4193)  loss_scale: 32768.0000 (37877.6140)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0004  max mem: 6186
Epoch: [25]  [20800/40201]  eta: 2:52:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5412 (3.4192)  loss_scale: 32768.0000 (37875.1575)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0004  max mem: 6186
Epoch: [25]  [20810/40201]  eta: 2:52:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0298 (3.4191)  loss_scale: 32768.0000 (37872.7035)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0004  max mem: 6186
Epoch: [25]  [20820/40201]  eta: 2:52:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5587 (3.4192)  loss_scale: 32768.0000 (37870.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0013  max mem: 6186
Epoch: [25]  [20830/40201]  eta: 2:52:13  lr: 0.000003  min_lr: 0.000000  loss: 3.3206 (3.4192)  loss_scale: 32768.0000 (37867.8024)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0013  max mem: 6186
Epoch: [25]  [20840/40201]  eta: 2:52:05  lr: 0.000003  min_lr: 0.000000  loss: 3.2941 (3.4191)  loss_scale: 32768.0000 (37865.3554)  weight_decay: 0.0500 (0.0500)  time: 0.3497  data: 0.0004  max mem: 6186
Epoch: [25]  [20850/40201]  eta: 2:51:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1170 (3.4190)  loss_scale: 32768.0000 (37862.9107)  weight_decay: 0.0500 (0.0500)  time: 0.1981  data: 0.0004  max mem: 6186
Epoch: [25]  [20860/40201]  eta: 2:51:51  lr: 0.000003  min_lr: 0.000000  loss: 3.0608 (3.4189)  loss_scale: 32768.0000 (37860.4684)  weight_decay: 0.0500 (0.0500)  time: 0.3551  data: 0.0056  max mem: 6186
Epoch: [25]  [20870/40201]  eta: 2:51:43  lr: 0.000003  min_lr: 0.000000  loss: 3.3022 (3.4189)  loss_scale: 32768.0000 (37858.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3760  data: 0.0056  max mem: 6186
Epoch: [25]  [20880/40201]  eta: 2:51:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2670 (3.4190)  loss_scale: 32768.0000 (37855.5908)  weight_decay: 0.0500 (0.0500)  time: 0.2248  data: 0.0004  max mem: 6186
Epoch: [25]  [20890/40201]  eta: 2:51:27  lr: 0.000003  min_lr: 0.000000  loss: 3.1614 (3.4191)  loss_scale: 32768.0000 (37853.1555)  weight_decay: 0.0500 (0.0500)  time: 0.2832  data: 0.0004  max mem: 6186
Epoch: [25]  [20900/40201]  eta: 2:51:20  lr: 0.000003  min_lr: 0.000000  loss: 3.0643 (3.4190)  loss_scale: 32768.0000 (37850.7225)  weight_decay: 0.0500 (0.0500)  time: 0.3501  data: 0.0018  max mem: 6186
Epoch: [25]  [20910/40201]  eta: 2:51:15  lr: 0.000003  min_lr: 0.000000  loss: 3.0643 (3.4189)  loss_scale: 32768.0000 (37848.2919)  weight_decay: 0.0500 (0.0500)  time: 0.4269  data: 0.0018  max mem: 6186
Epoch: [25]  [20920/40201]  eta: 2:51:09  lr: 0.000003  min_lr: 0.000000  loss: 3.3415 (3.4189)  loss_scale: 32768.0000 (37845.8636)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0011  max mem: 6186
Epoch: [25]  [20930/40201]  eta: 2:51:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5363 (3.4190)  loss_scale: 32768.0000 (37843.4376)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0011  max mem: 6186
Epoch: [25]  [20940/40201]  eta: 2:50:58  lr: 0.000003  min_lr: 0.000000  loss: 3.2151 (3.4189)  loss_scale: 32768.0000 (37841.0139)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0004  max mem: 6186
Epoch: [25]  [20950/40201]  eta: 2:50:52  lr: 0.000003  min_lr: 0.000000  loss: 3.0216 (3.4188)  loss_scale: 32768.0000 (37838.5925)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0004  max mem: 6186
Epoch: [25]  [20960/40201]  eta: 2:50:47  lr: 0.000003  min_lr: 0.000000  loss: 3.2322 (3.4188)  loss_scale: 32768.0000 (37836.1735)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0012  max mem: 6186
Epoch: [25]  [20970/40201]  eta: 2:50:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6414 (3.4190)  loss_scale: 32768.0000 (37833.7567)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0021  max mem: 6186
Epoch: [25]  [20980/40201]  eta: 2:50:35  lr: 0.000003  min_lr: 0.000000  loss: 3.6414 (3.4189)  loss_scale: 32768.0000 (37831.3423)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0013  max mem: 6186
[2023-07-24 18:21:22,634] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:21:22,634] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:21:22,638] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:21:22,638] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [20990/40201]  eta: 2:50:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3773 (3.4190)  loss_scale: 32768.0000 (37835.1743)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0005  max mem: 6186
[2023-07-24 18:21:28,196] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10499
[2023-07-24 18:21:28,196] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:21:28,200] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10499
[2023-07-24 18:21:28,200] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:21:28,200] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:21:28,201] [INFO] [timer.py:181:stop] 0/21000, SamplesPerSec=13.254391644675554
Epoch: [25]  [21000/40201]  eta: 2:50:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4627 (3.4190)  loss_scale: 65536.0000 (37845.2439)  weight_decay: 0.0500 (0.0500)  time: 0.4734  data: 0.0004  max mem: 6186
Epoch: [25]  [21010/40201]  eta: 2:50:18  lr: 0.000003  min_lr: 0.000000  loss: 3.5583 (3.4191)  loss_scale: 32768.0000 (37842.8275)  weight_decay: 0.0500 (0.0500)  time: 0.4768  data: 0.0004  max mem: 6186
Epoch: [25]  [21020/40201]  eta: 2:50:12  lr: 0.000003  min_lr: 0.000000  loss: 3.2029 (3.4190)  loss_scale: 32768.0000 (37840.4133)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0004  max mem: 6186
Epoch: [25]  [21030/40201]  eta: 2:50:07  lr: 0.000003  min_lr: 0.000000  loss: 3.2020 (3.4189)  loss_scale: 32768.0000 (37838.0014)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0004  max mem: 6186
Epoch: [25]  [21040/40201]  eta: 2:50:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4944 (3.4189)  loss_scale: 32768.0000 (37835.5918)  weight_decay: 0.0500 (0.0500)  time: 0.5006  data: 0.0005  max mem: 6186
Epoch: [25]  [21050/40201]  eta: 2:49:55  lr: 0.000003  min_lr: 0.000000  loss: 3.1735 (3.4188)  loss_scale: 32768.0000 (37833.1846)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0005  max mem: 6186
Epoch: [25]  [21060/40201]  eta: 2:49:50  lr: 0.000003  min_lr: 0.000000  loss: 3.3315 (3.4188)  loss_scale: 32768.0000 (37830.7795)  weight_decay: 0.0500 (0.0500)  time: 0.4929  data: 0.0005  max mem: 6186
Epoch: [25]  [21070/40201]  eta: 2:49:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1371 (3.4186)  loss_scale: 32768.0000 (37828.3768)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0004  max mem: 6186
Epoch: [25]  [21080/40201]  eta: 2:49:39  lr: 0.000003  min_lr: 0.000000  loss: 2.6574 (3.4183)  loss_scale: 32768.0000 (37825.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0004  max mem: 6186
Epoch: [25]  [21090/40201]  eta: 2:49:33  lr: 0.000003  min_lr: 0.000000  loss: 2.6350 (3.4183)  loss_scale: 32768.0000 (37823.5782)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0004  max mem: 6186
Epoch: [25]  [21100/40201]  eta: 2:49:27  lr: 0.000003  min_lr: 0.000000  loss: 2.7419 (3.4180)  loss_scale: 32768.0000 (37821.1823)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0004  max mem: 6186
Epoch: [25]  [21110/40201]  eta: 2:49:22  lr: 0.000003  min_lr: 0.000000  loss: 2.7841 (3.4180)  loss_scale: 32768.0000 (37818.7887)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0005  max mem: 6186
Epoch: [25]  [21120/40201]  eta: 2:49:16  lr: 0.000003  min_lr: 0.000000  loss: 3.1623 (3.4178)  loss_scale: 32768.0000 (37816.3973)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0013  max mem: 6186
Epoch: [25]  [21130/40201]  eta: 2:49:10  lr: 0.000003  min_lr: 0.000000  loss: 3.1047 (3.4178)  loss_scale: 32768.0000 (37814.0082)  weight_decay: 0.0500 (0.0500)  time: 0.4933  data: 0.0013  max mem: 6186
Epoch: [25]  [21140/40201]  eta: 2:49:05  lr: 0.000003  min_lr: 0.000000  loss: 3.1047 (3.4175)  loss_scale: 32768.0000 (37811.6214)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0005  max mem: 6186
Epoch: [25]  [21150/40201]  eta: 2:48:59  lr: 0.000003  min_lr: 0.000000  loss: 3.1662 (3.4177)  loss_scale: 32768.0000 (37809.2368)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0005  max mem: 6186
Epoch: [25]  [21160/40201]  eta: 2:48:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4079 (3.4177)  loss_scale: 32768.0000 (37806.8545)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0005  max mem: 6186
Epoch: [25]  [21170/40201]  eta: 2:48:45  lr: 0.000003  min_lr: 0.000000  loss: 3.5372 (3.4178)  loss_scale: 32768.0000 (37804.4744)  weight_decay: 0.0500 (0.0500)  time: 0.3686  data: 0.0005  max mem: 6186
Epoch: [25]  [21180/40201]  eta: 2:48:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5176 (3.4178)  loss_scale: 32768.0000 (37802.0966)  weight_decay: 0.0500 (0.0500)  time: 0.2182  data: 0.0004  max mem: 6186
Epoch: [25]  [21190/40201]  eta: 2:48:29  lr: 0.000003  min_lr: 0.000000  loss: 3.5751 (3.4180)  loss_scale: 32768.0000 (37799.7210)  weight_decay: 0.0500 (0.0500)  time: 0.2370  data: 0.0059  max mem: 6186
Epoch: [25]  [21200/40201]  eta: 2:48:22  lr: 0.000003  min_lr: 0.000000  loss: 3.3105 (3.4180)  loss_scale: 32768.0000 (37797.3477)  weight_decay: 0.0500 (0.0500)  time: 0.2978  data: 0.0059  max mem: 6186
Epoch: [25]  [21210/40201]  eta: 2:48:15  lr: 0.000003  min_lr: 0.000000  loss: 3.2146 (3.4179)  loss_scale: 32768.0000 (37794.9766)  weight_decay: 0.0500 (0.0500)  time: 0.3006  data: 0.0005  max mem: 6186
Epoch: [25]  [21220/40201]  eta: 2:48:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4830 (3.4181)  loss_scale: 32768.0000 (37792.6077)  weight_decay: 0.0500 (0.0500)  time: 0.2753  data: 0.0005  max mem: 6186
Epoch: [25]  [21230/40201]  eta: 2:47:59  lr: 0.000003  min_lr: 0.000000  loss: 3.9806 (3.4183)  loss_scale: 32768.0000 (37790.2411)  weight_decay: 0.0500 (0.0500)  time: 0.2680  data: 0.0196  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/bgCrldl9pQ8_000027_000037.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/bgCrldl9pQ8_000027_000037.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [21240/40201]  eta: 2:47:52  lr: 0.000003  min_lr: 0.000000  loss: 3.0892 (3.4181)  loss_scale: 32768.0000 (37787.8767)  weight_decay: 0.0500 (0.0500)  time: 0.2845  data: 0.0724  max mem: 6186
Epoch: [25]  [21250/40201]  eta: 2:47:47  lr: 0.000003  min_lr: 0.000000  loss: 3.0394 (3.4181)  loss_scale: 32768.0000 (37785.5145)  weight_decay: 0.0500 (0.0500)  time: 0.4176  data: 0.0579  max mem: 6186
[2023-07-24 18:23:18,672] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:23:18,672] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:23:18,674] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:23:18,674] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [21260/40201]  eta: 2:47:41  lr: 0.000003  min_lr: 0.000000  loss: 3.3048 (3.4180)  loss_scale: 32768.0000 (37789.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0050  max mem: 6186
Epoch: [25]  [21270/40201]  eta: 2:47:36  lr: 0.000003  min_lr: 0.000000  loss: 3.1428 (3.4179)  loss_scale: 65536.0000 (37802.3638)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0005  max mem: 6186
Epoch: [25]  [21280/40201]  eta: 2:47:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3922 (3.4181)  loss_scale: 65536.0000 (37815.3959)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0005  max mem: 6186
Epoch: [25]  [21290/40201]  eta: 2:47:25  lr: 0.000003  min_lr: 0.000000  loss: 3.6293 (3.4180)  loss_scale: 65536.0000 (37828.4158)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0005  max mem: 6186
Epoch: [25]  [21300/40201]  eta: 2:47:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4231 (3.4180)  loss_scale: 65536.0000 (37841.4234)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0005  max mem: 6186
Epoch: [25]  [21310/40201]  eta: 2:47:13  lr: 0.000003  min_lr: 0.000000  loss: 3.2469 (3.4178)  loss_scale: 65536.0000 (37854.4188)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0005  max mem: 6186
Epoch: [25]  [21320/40201]  eta: 2:47:08  lr: 0.000003  min_lr: 0.000000  loss: 2.9370 (3.4177)  loss_scale: 65536.0000 (37867.4021)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0005  max mem: 6186
Epoch: [25]  [21330/40201]  eta: 2:47:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2341 (3.4178)  loss_scale: 65536.0000 (37880.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0005  max mem: 6186
Epoch: [25]  [21340/40201]  eta: 2:46:57  lr: 0.000003  min_lr: 0.000000  loss: 3.3649 (3.4178)  loss_scale: 65536.0000 (37893.3321)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0005  max mem: 6186
Epoch: [25]  [21350/40201]  eta: 2:46:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3642 (3.4179)  loss_scale: 65536.0000 (37906.2789)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0005  max mem: 6186
Epoch: [25]  [21360/40201]  eta: 2:46:45  lr: 0.000003  min_lr: 0.000000  loss: 3.7834 (3.4180)  loss_scale: 65536.0000 (37919.2135)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0004  max mem: 6186
Epoch: [25]  [21370/40201]  eta: 2:46:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5021 (3.4180)  loss_scale: 65536.0000 (37932.1361)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0005  max mem: 6186
Epoch: [25]  [21380/40201]  eta: 2:46:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2975 (3.4178)  loss_scale: 65536.0000 (37945.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0005  max mem: 6186
Epoch: [25]  [21390/40201]  eta: 2:46:27  lr: 0.000003  min_lr: 0.000000  loss: 3.2975 (3.4180)  loss_scale: 65536.0000 (37957.9449)  weight_decay: 0.0500 (0.0500)  time: 0.3830  data: 0.0004  max mem: 6186
Epoch: [25]  [21400/40201]  eta: 2:46:18  lr: 0.000003  min_lr: 0.000000  loss: 3.4156 (3.4180)  loss_scale: 65536.0000 (37970.8313)  weight_decay: 0.0500 (0.0500)  time: 0.2268  data: 0.0005  max mem: 6186
Epoch: [25]  [21410/40201]  eta: 2:46:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3250 (3.4179)  loss_scale: 65536.0000 (37983.7056)  weight_decay: 0.0500 (0.0500)  time: 0.2144  data: 0.0337  max mem: 6186
Epoch: [25]  [21420/40201]  eta: 2:46:03  lr: 0.000003  min_lr: 0.000000  loss: 3.2125 (3.4179)  loss_scale: 65536.0000 (37996.5679)  weight_decay: 0.0500 (0.0500)  time: 0.2645  data: 0.0845  max mem: 6186
Epoch: [25]  [21430/40201]  eta: 2:45:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3149 (3.4180)  loss_scale: 65536.0000 (38009.4181)  weight_decay: 0.0500 (0.0500)  time: 0.3172  data: 0.1317  max mem: 6186
Epoch: [25]  [21440/40201]  eta: 2:45:49  lr: 0.000003  min_lr: 0.000000  loss: 3.0261 (3.4178)  loss_scale: 65536.0000 (38022.2564)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.1215  max mem: 6186
Epoch: [25]  [21450/40201]  eta: 2:45:42  lr: 0.000003  min_lr: 0.000000  loss: 3.0329 (3.4176)  loss_scale: 65536.0000 (38035.0827)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.1240  max mem: 6186
Epoch: [25]  [21460/40201]  eta: 2:45:34  lr: 0.000003  min_lr: 0.000000  loss: 3.0958 (3.4175)  loss_scale: 65536.0000 (38047.8971)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0923  max mem: 6186
[2023-07-24 18:24:44,925] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10732
[2023-07-24 18:24:44,925] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:24:44,925] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10732
[2023-07-24 18:24:44,925] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:24:44,925] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [21470/40201]  eta: 2:45:28  lr: 0.000003  min_lr: 0.000000  loss: 3.3249 (3.4176)  loss_scale: 65536.0000 (38051.5426)  weight_decay: 0.0500 (0.0500)  time: 0.3672  data: 0.0111  max mem: 6186
Epoch: [25]  [21480/40201]  eta: 2:45:23  lr: 0.000003  min_lr: 0.000000  loss: 3.2028 (3.4175)  loss_scale: 32768.0000 (38049.0830)  weight_decay: 0.0500 (0.0500)  time: 0.4874  data: 0.0022  max mem: 6186
Epoch: [25]  [21490/40201]  eta: 2:45:17  lr: 0.000003  min_lr: 0.000000  loss: 2.9159 (3.4174)  loss_scale: 32768.0000 (38046.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0004  max mem: 6186
Epoch: [25]  [21500/40201]  eta: 2:45:12  lr: 0.000003  min_lr: 0.000000  loss: 2.7385 (3.4173)  loss_scale: 32768.0000 (38044.1706)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0011  max mem: 6186
Epoch: [25]  [21510/40201]  eta: 2:45:06  lr: 0.000003  min_lr: 0.000000  loss: 3.0069 (3.4173)  loss_scale: 32768.0000 (38041.7178)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0011  max mem: 6186
Epoch: [25]  [21520/40201]  eta: 2:45:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4430 (3.4174)  loss_scale: 32768.0000 (38039.2673)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0004  max mem: 6186
Epoch: [25]  [21530/40201]  eta: 2:44:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4489 (3.4173)  loss_scale: 32768.0000 (38036.8191)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0004  max mem: 6186
Epoch: [25]  [21540/40201]  eta: 2:44:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1491 (3.4172)  loss_scale: 32768.0000 (38034.3731)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0018  max mem: 6186
Epoch: [25]  [21550/40201]  eta: 2:44:44  lr: 0.000003  min_lr: 0.000000  loss: 3.1352 (3.4171)  loss_scale: 32768.0000 (38031.9295)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0161  max mem: 6186
Epoch: [25]  [21560/40201]  eta: 2:44:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0578 (3.4170)  loss_scale: 32768.0000 (38029.4881)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0147  max mem: 6186
Epoch: [25]  [21570/40201]  eta: 2:44:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6092 (3.4171)  loss_scale: 32768.0000 (38027.0489)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0004  max mem: 6186
Epoch: [25]  [21580/40201]  eta: 2:44:28  lr: 0.000003  min_lr: 0.000000  loss: 3.6092 (3.4171)  loss_scale: 32768.0000 (38024.6120)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0004  max mem: 6186
Epoch: [25]  [21590/40201]  eta: 2:44:22  lr: 0.000003  min_lr: 0.000000  loss: 3.0805 (3.4169)  loss_scale: 32768.0000 (38022.1774)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [21600/40201]  eta: 2:44:17  lr: 0.000003  min_lr: 0.000000  loss: 2.6827 (3.4166)  loss_scale: 32768.0000 (38019.7450)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0005  max mem: 6186
Epoch: [25]  [21610/40201]  eta: 2:44:11  lr: 0.000003  min_lr: 0.000000  loss: 2.8939 (3.4165)  loss_scale: 32768.0000 (38017.3149)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0004  max mem: 6186
Epoch: [25]  [21620/40201]  eta: 2:44:06  lr: 0.000003  min_lr: 0.000000  loss: 3.2633 (3.4165)  loss_scale: 32768.0000 (38014.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0004  max mem: 6186
Epoch: [25]  [21630/40201]  eta: 2:44:00  lr: 0.000003  min_lr: 0.000000  loss: 3.3944 (3.4166)  loss_scale: 32768.0000 (38012.4614)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0004  max mem: 6186
Epoch: [25]  [21640/40201]  eta: 2:43:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6763 (3.4167)  loss_scale: 32768.0000 (38010.0380)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0004  max mem: 6186
Epoch: [25]  [21650/40201]  eta: 2:43:49  lr: 0.000003  min_lr: 0.000000  loss: 3.3838 (3.4167)  loss_scale: 32768.0000 (38007.6168)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0005  max mem: 6186
Epoch: [25]  [21660/40201]  eta: 2:43:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4245 (3.4167)  loss_scale: 32768.0000 (38005.1979)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0005  max mem: 6186
Epoch: [25]  [21670/40201]  eta: 2:43:38  lr: 0.000003  min_lr: 0.000000  loss: 3.2255 (3.4165)  loss_scale: 32768.0000 (38002.7812)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0004  max mem: 6186
Epoch: [25]  [21680/40201]  eta: 2:43:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1057 (3.4165)  loss_scale: 32768.0000 (38000.3668)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0004  max mem: 6186
Epoch: [25]  [21690/40201]  eta: 2:43:27  lr: 0.000003  min_lr: 0.000000  loss: 3.0607 (3.4164)  loss_scale: 32768.0000 (37997.9545)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0004  max mem: 6186
Epoch: [25]  [21700/40201]  eta: 2:43:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4485 (3.4166)  loss_scale: 32768.0000 (37995.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0004  max mem: 6186
Epoch: [25]  [21710/40201]  eta: 2:43:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6872 (3.4166)  loss_scale: 32768.0000 (37993.1368)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0006  max mem: 6186
Epoch: [25]  [21720/40201]  eta: 2:43:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2337 (3.4166)  loss_scale: 32768.0000 (37990.7312)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0011  max mem: 6186
[2023-07-24 18:26:56,318] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:26:56,318] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:26:56,320] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:26:56,320] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [21730/40201]  eta: 2:43:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3287 (3.4167)  loss_scale: 32768.0000 (38000.3910)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0012  max mem: 6186
[2023-07-24 18:27:03,805] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10868
[2023-07-24 18:27:03,806] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:27:03,806] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 10868
[2023-07-24 18:27:03,806] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:27:03,806] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [21740/40201]  eta: 2:43:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3742 (3.4168)  loss_scale: 65536.0000 (38007.0275)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0008  max mem: 6186
Epoch: [25]  [21750/40201]  eta: 2:42:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3154 (3.4167)  loss_scale: 32768.0000 (38004.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.0013  max mem: 6186
Epoch: [25]  [21760/40201]  eta: 2:42:53  lr: 0.000003  min_lr: 0.000000  loss: 3.4043 (3.4167)  loss_scale: 32768.0000 (38002.2124)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.0012  max mem: 6186
Epoch: [25]  [21770/40201]  eta: 2:42:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6108 (3.4168)  loss_scale: 32768.0000 (37999.8082)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0005  max mem: 6186
Epoch: [25]  [21780/40201]  eta: 2:42:43  lr: 0.000003  min_lr: 0.000000  loss: 3.6108 (3.4168)  loss_scale: 32768.0000 (37997.4062)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0007  max mem: 6186
Epoch: [25]  [21790/40201]  eta: 2:42:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5626 (3.4169)  loss_scale: 32768.0000 (37995.0064)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0007  max mem: 6186
Epoch: [25]  [21800/40201]  eta: 2:42:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5626 (3.4171)  loss_scale: 32768.0000 (37992.6088)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0012  max mem: 6186
Epoch: [25]  [21810/40201]  eta: 2:42:28  lr: 0.000003  min_lr: 0.000000  loss: 3.4431 (3.4171)  loss_scale: 32768.0000 (37990.2134)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0020  max mem: 6186
Epoch: [25]  [21820/40201]  eta: 2:42:23  lr: 0.000003  min_lr: 0.000000  loss: 3.4431 (3.4174)  loss_scale: 32768.0000 (37987.8202)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0016  max mem: 6186
Epoch: [25]  [21830/40201]  eta: 2:42:20  lr: 0.000003  min_lr: 0.000000  loss: 3.6312 (3.4174)  loss_scale: 32768.0000 (37985.4292)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0010  max mem: 6186
Epoch: [25]  [21840/40201]  eta: 2:42:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2123 (3.4174)  loss_scale: 32768.0000 (37983.0403)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0006  max mem: 6186
Epoch: [25]  [21850/40201]  eta: 2:42:09  lr: 0.000003  min_lr: 0.000000  loss: 3.0608 (3.4172)  loss_scale: 32768.0000 (37980.6537)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0005  max mem: 6186
Epoch: [25]  [21860/40201]  eta: 2:42:03  lr: 0.000003  min_lr: 0.000000  loss: 3.1158 (3.4171)  loss_scale: 32768.0000 (37978.2692)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0005  max mem: 6186
Epoch: [25]  [21870/40201]  eta: 2:41:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3299 (3.4170)  loss_scale: 32768.0000 (37975.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0005  max mem: 6186
Epoch: [25]  [21880/40201]  eta: 2:41:52  lr: 0.000003  min_lr: 0.000000  loss: 3.3944 (3.4170)  loss_scale: 32768.0000 (37973.5069)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0004  max mem: 6186
Epoch: [25]  [21890/40201]  eta: 2:41:47  lr: 0.000003  min_lr: 0.000000  loss: 3.1758 (3.4169)  loss_scale: 32768.0000 (37971.1290)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
Epoch: [25]  [21900/40201]  eta: 2:41:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1955 (3.4170)  loss_scale: 32768.0000 (37968.7532)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0004  max mem: 6186
Epoch: [25]  [21910/40201]  eta: 2:41:36  lr: 0.000003  min_lr: 0.000000  loss: 3.2500 (3.4169)  loss_scale: 32768.0000 (37966.3796)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0004  max mem: 6186
Epoch: [25]  [21920/40201]  eta: 2:41:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3343 (3.4170)  loss_scale: 32768.0000 (37964.0082)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0004  max mem: 6186
Epoch: [25]  [21930/40201]  eta: 2:41:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1920 (3.4169)  loss_scale: 32768.0000 (37961.6390)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
Epoch: [25]  [21940/40201]  eta: 2:41:19  lr: 0.000003  min_lr: 0.000000  loss: 3.1614 (3.4169)  loss_scale: 32768.0000 (37959.2719)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0004  max mem: 6186
Epoch: [25]  [21950/40201]  eta: 2:41:14  lr: 0.000003  min_lr: 0.000000  loss: 3.2459 (3.4168)  loss_scale: 32768.0000 (37956.9069)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0004  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/aj1bmhf-IyU_000118_000128.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/aj1bmhf-IyU_000118_000128.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [21960/40201]  eta: 2:41:08  lr: 0.000003  min_lr: 0.000000  loss: 3.4659 (3.4168)  loss_scale: 32768.0000 (37954.5441)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0005  max mem: 6186
Epoch: [25]  [21970/40201]  eta: 2:41:02  lr: 0.000003  min_lr: 0.000000  loss: 3.4741 (3.4168)  loss_scale: 32768.0000 (37952.1835)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0004  max mem: 6186
Epoch: [25]  [21980/40201]  eta: 2:40:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2919 (3.4167)  loss_scale: 32768.0000 (37949.8250)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0004  max mem: 6186
Epoch: [25]  [21990/40201]  eta: 2:40:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3892 (3.4168)  loss_scale: 32768.0000 (37947.4687)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0004  max mem: 6186
[2023-07-24 18:29:24,035] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:29:24,035] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:29:24,050] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:29:24,050] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:29:26,395] [INFO] [logging.py:69:log_dist] [Rank 0] step=11000, skipped=59, lr=[6.049916186008038e-08, 6.049916186008038e-08, 8.066554914677384e-08, 8.066554914677384e-08, 1.0755406552903179e-07, 1.0755406552903179e-07, 1.434054207053757e-07, 1.434054207053757e-07, 1.9120722760716762e-07, 1.9120722760716762e-07, 2.549429701428902e-07, 2.549429701428902e-07, 3.399239601905202e-07, 3.399239601905202e-07, 4.532319469206936e-07, 4.532319469206936e-07, 6.043092625609248e-07, 6.043092625609248e-07, 8.057456834145664e-07, 8.057456834145664e-07, 1.0743275778860886e-06, 1.0743275778860886e-06, 1.4324367705147847e-06, 1.4324367705147847e-06, 1.909915694019713e-06, 1.909915694019713e-06, 2.5465542586929506e-06, 2.5465542586929506e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 18:29:26,400] [INFO] [timer.py:181:stop] 0/22000, SamplesPerSec=13.2825213464432
Epoch: [25]  [22000/40201]  eta: 2:40:46  lr: 0.000003  min_lr: 0.000000  loss: 3.2410 (3.4168)  loss_scale: 32768.0000 (37954.0508)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0004  max mem: 6186
Epoch: [25]  [22010/40201]  eta: 2:40:41  lr: 0.000003  min_lr: 0.000000  loss: 3.2410 (3.4169)  loss_scale: 65536.0000 (37966.5818)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0005  max mem: 6186
Epoch: [25]  [22020/40201]  eta: 2:40:36  lr: 0.000003  min_lr: 0.000000  loss: 3.2644 (3.4168)  loss_scale: 65536.0000 (37979.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0013  max mem: 6186
[2023-07-24 18:29:37,813] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11010
[2023-07-24 18:29:37,813] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:29:37,816] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11010
[2023-07-24 18:29:37,816] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:29:37,816] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [22030/40201]  eta: 2:40:30  lr: 0.000003  min_lr: 0.000000  loss: 3.0637 (3.4165)  loss_scale: 32768.0000 (37976.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0012  max mem: 6186
Epoch: [25]  [22040/40201]  eta: 2:40:25  lr: 0.000003  min_lr: 0.000000  loss: 3.1085 (3.4165)  loss_scale: 32768.0000 (37974.3729)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0006  max mem: 6186
Epoch: [25]  [22050/40201]  eta: 2:40:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2185 (3.4166)  loss_scale: 32768.0000 (37972.0118)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0317  max mem: 6186
Epoch: [25]  [22060/40201]  eta: 2:40:15  lr: 0.000003  min_lr: 0.000000  loss: 3.4845 (3.4166)  loss_scale: 32768.0000 (37969.6529)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0316  max mem: 6186
Epoch: [25]  [22070/40201]  eta: 2:40:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4845 (3.4167)  loss_scale: 32768.0000 (37967.2961)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0871  max mem: 6186
Epoch: [25]  [22080/40201]  eta: 2:40:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3611 (3.4167)  loss_scale: 32768.0000 (37964.9414)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0874  max mem: 6186
Epoch: [25]  [22090/40201]  eta: 2:40:01  lr: 0.000003  min_lr: 0.000000  loss: 3.3000 (3.4167)  loss_scale: 32768.0000 (37962.5889)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0008  max mem: 6186
Epoch: [25]  [22100/40201]  eta: 2:39:56  lr: 0.000003  min_lr: 0.000000  loss: 2.8304 (3.4165)  loss_scale: 32768.0000 (37960.2385)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0011  max mem: 6186
Epoch: [25]  [22110/40201]  eta: 2:39:51  lr: 0.000003  min_lr: 0.000000  loss: 3.0982 (3.4166)  loss_scale: 32768.0000 (37957.8903)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0014  max mem: 6186
Epoch: [25]  [22120/40201]  eta: 2:39:45  lr: 0.000003  min_lr: 0.000000  loss: 3.3987 (3.4167)  loss_scale: 32768.0000 (37955.5441)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0008  max mem: 6186
Epoch: [25]  [22130/40201]  eta: 2:39:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3008 (3.4166)  loss_scale: 32768.0000 (37953.2001)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0006  max mem: 6186
Epoch: [25]  [22140/40201]  eta: 2:39:32  lr: 0.000003  min_lr: 0.000000  loss: 3.1282 (3.4163)  loss_scale: 32768.0000 (37950.8582)  weight_decay: 0.0500 (0.0500)  time: 0.3743  data: 0.0005  max mem: 6186
Epoch: [25]  [22150/40201]  eta: 2:39:24  lr: 0.000003  min_lr: 0.000000  loss: 3.1083 (3.4162)  loss_scale: 32768.0000 (37948.5184)  weight_decay: 0.0500 (0.0500)  time: 0.2163  data: 0.0014  max mem: 6186
Epoch: [25]  [22160/40201]  eta: 2:39:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3271 (3.4162)  loss_scale: 32768.0000 (37946.1808)  weight_decay: 0.0500 (0.0500)  time: 0.3114  data: 0.1116  max mem: 6186
Epoch: [25]  [22170/40201]  eta: 2:39:12  lr: 0.000003  min_lr: 0.000000  loss: 3.5339 (3.4163)  loss_scale: 32768.0000 (37943.8452)  weight_decay: 0.0500 (0.0500)  time: 0.4440  data: 0.2571  max mem: 6186
Epoch: [25]  [22180/40201]  eta: 2:39:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5339 (3.4164)  loss_scale: 32768.0000 (37941.5117)  weight_decay: 0.0500 (0.0500)  time: 0.3695  data: 0.1860  max mem: 6186
Epoch: [25]  [22190/40201]  eta: 2:38:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3038 (3.4163)  loss_scale: 32768.0000 (37939.1804)  weight_decay: 0.0500 (0.0500)  time: 0.2817  data: 0.0886  max mem: 6186
Epoch: [25]  [22200/40201]  eta: 2:38:50  lr: 0.000003  min_lr: 0.000000  loss: 3.2765 (3.4161)  loss_scale: 32768.0000 (37936.8511)  weight_decay: 0.0500 (0.0500)  time: 0.2423  data: 0.0502  max mem: 6186
Epoch: [25]  [22210/40201]  eta: 2:38:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3870 (3.4164)  loss_scale: 32768.0000 (37934.5240)  weight_decay: 0.0500 (0.0500)  time: 0.3534  data: 0.0291  max mem: 6186
Epoch: [25]  [22220/40201]  eta: 2:38:39  lr: 0.000003  min_lr: 0.000000  loss: 3.9871 (3.4165)  loss_scale: 32768.0000 (37932.1989)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0283  max mem: 6186
Epoch: [25]  [22230/40201]  eta: 2:38:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1973 (3.4165)  loss_scale: 32768.0000 (37929.8759)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0004  max mem: 6186
Epoch: [25]  [22240/40201]  eta: 2:38:28  lr: 0.000003  min_lr: 0.000000  loss: 3.1566 (3.4165)  loss_scale: 32768.0000 (37927.5551)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0005  max mem: 6186
Epoch: [25]  [22250/40201]  eta: 2:38:23  lr: 0.000003  min_lr: 0.000000  loss: 3.1231 (3.4164)  loss_scale: 32768.0000 (37925.2363)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0004  max mem: 6186
Epoch: [25]  [22260/40201]  eta: 2:38:17  lr: 0.000003  min_lr: 0.000000  loss: 3.2912 (3.4165)  loss_scale: 32768.0000 (37922.9195)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0004  max mem: 6186
Epoch: [25]  [22270/40201]  eta: 2:38:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6010 (3.4166)  loss_scale: 32768.0000 (37920.6049)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0005  max mem: 6186
[2023-07-24 18:31:40,136] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:31:40,136] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:31:40,139] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:31:40,139] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [22280/40201]  eta: 2:38:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5231 (3.4166)  loss_scale: 32768.0000 (37921.2337)  weight_decay: 0.0500 (0.0500)  time: 0.4933  data: 0.0005  max mem: 6186
Epoch: [25]  [22290/40201]  eta: 2:38:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5076 (3.4165)  loss_scale: 65536.0000 (37933.6220)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0004  max mem: 6186
Epoch: [25]  [22300/40201]  eta: 2:37:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4546 (3.4165)  loss_scale: 65536.0000 (37945.9992)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [22310/40201]  eta: 2:37:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4546 (3.4166)  loss_scale: 65536.0000 (37958.3653)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0004  max mem: 6186
Epoch: [25]  [22320/40201]  eta: 2:37:44  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4166)  loss_scale: 65536.0000 (37970.7203)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0004  max mem: 6186
Epoch: [25]  [22330/40201]  eta: 2:37:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4889 (3.4167)  loss_scale: 65536.0000 (37983.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0012  max mem: 6186
Epoch: [25]  [22340/40201]  eta: 2:37:33  lr: 0.000003  min_lr: 0.000000  loss: 3.4889 (3.4167)  loss_scale: 65536.0000 (37995.3972)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0012  max mem: 6186
Epoch: [25]  [22350/40201]  eta: 2:37:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7226 (3.4170)  loss_scale: 65536.0000 (38007.7190)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0004  max mem: 6186
Epoch: [25]  [22360/40201]  eta: 2:37:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7226 (3.4170)  loss_scale: 65536.0000 (38020.0299)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0004  max mem: 6186
Epoch: [25]  [22370/40201]  eta: 2:37:16  lr: 0.000003  min_lr: 0.000000  loss: 3.0447 (3.4169)  loss_scale: 65536.0000 (38032.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0004  max mem: 6186
Epoch: [25]  [22380/40201]  eta: 2:37:11  lr: 0.000003  min_lr: 0.000000  loss: 3.2397 (3.4170)  loss_scale: 65536.0000 (38044.6186)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0005  max mem: 6186
Epoch: [25]  [22390/40201]  eta: 2:37:05  lr: 0.000003  min_lr: 0.000000  loss: 3.4327 (3.4170)  loss_scale: 65536.0000 (38056.8964)  weight_decay: 0.0500 (0.0500)  time: 0.4943  data: 0.0004  max mem: 6186
Epoch: [25]  [22400/40201]  eta: 2:36:59  lr: 0.000003  min_lr: 0.000000  loss: 3.2547 (3.4169)  loss_scale: 65536.0000 (38069.1633)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0004  max mem: 6186
Epoch: [25]  [22410/40201]  eta: 2:36:54  lr: 0.000003  min_lr: 0.000000  loss: 3.4494 (3.4170)  loss_scale: 65536.0000 (38081.4193)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0005  max mem: 6186
Epoch: [25]  [22420/40201]  eta: 2:36:48  lr: 0.000003  min_lr: 0.000000  loss: 3.5049 (3.4170)  loss_scale: 65536.0000 (38093.6643)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0004  max mem: 6186
Epoch: [25]  [22430/40201]  eta: 2:36:43  lr: 0.000003  min_lr: 0.000000  loss: 3.4782 (3.4171)  loss_scale: 65536.0000 (38105.8984)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0004  max mem: 6186
Epoch: [25]  [22440/40201]  eta: 2:36:38  lr: 0.000003  min_lr: 0.000000  loss: 3.4645 (3.4171)  loss_scale: 65536.0000 (38118.1217)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0004  max mem: 6186
Epoch: [25]  [22450/40201]  eta: 2:36:32  lr: 0.000003  min_lr: 0.000000  loss: 3.5757 (3.4172)  loss_scale: 65536.0000 (38130.3340)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0004  max mem: 6186
Epoch: [25]  [22460/40201]  eta: 2:36:24  lr: 0.000003  min_lr: 0.000000  loss: 3.4657 (3.4172)  loss_scale: 65536.0000 (38142.5354)  weight_decay: 0.0500 (0.0500)  time: 0.2869  data: 0.0009  max mem: 6186
Epoch: [25]  [22470/40201]  eta: 2:36:16  lr: 0.000003  min_lr: 0.000000  loss: 3.3487 (3.4172)  loss_scale: 65536.0000 (38154.7260)  weight_decay: 0.0500 (0.0500)  time: 0.1827  data: 0.0009  max mem: 6186
[2023-07-24 18:33:10,374] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11237
[2023-07-24 18:33:10,374] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11237
[2023-07-24 18:33:10,375] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:33:10,375] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:33:10,375] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [22480/40201]  eta: 2:36:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5530 (3.4172)  loss_scale: 65536.0000 (38158.1602)  weight_decay: 0.0500 (0.0500)  time: 0.2349  data: 0.0276  max mem: 6186
Epoch: [25]  [22490/40201]  eta: 2:36:02  lr: 0.000003  min_lr: 0.000000  loss: 3.0012 (3.4170)  loss_scale: 32768.0000 (38155.7636)  weight_decay: 0.0500 (0.0500)  time: 0.3458  data: 0.1325  max mem: 6186
Epoch: [25]  [22500/40201]  eta: 2:35:55  lr: 0.000003  min_lr: 0.000000  loss: 2.9682 (3.4170)  loss_scale: 32768.0000 (38153.3692)  weight_decay: 0.0500 (0.0500)  time: 0.3457  data: 0.1616  max mem: 6186
Epoch: [25]  [22510/40201]  eta: 2:35:48  lr: 0.000003  min_lr: 0.000000  loss: 3.4548 (3.4171)  loss_scale: 32768.0000 (38150.9769)  weight_decay: 0.0500 (0.0500)  time: 0.2883  data: 0.1061  max mem: 6186
Epoch: [25]  [22520/40201]  eta: 2:35:40  lr: 0.000003  min_lr: 0.000000  loss: 3.5056 (3.4172)  loss_scale: 32768.0000 (38148.5867)  weight_decay: 0.0500 (0.0500)  time: 0.2619  data: 0.0791  max mem: 6186
Epoch: [25]  [22530/40201]  eta: 2:35:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6075 (3.4174)  loss_scale: 32768.0000 (38146.1986)  weight_decay: 0.0500 (0.0500)  time: 0.4255  data: 0.0820  max mem: 6186
[2023-07-24 18:33:33,707] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11268
[2023-07-24 18:33:33,707] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-07-24 18:33:33,713] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11268
[2023-07-24 18:33:33,714] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-07-24 18:33:33,714] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [22540/40201]  eta: 2:35:30  lr: 0.000003  min_lr: 0.000000  loss: 3.6264 (3.4175)  loss_scale: 32768.0000 (38140.9052)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0528  max mem: 6186
Epoch: [25]  [22550/40201]  eta: 2:35:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6263 (3.4176)  loss_scale: 16384.0000 (38131.2573)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0004  max mem: 6186
Epoch: [25]  [22560/40201]  eta: 2:35:19  lr: 0.000003  min_lr: 0.000000  loss: 3.6804 (3.4177)  loss_scale: 16384.0000 (38121.6180)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0004  max mem: 6186
Epoch: [25]  [22570/40201]  eta: 2:35:14  lr: 0.000003  min_lr: 0.000000  loss: 3.7622 (3.4179)  loss_scale: 16384.0000 (38111.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0004  max mem: 6186
Epoch: [25]  [22580/40201]  eta: 2:35:08  lr: 0.000003  min_lr: 0.000000  loss: 3.8992 (3.4180)  loss_scale: 16384.0000 (38102.3650)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0005  max mem: 6186
Epoch: [25]  [22590/40201]  eta: 2:35:02  lr: 0.000003  min_lr: 0.000000  loss: 3.3401 (3.4178)  loss_scale: 16384.0000 (38092.7513)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0005  max mem: 6186
Epoch: [25]  [22600/40201]  eta: 2:34:57  lr: 0.000003  min_lr: 0.000000  loss: 3.2466 (3.4179)  loss_scale: 16384.0000 (38083.1461)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0004  max mem: 6186
Epoch: [25]  [22610/40201]  eta: 2:34:52  lr: 0.000003  min_lr: 0.000000  loss: 3.4526 (3.4180)  loss_scale: 16384.0000 (38073.5493)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0004  max mem: 6186
Epoch: [25]  [22620/40201]  eta: 2:34:46  lr: 0.000003  min_lr: 0.000000  loss: 3.1400 (3.4180)  loss_scale: 16384.0000 (38063.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0004  max mem: 6186
Epoch: [25]  [22630/40201]  eta: 2:34:41  lr: 0.000003  min_lr: 0.000000  loss: 3.1540 (3.4181)  loss_scale: 16384.0000 (38054.3813)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0004  max mem: 6186
Epoch: [25]  [22640/40201]  eta: 2:34:35  lr: 0.000003  min_lr: 0.000000  loss: 3.7018 (3.4183)  loss_scale: 16384.0000 (38044.8100)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0011  max mem: 6186
Epoch: [25]  [22650/40201]  eta: 2:34:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7018 (3.4184)  loss_scale: 16384.0000 (38035.2472)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0018  max mem: 6186
Epoch: [25]  [22660/40201]  eta: 2:34:24  lr: 0.000003  min_lr: 0.000000  loss: 3.8470 (3.4186)  loss_scale: 16384.0000 (38025.6928)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0018  max mem: 6186
Epoch: [25]  [22670/40201]  eta: 2:34:19  lr: 0.000003  min_lr: 0.000000  loss: 3.3039 (3.4184)  loss_scale: 16384.0000 (38016.1468)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0011  max mem: 6186
Epoch: [25]  [22680/40201]  eta: 2:34:13  lr: 0.000003  min_lr: 0.000000  loss: 2.9582 (3.4184)  loss_scale: 16384.0000 (38006.6092)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0005  max mem: 6186
Epoch: [25]  [22690/40201]  eta: 2:34:08  lr: 0.000003  min_lr: 0.000000  loss: 2.9582 (3.4182)  loss_scale: 16384.0000 (37997.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0005  max mem: 6186
Epoch: [25]  [22700/40201]  eta: 2:34:02  lr: 0.000003  min_lr: 0.000000  loss: 2.9929 (3.4180)  loss_scale: 16384.0000 (37987.5593)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0004  max mem: 6186
Epoch: [25]  [22710/40201]  eta: 2:33:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3464 (3.4180)  loss_scale: 16384.0000 (37978.0469)  weight_decay: 0.0500 (0.0500)  time: 0.4925  data: 0.0004  max mem: 6186
Epoch: [25]  [22720/40201]  eta: 2:33:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3464 (3.4180)  loss_scale: 16384.0000 (37968.5429)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0004  max mem: 6186
Epoch: [25]  [22730/40201]  eta: 2:33:46  lr: 0.000003  min_lr: 0.000000  loss: 3.4431 (3.4180)  loss_scale: 16384.0000 (37959.0473)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0004  max mem: 6186
Epoch: [25]  [22740/40201]  eta: 2:33:40  lr: 0.000003  min_lr: 0.000000  loss: 3.3922 (3.4179)  loss_scale: 16384.0000 (37949.5600)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0005  max mem: 6186
Epoch: [25]  [22750/40201]  eta: 2:33:35  lr: 0.000003  min_lr: 0.000000  loss: 3.2221 (3.4178)  loss_scale: 16384.0000 (37940.0811)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0017  max mem: 6186
Epoch: [25]  [22760/40201]  eta: 2:33:29  lr: 0.000003  min_lr: 0.000000  loss: 3.1266 (3.4178)  loss_scale: 16384.0000 (37930.6104)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0016  max mem: 6186
Epoch: [25]  [22770/40201]  eta: 2:33:24  lr: 0.000003  min_lr: 0.000000  loss: 3.0455 (3.4175)  loss_scale: 16384.0000 (37921.1481)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0004  max mem: 6186
Epoch: [25]  [22780/40201]  eta: 2:33:18  lr: 0.000003  min_lr: 0.000000  loss: 2.9461 (3.4175)  loss_scale: 16384.0000 (37911.6941)  weight_decay: 0.0500 (0.0500)  time: 0.4975  data: 0.0008  max mem: 6186
Epoch: [25]  [22790/40201]  eta: 2:33:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5023 (3.4176)  loss_scale: 16384.0000 (37902.2484)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0009  max mem: 6186
[2023-07-24 18:35:42,862] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:35:42,863] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-07-24 18:35:42,878] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:35:42,878] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [22800/40201]  eta: 2:33:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5023 (3.4177)  loss_scale: 16384.0000 (37897.1224)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0004  max mem: 6186
Epoch: [25]  [22810/40201]  eta: 2:33:02  lr: 0.000003  min_lr: 0.000000  loss: 3.2180 (3.4176)  loss_scale: 32768.0000 (37894.8739)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0004  max mem: 6186
Epoch: [25]  [22820/40201]  eta: 2:32:56  lr: 0.000003  min_lr: 0.000000  loss: 3.1291 (3.4178)  loss_scale: 32768.0000 (37892.6273)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0004  max mem: 6186
Epoch: [25]  [22830/40201]  eta: 2:32:51  lr: 0.000003  min_lr: 0.000000  loss: 3.2001 (3.4176)  loss_scale: 32768.0000 (37890.3827)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0004  max mem: 6186
Epoch: [25]  [22840/40201]  eta: 2:32:45  lr: 0.000003  min_lr: 0.000000  loss: 2.8477 (3.4174)  loss_scale: 32768.0000 (37888.1401)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0004  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/Hm8X9u8jtOk_000022_000032.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/Hm8X9u8jtOk_000022_000032.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [22850/40201]  eta: 2:32:40  lr: 0.000003  min_lr: 0.000000  loss: 3.2784 (3.4176)  loss_scale: 32768.0000 (37885.8994)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0013  max mem: 6186
Epoch: [25]  [22860/40201]  eta: 2:32:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6047 (3.4176)  loss_scale: 32768.0000 (37883.6607)  weight_decay: 0.0500 (0.0500)  time: 0.4985  data: 0.0013  max mem: 6186
Epoch: [25]  [22870/40201]  eta: 2:32:29  lr: 0.000003  min_lr: 0.000000  loss: 3.3008 (3.4174)  loss_scale: 32768.0000 (37881.4240)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0004  max mem: 6186
Epoch: [25]  [22880/40201]  eta: 2:32:23  lr: 0.000003  min_lr: 0.000000  loss: 2.8627 (3.4172)  loss_scale: 32768.0000 (37879.1892)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0015  max mem: 6186
Epoch: [25]  [22890/40201]  eta: 2:32:18  lr: 0.000003  min_lr: 0.000000  loss: 3.0683 (3.4171)  loss_scale: 32768.0000 (37876.9564)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0015  max mem: 6186
Epoch: [25]  [22900/40201]  eta: 2:32:12  lr: 0.000003  min_lr: 0.000000  loss: 3.4881 (3.4172)  loss_scale: 32768.0000 (37874.7255)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0004  max mem: 6186
Epoch: [25]  [22910/40201]  eta: 2:32:07  lr: 0.000003  min_lr: 0.000000  loss: 3.3180 (3.4172)  loss_scale: 32768.0000 (37872.4965)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0004  max mem: 6186
Epoch: [25]  [22920/40201]  eta: 2:32:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4841 (3.4174)  loss_scale: 32768.0000 (37870.2695)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0013  max mem: 6186
Epoch: [25]  [22930/40201]  eta: 2:31:56  lr: 0.000003  min_lr: 0.000000  loss: 3.3118 (3.4173)  loss_scale: 32768.0000 (37868.0445)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0014  max mem: 6186
Epoch: [25]  [22940/40201]  eta: 2:31:50  lr: 0.000003  min_lr: 0.000000  loss: 3.1996 (3.4173)  loss_scale: 32768.0000 (37865.8214)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0014  max mem: 6186
Epoch: [25]  [22950/40201]  eta: 2:31:45  lr: 0.000003  min_lr: 0.000000  loss: 3.4306 (3.4175)  loss_scale: 32768.0000 (37863.6002)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0014  max mem: 6186
Epoch: [25]  [22960/40201]  eta: 2:31:39  lr: 0.000003  min_lr: 0.000000  loss: 3.3758 (3.4175)  loss_scale: 32768.0000 (37861.3810)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0004  max mem: 6186
Epoch: [25]  [22970/40201]  eta: 2:31:34  lr: 0.000003  min_lr: 0.000000  loss: 3.3146 (3.4176)  loss_scale: 32768.0000 (37859.1636)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0004  max mem: 6186
Epoch: [25]  [22980/40201]  eta: 2:31:28  lr: 0.000003  min_lr: 0.000000  loss: 3.7002 (3.4177)  loss_scale: 32768.0000 (37856.9483)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0004  max mem: 6186
Epoch: [25]  [22990/40201]  eta: 2:31:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6550 (3.4178)  loss_scale: 32768.0000 (37854.7348)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0004  max mem: 6186
[2023-07-24 18:37:24,873] [INFO] [timer.py:181:stop] 0/23000, SamplesPerSec=13.318147590378391
Epoch: [25]  [23000/40201]  eta: 2:31:17  lr: 0.000003  min_lr: 0.000000  loss: 3.6550 (3.4178)  loss_scale: 32768.0000 (37852.5233)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0004  max mem: 6186
Epoch: [25]  [23010/40201]  eta: 2:31:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7064 (3.4181)  loss_scale: 32768.0000 (37850.3137)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0005  max mem: 6186
Epoch: [25]  [23020/40201]  eta: 2:31:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5062 (3.4180)  loss_scale: 32768.0000 (37848.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0005  max mem: 6186
Epoch: [25]  [23030/40201]  eta: 2:31:01  lr: 0.000003  min_lr: 0.000000  loss: 2.8974 (3.4178)  loss_scale: 32768.0000 (37845.9002)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0004  max mem: 6186
Epoch: [25]  [23040/40201]  eta: 2:30:55  lr: 0.000003  min_lr: 0.000000  loss: 3.0110 (3.4179)  loss_scale: 32768.0000 (37843.6964)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0004  max mem: 6186
Epoch: [25]  [23050/40201]  eta: 2:30:50  lr: 0.000003  min_lr: 0.000000  loss: 3.3221 (3.4178)  loss_scale: 32768.0000 (37841.4944)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0004  max mem: 6186
[2023-07-24 18:37:50,821] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:37:50,821] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:37:50,828] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:37:50,828] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [23060/40201]  eta: 2:30:44  lr: 0.000003  min_lr: 0.000000  loss: 2.8606 (3.4176)  loss_scale: 32768.0000 (37853.5037)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0004  max mem: 6186
Epoch: [25]  [23070/40201]  eta: 2:30:39  lr: 0.000003  min_lr: 0.000000  loss: 3.0835 (3.4176)  loss_scale: 65536.0000 (37865.5025)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0005  max mem: 6186
Epoch: [25]  [23080/40201]  eta: 2:30:33  lr: 0.000003  min_lr: 0.000000  loss: 3.1041 (3.4174)  loss_scale: 65536.0000 (37877.4909)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0005  max mem: 6186
Epoch: [25]  [23090/40201]  eta: 2:30:28  lr: 0.000003  min_lr: 0.000000  loss: 2.8832 (3.4173)  loss_scale: 65536.0000 (37889.4690)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0005  max mem: 6186
[2023-07-24 18:38:13,598] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11548
[2023-07-24 18:38:13,598] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:38:13,613] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11548
[2023-07-24 18:38:13,613] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:38:13,613] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [23100/40201]  eta: 2:30:22  lr: 0.000003  min_lr: 0.000000  loss: 3.4589 (3.4174)  loss_scale: 65536.0000 (37895.7628)  weight_decay: 0.0500 (0.0500)  time: 0.4812  data: 0.0004  max mem: 6186
Epoch: [25]  [23110/40201]  eta: 2:30:17  lr: 0.000003  min_lr: 0.000000  loss: 3.7987 (3.4174)  loss_scale: 32768.0000 (37893.5440)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0004  max mem: 6186
Epoch: [25]  [23120/40201]  eta: 2:30:11  lr: 0.000003  min_lr: 0.000000  loss: 3.8136 (3.4176)  loss_scale: 32768.0000 (37891.3272)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0004  max mem: 6186
Epoch: [25]  [23130/40201]  eta: 2:30:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3011 (3.4174)  loss_scale: 32768.0000 (37889.1123)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0004  max mem: 6186
Epoch: [25]  [23140/40201]  eta: 2:30:00  lr: 0.000003  min_lr: 0.000000  loss: 3.2686 (3.4174)  loss_scale: 32768.0000 (37886.8993)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0004  max mem: 6186
Epoch: [25]  [23150/40201]  eta: 2:29:55  lr: 0.000003  min_lr: 0.000000  loss: 3.2582 (3.4173)  loss_scale: 32768.0000 (37884.6882)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0004  max mem: 6186
Epoch: [25]  [23160/40201]  eta: 2:29:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4563 (3.4175)  loss_scale: 32768.0000 (37882.4790)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0004  max mem: 6186
Epoch: [25]  [23170/40201]  eta: 2:29:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3737 (3.4173)  loss_scale: 32768.0000 (37880.2717)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0004  max mem: 6186
Epoch: [25]  [23180/40201]  eta: 2:29:38  lr: 0.000003  min_lr: 0.000000  loss: 2.9352 (3.4172)  loss_scale: 32768.0000 (37878.0663)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0004  max mem: 6186
Epoch: [25]  [23190/40201]  eta: 2:29:33  lr: 0.000003  min_lr: 0.000000  loss: 2.7642 (3.4171)  loss_scale: 32768.0000 (37875.8629)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0004  max mem: 6186
Epoch: [25]  [23200/40201]  eta: 2:29:27  lr: 0.000003  min_lr: 0.000000  loss: 3.3326 (3.4171)  loss_scale: 32768.0000 (37873.6613)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0004  max mem: 6186
Epoch: [25]  [23210/40201]  eta: 2:29:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5255 (3.4172)  loss_scale: 32768.0000 (37871.4616)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0004  max mem: 6186
Epoch: [25]  [23220/40201]  eta: 2:29:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5383 (3.4174)  loss_scale: 32768.0000 (37869.2639)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0012  max mem: 6186
Epoch: [25]  [23230/40201]  eta: 2:29:11  lr: 0.000003  min_lr: 0.000000  loss: 3.3100 (3.4172)  loss_scale: 32768.0000 (37867.0680)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0019  max mem: 6186
Epoch: [25]  [23240/40201]  eta: 2:29:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3100 (3.4172)  loss_scale: 32768.0000 (37864.8740)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0011  max mem: 6186
Epoch: [25]  [23250/40201]  eta: 2:28:58  lr: 0.000003  min_lr: 0.000000  loss: 3.3632 (3.4172)  loss_scale: 32768.0000 (37862.6819)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0004  max mem: 6186
Epoch: [25]  [23260/40201]  eta: 2:28:51  lr: 0.000003  min_lr: 0.000000  loss: 3.1004 (3.4171)  loss_scale: 32768.0000 (37860.4916)  weight_decay: 0.0500 (0.0500)  time: 0.2230  data: 0.0013  max mem: 6186
Epoch: [25]  [23270/40201]  eta: 2:28:44  lr: 0.000003  min_lr: 0.000000  loss: 3.3151 (3.4172)  loss_scale: 32768.0000 (37858.3033)  weight_decay: 0.0500 (0.0500)  time: 0.2583  data: 0.0690  max mem: 6186
Epoch: [25]  [23280/40201]  eta: 2:28:39  lr: 0.000003  min_lr: 0.000000  loss: 3.6379 (3.4173)  loss_scale: 32768.0000 (37856.1168)  weight_decay: 0.0500 (0.0500)  time: 0.4637  data: 0.2660  max mem: 6186
Epoch: [25]  [23290/40201]  eta: 2:28:32  lr: 0.000003  min_lr: 0.000000  loss: 3.0982 (3.4170)  loss_scale: 32768.0000 (37853.9322)  weight_decay: 0.0500 (0.0500)  time: 0.4289  data: 0.2441  max mem: 6186
Epoch: [25]  [23300/40201]  eta: 2:28:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0264 (3.4169)  loss_scale: 32768.0000 (37851.7495)  weight_decay: 0.0500 (0.0500)  time: 0.2716  data: 0.0974  max mem: 6186
Epoch: [25]  [23310/40201]  eta: 2:28:20  lr: 0.000003  min_lr: 0.000000  loss: 3.1714 (3.4168)  loss_scale: 32768.0000 (37849.5687)  weight_decay: 0.0500 (0.0500)  time: 0.4129  data: 0.1026  max mem: 6186
Epoch: [25]  [23320/40201]  eta: 2:28:14  lr: 0.000003  min_lr: 0.000000  loss: 3.1991 (3.4168)  loss_scale: 32768.0000 (37847.3897)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0523  max mem: 6186
Epoch: [25]  [23330/40201]  eta: 2:28:09  lr: 0.000003  min_lr: 0.000000  loss: 3.3145 (3.4168)  loss_scale: 32768.0000 (37845.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0012  max mem: 6186
Epoch: [25]  [23340/40201]  eta: 2:28:03  lr: 0.000003  min_lr: 0.000000  loss: 3.3427 (3.4167)  loss_scale: 32768.0000 (37843.0374)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0004  max mem: 6186
Epoch: [25]  [23350/40201]  eta: 2:27:58  lr: 0.000003  min_lr: 0.000000  loss: 3.0611 (3.4166)  loss_scale: 32768.0000 (37840.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0005  max mem: 6186
[2023-07-24 18:40:12,379] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:40:12,379] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:40:12,383] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:40:12,383] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [23360/40201]  eta: 2:27:53  lr: 0.000003  min_lr: 0.000000  loss: 2.9265 (3.4164)  loss_scale: 32768.0000 (37847.1086)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0007  max mem: 6186
Epoch: [25]  [23370/40201]  eta: 2:27:48  lr: 0.000003  min_lr: 0.000000  loss: 3.0041 (3.4164)  loss_scale: 65536.0000 (37858.9561)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0008  max mem: 6186
Epoch: [25]  [23380/40201]  eta: 2:27:43  lr: 0.000003  min_lr: 0.000000  loss: 3.2667 (3.4164)  loss_scale: 65536.0000 (37870.7936)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0007  max mem: 6186
[2023-07-24 18:40:31,577] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11693
[2023-07-24 18:40:31,577] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:40:31,584] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11693
[2023-07-24 18:40:31,584] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:40:31,584] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [23390/40201]  eta: 2:27:39  lr: 0.000003  min_lr: 0.000000  loss: 3.2274 (3.4164)  loss_scale: 65536.0000 (37877.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0008  max mem: 6186
Epoch: [25]  [23400/40201]  eta: 2:27:34  lr: 0.000003  min_lr: 0.000000  loss: 3.2274 (3.4164)  loss_scale: 32768.0000 (37874.8341)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0009  max mem: 6186
Epoch: [25]  [23410/40201]  eta: 2:27:29  lr: 0.000003  min_lr: 0.000000  loss: 2.9873 (3.4162)  loss_scale: 32768.0000 (37872.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0006  max mem: 6186
Epoch: [25]  [23420/40201]  eta: 2:27:25  lr: 0.000003  min_lr: 0.000000  loss: 3.0919 (3.4162)  loss_scale: 32768.0000 (37870.4732)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0010  max mem: 6186
Epoch: [25]  [23430/40201]  eta: 2:27:20  lr: 0.000003  min_lr: 0.000000  loss: 3.2784 (3.4161)  loss_scale: 32768.0000 (37868.2955)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0011  max mem: 6186
Epoch: [25]  [23440/40201]  eta: 2:27:15  lr: 0.000003  min_lr: 0.000000  loss: 2.9088 (3.4160)  loss_scale: 32768.0000 (37866.1197)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0008  max mem: 6186
Epoch: [25]  [23450/40201]  eta: 2:27:11  lr: 0.000003  min_lr: 0.000000  loss: 2.9625 (3.4162)  loss_scale: 32768.0000 (37863.9458)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0010  max mem: 6186
Epoch: [25]  [23460/40201]  eta: 2:27:08  lr: 0.000003  min_lr: 0.000000  loss: 3.8338 (3.4164)  loss_scale: 32768.0000 (37861.7737)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.0011  max mem: 6186
Epoch: [25]  [23470/40201]  eta: 2:27:03  lr: 0.000003  min_lr: 0.000000  loss: 3.9392 (3.4165)  loss_scale: 32768.0000 (37859.6034)  weight_decay: 0.0500 (0.0500)  time: 0.7322  data: 0.0008  max mem: 6186
Epoch: [25]  [23480/40201]  eta: 2:27:01  lr: 0.000003  min_lr: 0.000000  loss: 3.4451 (3.4165)  loss_scale: 32768.0000 (37857.4350)  weight_decay: 0.0500 (0.0500)  time: 0.7731  data: 0.1624  max mem: 6186
Epoch: [25]  [23490/40201]  eta: 2:26:56  lr: 0.000003  min_lr: 0.000000  loss: 3.4451 (3.4166)  loss_scale: 32768.0000 (37855.2685)  weight_decay: 0.0500 (0.0500)  time: 0.7750  data: 0.1624  max mem: 6186
Epoch: [25]  [23500/40201]  eta: 2:26:51  lr: 0.000003  min_lr: 0.000000  loss: 2.8722 (3.4163)  loss_scale: 32768.0000 (37853.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0006  max mem: 6186
Epoch: [25]  [23510/40201]  eta: 2:26:45  lr: 0.000003  min_lr: 0.000000  loss: 2.8722 (3.4162)  loss_scale: 32768.0000 (37850.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0014  max mem: 6186
Epoch: [25]  [23520/40201]  eta: 2:26:38  lr: 0.000003  min_lr: 0.000000  loss: 3.3016 (3.4162)  loss_scale: 32768.0000 (37848.7799)  weight_decay: 0.0500 (0.0500)  time: 0.3461  data: 0.0014  max mem: 6186
Epoch: [25]  [23530/40201]  eta: 2:26:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2747 (3.4163)  loss_scale: 32768.0000 (37846.6207)  weight_decay: 0.0500 (0.0500)  time: 0.3249  data: 0.0809  max mem: 6186
Epoch: [25]  [23540/40201]  eta: 2:26:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2747 (3.4163)  loss_scale: 32768.0000 (37844.4634)  weight_decay: 0.0500 (0.0500)  time: 0.4031  data: 0.1470  max mem: 6186
Epoch: [25]  [23550/40201]  eta: 2:26:20  lr: 0.000003  min_lr: 0.000000  loss: 3.4181 (3.4163)  loss_scale: 32768.0000 (37842.3078)  weight_decay: 0.0500 (0.0500)  time: 0.4272  data: 0.1482  max mem: 6186
Epoch: [25]  [23560/40201]  eta: 2:26:13  lr: 0.000003  min_lr: 0.000000  loss: 3.7353 (3.4165)  loss_scale: 32768.0000 (37840.1542)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.1259  max mem: 6186
Epoch: [25]  [23570/40201]  eta: 2:26:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5065 (3.4164)  loss_scale: 32768.0000 (37838.0023)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.2017  max mem: 6186
Epoch: [25]  [23580/40201]  eta: 2:26:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4753 (3.4165)  loss_scale: 32768.0000 (37835.8523)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1590  max mem: 6186
Epoch: [25]  [23590/40201]  eta: 2:25:59  lr: 0.000003  min_lr: 0.000000  loss: 3.4050 (3.4164)  loss_scale: 32768.0000 (37833.7040)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0022  max mem: 6186
Epoch: [25]  [23600/40201]  eta: 2:25:54  lr: 0.000003  min_lr: 0.000000  loss: 3.3605 (3.4163)  loss_scale: 32768.0000 (37831.5576)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0012  max mem: 6186
Epoch: [25]  [23610/40201]  eta: 2:25:49  lr: 0.000003  min_lr: 0.000000  loss: 3.4296 (3.4164)  loss_scale: 32768.0000 (37829.4131)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0017  max mem: 6186
Epoch: [25]  [23620/40201]  eta: 2:25:45  lr: 0.000003  min_lr: 0.000000  loss: 3.4296 (3.4164)  loss_scale: 32768.0000 (37827.2703)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0017  max mem: 6186
Epoch: [25]  [23630/40201]  eta: 2:25:40  lr: 0.000003  min_lr: 0.000000  loss: 2.8061 (3.4161)  loss_scale: 32768.0000 (37825.1294)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0013  max mem: 6186
Epoch: [25]  [23640/40201]  eta: 2:25:35  lr: 0.000003  min_lr: 0.000000  loss: 2.9417 (3.4163)  loss_scale: 32768.0000 (37822.9902)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0017  max mem: 6186
[2023-07-24 18:42:59,908] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:42:59,909] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:42:59,916] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:42:59,916] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [23650/40201]  eta: 2:25:30  lr: 0.000003  min_lr: 0.000000  loss: 3.3311 (3.4161)  loss_scale: 32768.0000 (37829.1658)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0011  max mem: 6186
Epoch: [25]  [23660/40201]  eta: 2:25:26  lr: 0.000003  min_lr: 0.000000  loss: 3.2188 (3.4162)  loss_scale: 65536.0000 (37840.8757)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0008  max mem: 6186
Epoch: [25]  [23670/40201]  eta: 2:25:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5307 (3.4162)  loss_scale: 65536.0000 (37852.5757)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0010  max mem: 6186
Epoch: [25]  [23680/40201]  eta: 2:25:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7631 (3.4164)  loss_scale: 65536.0000 (37864.2659)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0012  max mem: 6186
Epoch: [25]  [23690/40201]  eta: 2:25:11  lr: 0.000003  min_lr: 0.000000  loss: 3.5694 (3.4163)  loss_scale: 65536.0000 (37875.9461)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0008  max mem: 6186
Epoch: [25]  [23700/40201]  eta: 2:25:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5694 (3.4164)  loss_scale: 65536.0000 (37887.6166)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0014  max mem: 6186
[2023-07-24 18:43:36,840] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11853
[2023-07-24 18:43:36,840] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:43:36,840] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:43:36,841] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 11853
[2023-07-24 18:43:36,842] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [23710/40201]  eta: 2:25:01  lr: 0.000002  min_lr: 0.000000  loss: 3.2402 (3.4162)  loss_scale: 65536.0000 (37893.7492)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0016  max mem: 6186
Epoch: [25]  [23720/40201]  eta: 2:24:56  lr: 0.000002  min_lr: 0.000000  loss: 2.9249 (3.4162)  loss_scale: 32768.0000 (37891.5884)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0016  max mem: 6186
Epoch: [25]  [23730/40201]  eta: 2:24:52  lr: 0.000002  min_lr: 0.000000  loss: 3.0911 (3.4160)  loss_scale: 32768.0000 (37889.4294)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.0016  max mem: 6186
Epoch: [25]  [23740/40201]  eta: 2:24:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3507 (3.4161)  loss_scale: 32768.0000 (37887.2721)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0009  max mem: 6186
Epoch: [25]  [23750/40201]  eta: 2:24:43  lr: 0.000002  min_lr: 0.000000  loss: 3.3507 (3.4160)  loss_scale: 32768.0000 (37885.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0013  max mem: 6186
Epoch: [25]  [23760/40201]  eta: 2:24:38  lr: 0.000002  min_lr: 0.000000  loss: 3.3435 (3.4160)  loss_scale: 32768.0000 (37882.9632)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0014  max mem: 6186
Epoch: [25]  [23770/40201]  eta: 2:24:32  lr: 0.000002  min_lr: 0.000000  loss: 3.5368 (3.4160)  loss_scale: 32768.0000 (37880.8114)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0008  max mem: 6186
Epoch: [25]  [23780/40201]  eta: 2:24:25  lr: 0.000002  min_lr: 0.000000  loss: 3.3060 (3.4158)  loss_scale: 32768.0000 (37878.6615)  weight_decay: 0.0500 (0.0500)  time: 0.3584  data: 0.0190  max mem: 6186
Epoch: [25]  [23790/40201]  eta: 2:24:18  lr: 0.000002  min_lr: 0.000000  loss: 3.1426 (3.4158)  loss_scale: 32768.0000 (37876.5133)  weight_decay: 0.0500 (0.0500)  time: 0.3133  data: 0.0465  max mem: 6186
Epoch: [25]  [23800/40201]  eta: 2:24:13  lr: 0.000002  min_lr: 0.000000  loss: 3.3838 (3.4157)  loss_scale: 32768.0000 (37874.3670)  weight_decay: 0.0500 (0.0500)  time: 0.3845  data: 0.1404  max mem: 6186
Epoch: [25]  [23810/40201]  eta: 2:24:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5122 (3.4158)  loss_scale: 32768.0000 (37872.2224)  weight_decay: 0.0500 (0.0500)  time: 0.4110  data: 0.1814  max mem: 6186
Epoch: [25]  [23820/40201]  eta: 2:24:01  lr: 0.000002  min_lr: 0.000000  loss: 3.3371 (3.4158)  loss_scale: 32768.0000 (37870.0797)  weight_decay: 0.0500 (0.0500)  time: 0.4638  data: 0.2458  max mem: 6186
Epoch: [25]  [23830/40201]  eta: 2:23:56  lr: 0.000002  min_lr: 0.000000  loss: 3.3371 (3.4159)  loss_scale: 32768.0000 (37867.9387)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.2734  max mem: 6186
Epoch: [25]  [23840/40201]  eta: 2:23:51  lr: 0.000002  min_lr: 0.000000  loss: 3.4773 (3.4159)  loss_scale: 32768.0000 (37865.7996)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0966  max mem: 6186
Epoch: [25]  [23850/40201]  eta: 2:23:46  lr: 0.000002  min_lr: 0.000000  loss: 3.2176 (3.4158)  loss_scale: 32768.0000 (37863.6622)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0005  max mem: 6186
Epoch: [25]  [23860/40201]  eta: 2:23:41  lr: 0.000002  min_lr: 0.000000  loss: 3.1648 (3.4159)  loss_scale: 32768.0000 (37861.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0006  max mem: 6186
Epoch: [25]  [23870/40201]  eta: 2:23:36  lr: 0.000002  min_lr: 0.000000  loss: 3.2935 (3.4158)  loss_scale: 32768.0000 (37859.3929)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0007  max mem: 6186
Epoch: [25]  [23880/40201]  eta: 2:23:32  lr: 0.000002  min_lr: 0.000000  loss: 3.2467 (3.4159)  loss_scale: 32768.0000 (37857.2609)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0008  max mem: 6186
Epoch: [25]  [23890/40201]  eta: 2:23:27  lr: 0.000002  min_lr: 0.000000  loss: 3.5063 (3.4159)  loss_scale: 32768.0000 (37855.1307)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0023  max mem: 6186
Epoch: [25]  [23900/40201]  eta: 2:23:23  lr: 0.000002  min_lr: 0.000000  loss: 3.5063 (3.4158)  loss_scale: 32768.0000 (37853.0023)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.0021  max mem: 6186
Epoch: [25]  [23910/40201]  eta: 2:23:18  lr: 0.000002  min_lr: 0.000000  loss: 3.1823 (3.4157)  loss_scale: 32768.0000 (37850.8757)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0007  max mem: 6186
Epoch: [25]  [23920/40201]  eta: 2:23:13  lr: 0.000002  min_lr: 0.000000  loss: 3.2912 (3.4158)  loss_scale: 32768.0000 (37848.7508)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0010  max mem: 6186
Epoch: [25]  [23930/40201]  eta: 2:23:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5898 (3.4159)  loss_scale: 32768.0000 (37846.6277)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0010  max mem: 6186
Epoch: [25]  [23940/40201]  eta: 2:23:04  lr: 0.000002  min_lr: 0.000000  loss: 3.1136 (3.4159)  loss_scale: 32768.0000 (37844.5064)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0008  max mem: 6186
Epoch: [25]  [23950/40201]  eta: 2:22:59  lr: 0.000002  min_lr: 0.000000  loss: 3.1661 (3.4159)  loss_scale: 32768.0000 (37842.3869)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0025  max mem: 6186
Epoch: [25]  [23960/40201]  eta: 2:22:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5848 (3.4160)  loss_scale: 32768.0000 (37840.2691)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0024  max mem: 6186
[2023-07-24 18:45:59,731] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:45:59,732] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:45:59,742] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:45:59,742] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [23970/40201]  eta: 2:22:49  lr: 0.000002  min_lr: 0.000000  loss: 3.2717 (3.4159)  loss_scale: 32768.0000 (37846.3550)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0006  max mem: 6186
Epoch: [25]  [23980/40201]  eta: 2:22:44  lr: 0.000002  min_lr: 0.000000  loss: 3.1979 (3.4159)  loss_scale: 65536.0000 (37857.9015)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0005  max mem: 6186
Epoch: [25]  [23990/40201]  eta: 2:22:40  lr: 0.000002  min_lr: 0.000000  loss: 3.1194 (3.4158)  loss_scale: 65536.0000 (37869.4384)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0010  max mem: 6186
[2023-07-24 18:46:20,384] [INFO] [logging.py:69:log_dist] [Rank 0] step=12000, skipped=65, lr=[5.9203007215198074e-08, 5.9203007215198074e-08, 7.893734295359743e-08, 7.893734295359743e-08, 1.0524979060479657e-07, 1.0524979060479657e-07, 1.4033305413972878e-07, 1.4033305413972878e-07, 1.871107388529717e-07, 1.871107388529717e-07, 2.494809851372956e-07, 2.494809851372956e-07, 3.326413135163941e-07, 3.326413135163941e-07, 4.4352175135519214e-07, 4.4352175135519214e-07, 5.913623351402562e-07, 5.913623351402562e-07, 7.884831135203417e-07, 7.884831135203417e-07, 1.051310818027122e-06, 1.051310818027122e-06, 1.4017477573694963e-06, 1.4017477573694963e-06, 1.868997009825995e-06, 1.868997009825995e-06, 2.4919960131013266e-06, 2.4919960131013266e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 18:46:20,385] [INFO] [timer.py:181:stop] 0/24000, SamplesPerSec=13.331152051259743
Epoch: [25]  [24000/40201]  eta: 2:22:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4495 (3.4159)  loss_scale: 65536.0000 (37880.9656)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0010  max mem: 6186
Epoch: [25]  [24010/40201]  eta: 2:22:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7408 (3.4161)  loss_scale: 65536.0000 (37892.4833)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0008  max mem: 6186
Epoch: [25]  [24020/40201]  eta: 2:22:25  lr: 0.000002  min_lr: 0.000000  loss: 3.6152 (3.4160)  loss_scale: 65536.0000 (37903.9913)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0021  max mem: 6186
Epoch: [25]  [24030/40201]  eta: 2:22:20  lr: 0.000002  min_lr: 0.000000  loss: 3.2912 (3.4159)  loss_scale: 65536.0000 (37915.4898)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0020  max mem: 6186
Epoch: [25]  [24040/40201]  eta: 2:22:16  lr: 0.000002  min_lr: 0.000000  loss: 2.9237 (3.4157)  loss_scale: 65536.0000 (37926.9787)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0007  max mem: 6186
Epoch: [25]  [24050/40201]  eta: 2:22:11  lr: 0.000002  min_lr: 0.000000  loss: 3.4582 (3.4160)  loss_scale: 65536.0000 (37938.4581)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0008  max mem: 6186
Epoch: [25]  [24060/40201]  eta: 2:22:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5206 (3.4160)  loss_scale: 65536.0000 (37949.9279)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0011  max mem: 6186
Epoch: [25]  [24070/40201]  eta: 2:22:01  lr: 0.000002  min_lr: 0.000000  loss: 3.5833 (3.4162)  loss_scale: 65536.0000 (37961.3882)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0011  max mem: 6186
Epoch: [25]  [24080/40201]  eta: 2:21:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7846 (3.4163)  loss_scale: 65536.0000 (37972.8390)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0010  max mem: 6186
Epoch: [25]  [24090/40201]  eta: 2:21:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5088 (3.4165)  loss_scale: 65536.0000 (37984.2803)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0008  max mem: 6186
Epoch: [25]  [24100/40201]  eta: 2:21:47  lr: 0.000002  min_lr: 0.000000  loss: 3.5030 (3.4165)  loss_scale: 65536.0000 (37995.7120)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0015  max mem: 6186
Epoch: [25]  [24110/40201]  eta: 2:21:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4351 (3.4165)  loss_scale: 65536.0000 (38007.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0014  max mem: 6186
Epoch: [25]  [24120/40201]  eta: 2:21:37  lr: 0.000002  min_lr: 0.000000  loss: 3.0463 (3.4163)  loss_scale: 65536.0000 (38018.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0006  max mem: 6186
Epoch: [25]  [24130/40201]  eta: 2:21:33  lr: 0.000002  min_lr: 0.000000  loss: 2.6834 (3.4160)  loss_scale: 65536.0000 (38029.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.0890  max mem: 6186
[2023-07-24 18:47:40,633] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12065
[2023-07-24 18:47:40,633] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:47:40,637] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12065
[2023-07-24 18:47:40,637] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:47:40,637] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [24140/40201]  eta: 2:21:28  lr: 0.000002  min_lr: 0.000000  loss: 2.9587 (3.4161)  loss_scale: 32768.0000 (38027.7708)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.0893  max mem: 6186
Epoch: [25]  [24150/40201]  eta: 2:21:23  lr: 0.000002  min_lr: 0.000000  loss: 3.4711 (3.4160)  loss_scale: 32768.0000 (38025.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0010  max mem: 6186
Epoch: [25]  [24160/40201]  eta: 2:21:18  lr: 0.000002  min_lr: 0.000000  loss: 3.4213 (3.4161)  loss_scale: 32768.0000 (38023.4169)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0006  max mem: 6186
Epoch: [25]  [24170/40201]  eta: 2:21:13  lr: 0.000002  min_lr: 0.000000  loss: 3.3940 (3.4160)  loss_scale: 32768.0000 (38021.2426)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0006  max mem: 6186
Epoch: [25]  [24180/40201]  eta: 2:21:09  lr: 0.000002  min_lr: 0.000000  loss: 3.3894 (3.4160)  loss_scale: 32768.0000 (38019.0702)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0015  max mem: 6186
Epoch: [25]  [24190/40201]  eta: 2:21:04  lr: 0.000002  min_lr: 0.000000  loss: 3.4450 (3.4161)  loss_scale: 32768.0000 (38016.8995)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0020  max mem: 6186
Epoch: [25]  [24200/40201]  eta: 2:20:57  lr: 0.000002  min_lr: 0.000000  loss: 3.9005 (3.4164)  loss_scale: 32768.0000 (38014.7306)  weight_decay: 0.0500 (0.0500)  time: 0.4691  data: 0.0019  max mem: 6186
Epoch: [25]  [24210/40201]  eta: 2:20:50  lr: 0.000002  min_lr: 0.000000  loss: 3.9005 (3.4164)  loss_scale: 32768.0000 (38012.5635)  weight_decay: 0.0500 (0.0500)  time: 0.2891  data: 0.0017  max mem: 6186
Epoch: [25]  [24220/40201]  eta: 2:20:44  lr: 0.000002  min_lr: 0.000000  loss: 3.3594 (3.4164)  loss_scale: 32768.0000 (38010.3982)  weight_decay: 0.0500 (0.0500)  time: 0.3048  data: 0.0648  max mem: 6186
Epoch: [25]  [24230/40201]  eta: 2:20:38  lr: 0.000002  min_lr: 0.000000  loss: 3.4678 (3.4165)  loss_scale: 32768.0000 (38008.2347)  weight_decay: 0.0500 (0.0500)  time: 0.3818  data: 0.1302  max mem: 6186
Epoch: [25]  [24240/40201]  eta: 2:20:32  lr: 0.000002  min_lr: 0.000000  loss: 3.2906 (3.4163)  loss_scale: 32768.0000 (38006.0730)  weight_decay: 0.0500 (0.0500)  time: 0.4397  data: 0.0752  max mem: 6186
Epoch: [25]  [24250/40201]  eta: 2:20:26  lr: 0.000002  min_lr: 0.000000  loss: 3.2906 (3.4164)  loss_scale: 32768.0000 (38003.9131)  weight_decay: 0.0500 (0.0500)  time: 0.4127  data: 0.0130  max mem: 6186
Epoch: [25]  [24260/40201]  eta: 2:20:23  lr: 0.000002  min_lr: 0.000000  loss: 3.2677 (3.4162)  loss_scale: 32768.0000 (38001.7549)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.0038  max mem: 6186
Epoch: [25]  [24270/40201]  eta: 2:20:19  lr: 0.000002  min_lr: 0.000000  loss: 3.5358 (3.4164)  loss_scale: 32768.0000 (37999.5985)  weight_decay: 0.0500 (0.0500)  time: 0.8176  data: 0.0007  max mem: 6186
Epoch: [25]  [24280/40201]  eta: 2:20:14  lr: 0.000002  min_lr: 0.000000  loss: 3.5918 (3.4165)  loss_scale: 32768.0000 (37997.4439)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0215  max mem: 6186
Epoch: [25]  [24290/40201]  eta: 2:20:09  lr: 0.000002  min_lr: 0.000000  loss: 3.2504 (3.4164)  loss_scale: 32768.0000 (37995.2911)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0223  max mem: 6186
Epoch: [25]  [24300/40201]  eta: 2:20:04  lr: 0.000002  min_lr: 0.000000  loss: 3.2024 (3.4163)  loss_scale: 32768.0000 (37993.1400)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0017  max mem: 6186
Epoch: [25]  [24310/40201]  eta: 2:19:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4342 (3.4164)  loss_scale: 32768.0000 (37990.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0007  max mem: 6186
Epoch: [25]  [24320/40201]  eta: 2:19:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5478 (3.4164)  loss_scale: 32768.0000 (37988.8432)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0012  max mem: 6186
Epoch: [25]  [24330/40201]  eta: 2:19:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7315 (3.4165)  loss_scale: 32768.0000 (37986.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0017  max mem: 6186
Epoch: [25]  [24340/40201]  eta: 2:19:45  lr: 0.000002  min_lr: 0.000000  loss: 3.6688 (3.4166)  loss_scale: 32768.0000 (37984.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0012  max mem: 6186
Epoch: [25]  [24350/40201]  eta: 2:19:40  lr: 0.000002  min_lr: 0.000000  loss: 3.5623 (3.4165)  loss_scale: 32768.0000 (37982.4112)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0011  max mem: 6186
Epoch: [25]  [24360/40201]  eta: 2:19:35  lr: 0.000002  min_lr: 0.000000  loss: 3.3842 (3.4164)  loss_scale: 32768.0000 (37980.2708)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0010  max mem: 6186
Epoch: [25]  [24370/40201]  eta: 2:19:30  lr: 0.000002  min_lr: 0.000000  loss: 3.3816 (3.4164)  loss_scale: 32768.0000 (37978.1320)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0010  max mem: 6186
Epoch: [25]  [24380/40201]  eta: 2:19:25  lr: 0.000002  min_lr: 0.000000  loss: 3.1649 (3.4163)  loss_scale: 32768.0000 (37975.9951)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0011  max mem: 6186
[2023-07-24 18:50:03,546] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:50:03,546] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:50:03,565] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:50:03,565] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [24390/40201]  eta: 2:19:20  lr: 0.000002  min_lr: 0.000000  loss: 2.6697 (3.4160)  loss_scale: 32768.0000 (37976.5468)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0032  max mem: 6186
Epoch: [25]  [24400/40201]  eta: 2:19:16  lr: 0.000002  min_lr: 0.000000  loss: 3.0110 (3.4159)  loss_scale: 65536.0000 (37987.8412)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.0738  max mem: 6186
Epoch: [25]  [24410/40201]  eta: 2:19:11  lr: 0.000002  min_lr: 0.000000  loss: 3.1602 (3.4159)  loss_scale: 65536.0000 (37999.1263)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0720  max mem: 6186
Epoch: [25]  [24420/40201]  eta: 2:19:06  lr: 0.000002  min_lr: 0.000000  loss: 3.3715 (3.4158)  loss_scale: 65536.0000 (38010.4022)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0014  max mem: 6186
Epoch: [25]  [24430/40201]  eta: 2:19:01  lr: 0.000002  min_lr: 0.000000  loss: 3.4571 (3.4157)  loss_scale: 65536.0000 (38021.6689)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0015  max mem: 6186
Epoch: [25]  [24440/40201]  eta: 2:18:56  lr: 0.000002  min_lr: 0.000000  loss: 3.3915 (3.4157)  loss_scale: 65536.0000 (38032.9263)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0014  max mem: 6186
Epoch: [25]  [24450/40201]  eta: 2:18:51  lr: 0.000002  min_lr: 0.000000  loss: 2.9286 (3.4155)  loss_scale: 65536.0000 (38044.1746)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0006  max mem: 6186
Epoch: [25]  [24460/40201]  eta: 2:18:46  lr: 0.000002  min_lr: 0.000000  loss: 3.2824 (3.4155)  loss_scale: 65536.0000 (38055.4136)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0012  max mem: 6186
Epoch: [25]  [24470/40201]  eta: 2:18:42  lr: 0.000002  min_lr: 0.000000  loss: 3.2824 (3.4153)  loss_scale: 65536.0000 (38066.6435)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0014  max mem: 6186
Epoch: [25]  [24480/40201]  eta: 2:18:36  lr: 0.000002  min_lr: 0.000000  loss: 3.3803 (3.4155)  loss_scale: 65536.0000 (38077.8641)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0009  max mem: 6186
Epoch: [25]  [24490/40201]  eta: 2:18:32  lr: 0.000002  min_lr: 0.000000  loss: 3.2949 (3.4154)  loss_scale: 65536.0000 (38089.0757)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0009  max mem: 6186
Epoch: [25]  [24500/40201]  eta: 2:18:27  lr: 0.000002  min_lr: 0.000000  loss: 3.3266 (3.4156)  loss_scale: 65536.0000 (38100.2780)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0008  max mem: 6186
Epoch: [25]  [24510/40201]  eta: 2:18:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7384 (3.4155)  loss_scale: 65536.0000 (38111.4713)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.0006  max mem: 6186
Epoch: [25]  [24520/40201]  eta: 2:18:18  lr: 0.000002  min_lr: 0.000000  loss: 3.2810 (3.4155)  loss_scale: 65536.0000 (38122.6554)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0008  max mem: 6186
Epoch: [25]  [24530/40201]  eta: 2:18:13  lr: 0.000002  min_lr: 0.000000  loss: 3.1860 (3.4154)  loss_scale: 65536.0000 (38133.8303)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0009  max mem: 6186
[2023-07-24 18:51:34,054] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12269
[2023-07-24 18:51:34,055] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:51:34,060] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12269
[2023-07-24 18:51:34,060] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:51:34,060] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [24540/40201]  eta: 2:18:08  lr: 0.000002  min_lr: 0.000000  loss: 3.1982 (3.4155)  loss_scale: 65536.0000 (38142.3257)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0008  max mem: 6186
Epoch: [25]  [24550/40201]  eta: 2:18:03  lr: 0.000002  min_lr: 0.000000  loss: 3.1859 (3.4154)  loss_scale: 32768.0000 (38140.1367)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0009  max mem: 6186
Epoch: [25]  [24560/40201]  eta: 2:17:58  lr: 0.000002  min_lr: 0.000000  loss: 3.7157 (3.4156)  loss_scale: 32768.0000 (38137.9494)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0006  max mem: 6186
Epoch: [25]  [24570/40201]  eta: 2:17:53  lr: 0.000002  min_lr: 0.000000  loss: 3.7067 (3.4156)  loss_scale: 32768.0000 (38135.7639)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0009  max mem: 6186
Epoch: [25]  [24580/40201]  eta: 2:17:48  lr: 0.000002  min_lr: 0.000000  loss: 3.5086 (3.4156)  loss_scale: 32768.0000 (38133.5802)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0015  max mem: 6186
Epoch: [25]  [24590/40201]  eta: 2:17:43  lr: 0.000002  min_lr: 0.000000  loss: 3.4384 (3.4158)  loss_scale: 32768.0000 (38131.3983)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0012  max mem: 6186
Epoch: [25]  [24600/40201]  eta: 2:17:38  lr: 0.000002  min_lr: 0.000000  loss: 3.2906 (3.4157)  loss_scale: 32768.0000 (38129.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0006  max mem: 6186
Epoch: [25]  [24610/40201]  eta: 2:17:32  lr: 0.000002  min_lr: 0.000000  loss: 3.1090 (3.4155)  loss_scale: 32768.0000 (38127.0398)  weight_decay: 0.0500 (0.0500)  time: 0.4780  data: 0.0517  max mem: 6186
Epoch: [25]  [24620/40201]  eta: 2:17:26  lr: 0.000002  min_lr: 0.000000  loss: 3.2096 (3.4155)  loss_scale: 32768.0000 (38124.8632)  weight_decay: 0.0500 (0.0500)  time: 0.3736  data: 0.1414  max mem: 6186
Epoch: [25]  [24630/40201]  eta: 2:17:20  lr: 0.000002  min_lr: 0.000000  loss: 3.0770 (3.4155)  loss_scale: 32768.0000 (38122.6883)  weight_decay: 0.0500 (0.0500)  time: 0.3863  data: 0.1616  max mem: 6186
Epoch: [25]  [24640/40201]  eta: 2:17:12  lr: 0.000002  min_lr: 0.000000  loss: 3.0452 (3.4153)  loss_scale: 32768.0000 (38120.5152)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0729  max mem: 6186
Epoch: [25]  [24650/40201]  eta: 2:17:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5057 (3.4154)  loss_scale: 32768.0000 (38118.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3603  data: 0.1272  max mem: 6186
Epoch: [25]  [24660/40201]  eta: 2:17:01  lr: 0.000002  min_lr: 0.000000  loss: 3.5070 (3.4155)  loss_scale: 32768.0000 (38116.1744)  weight_decay: 0.0500 (0.0500)  time: 0.4378  data: 0.2048  max mem: 6186
Epoch: [25]  [24670/40201]  eta: 2:16:55  lr: 0.000002  min_lr: 0.000000  loss: 3.2216 (3.4154)  loss_scale: 32768.0000 (38114.0066)  weight_decay: 0.0500 (0.0500)  time: 0.4053  data: 0.1030  max mem: 6186
Epoch: [25]  [24680/40201]  eta: 2:16:50  lr: 0.000002  min_lr: 0.000000  loss: 3.1832 (3.4153)  loss_scale: 32768.0000 (38111.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0244  max mem: 6186
Epoch: [25]  [24690/40201]  eta: 2:16:45  lr: 0.000002  min_lr: 0.000000  loss: 3.1832 (3.4153)  loss_scale: 32768.0000 (38109.6762)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0008  max mem: 6186
Epoch: [25]  [24700/40201]  eta: 2:16:40  lr: 0.000002  min_lr: 0.000000  loss: 3.4706 (3.4154)  loss_scale: 32768.0000 (38107.5137)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0008  max mem: 6186
Epoch: [25]  [24710/40201]  eta: 2:16:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4166 (3.4154)  loss_scale: 32768.0000 (38105.3529)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0009  max mem: 6186
Epoch: [25]  [24720/40201]  eta: 2:16:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5680 (3.4155)  loss_scale: 32768.0000 (38103.1939)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0012  max mem: 6186
Epoch: [25]  [24730/40201]  eta: 2:16:25  lr: 0.000002  min_lr: 0.000000  loss: 3.3625 (3.4154)  loss_scale: 32768.0000 (38101.0366)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0009  max mem: 6186
Epoch: [25]  [24740/40201]  eta: 2:16:21  lr: 0.000002  min_lr: 0.000000  loss: 3.2446 (3.4155)  loss_scale: 32768.0000 (38098.8810)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0009  max mem: 6186
Epoch: [25]  [24750/40201]  eta: 2:16:16  lr: 0.000002  min_lr: 0.000000  loss: 3.4683 (3.4156)  loss_scale: 32768.0000 (38096.7272)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0011  max mem: 6186
Epoch: [25]  [24760/40201]  eta: 2:16:11  lr: 0.000002  min_lr: 0.000000  loss: 3.6769 (3.4158)  loss_scale: 32768.0000 (38094.5752)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0009  max mem: 6186
Epoch: [25]  [24770/40201]  eta: 2:16:06  lr: 0.000002  min_lr: 0.000000  loss: 3.7611 (3.4159)  loss_scale: 32768.0000 (38092.4249)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0010  max mem: 6186
Epoch: [25]  [24780/40201]  eta: 2:16:01  lr: 0.000002  min_lr: 0.000000  loss: 3.6253 (3.4160)  loss_scale: 32768.0000 (38090.2763)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0008  max mem: 6186
Epoch: [25]  [24790/40201]  eta: 2:15:56  lr: 0.000002  min_lr: 0.000000  loss: 3.4126 (3.4159)  loss_scale: 32768.0000 (38088.1294)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0009  max mem: 6186
[2023-07-24 18:53:52,014] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:53:52,014] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:53:52,018] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:53:52,018] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [24800/40201]  eta: 2:15:51  lr: 0.000002  min_lr: 0.000000  loss: 3.0327 (3.4159)  loss_scale: 32768.0000 (38091.2692)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0014  max mem: 6186
Epoch: [25]  [24810/40201]  eta: 2:15:46  lr: 0.000002  min_lr: 0.000000  loss: 2.9606 (3.4156)  loss_scale: 65536.0000 (38102.3307)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0011  max mem: 6186
Epoch: [25]  [24820/40201]  eta: 2:15:43  lr: 0.000002  min_lr: 0.000000  loss: 2.6133 (3.4153)  loss_scale: 65536.0000 (38113.3833)  weight_decay: 0.0500 (0.0500)  time: 0.7072  data: 0.1319  max mem: 6186
[2023-07-24 18:54:10,526] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12412
[2023-07-24 18:54:10,527] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:54:10,527] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:54:10,541] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12412
[2023-07-24 18:54:10,541] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [24830/40201]  eta: 2:15:37  lr: 0.000002  min_lr: 0.000000  loss: 2.8573 (3.4153)  loss_scale: 65536.0000 (38116.5092)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1318  max mem: 6186
Epoch: [25]  [24840/40201]  eta: 2:15:32  lr: 0.000002  min_lr: 0.000000  loss: 3.3585 (3.4154)  loss_scale: 32768.0000 (38114.3561)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0009  max mem: 6186
Epoch: [25]  [24850/40201]  eta: 2:15:26  lr: 0.000002  min_lr: 0.000000  loss: 3.5476 (3.4154)  loss_scale: 32768.0000 (38112.2047)  weight_decay: 0.0500 (0.0500)  time: 0.4234  data: 0.0009  max mem: 6186
Epoch: [25]  [24860/40201]  eta: 2:15:19  lr: 0.000002  min_lr: 0.000000  loss: 3.5018 (3.4155)  loss_scale: 32768.0000 (38110.0551)  weight_decay: 0.0500 (0.0500)  time: 0.2728  data: 0.0011  max mem: 6186
Epoch: [25]  [24870/40201]  eta: 2:15:15  lr: 0.000002  min_lr: 0.000000  loss: 3.3926 (3.4155)  loss_scale: 32768.0000 (38107.9072)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.2559  max mem: 6186
Epoch: [25]  [24880/40201]  eta: 2:15:08  lr: 0.000002  min_lr: 0.000000  loss: 3.2362 (3.4154)  loss_scale: 32768.0000 (38105.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.2854  max mem: 6186
Epoch: [25]  [24890/40201]  eta: 2:15:04  lr: 0.000002  min_lr: 0.000000  loss: 3.2362 (3.4154)  loss_scale: 32768.0000 (38103.6166)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.2960  max mem: 6186
Epoch: [25]  [24900/40201]  eta: 2:15:00  lr: 0.000002  min_lr: 0.000000  loss: 3.3467 (3.4155)  loss_scale: 32768.0000 (38101.4738)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.2954  max mem: 6186
Epoch: [25]  [24910/40201]  eta: 2:14:55  lr: 0.000002  min_lr: 0.000000  loss: 3.2912 (3.4156)  loss_scale: 32768.0000 (38099.3328)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.0301  max mem: 6186
Epoch: [25]  [24920/40201]  eta: 2:14:50  lr: 0.000002  min_lr: 0.000000  loss: 3.2135 (3.4155)  loss_scale: 32768.0000 (38097.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0007  max mem: 6186
Epoch: [25]  [24930/40201]  eta: 2:14:46  lr: 0.000002  min_lr: 0.000000  loss: 3.0763 (3.4153)  loss_scale: 32768.0000 (38095.0560)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0009  max mem: 6186
Epoch: [25]  [24940/40201]  eta: 2:14:41  lr: 0.000002  min_lr: 0.000000  loss: 3.1427 (3.4155)  loss_scale: 32768.0000 (38092.9201)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0010  max mem: 6186
Epoch: [25]  [24950/40201]  eta: 2:14:36  lr: 0.000002  min_lr: 0.000000  loss: 3.4345 (3.4157)  loss_scale: 32768.0000 (38090.7859)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0009  max mem: 6186
Epoch: [25]  [24960/40201]  eta: 2:14:32  lr: 0.000002  min_lr: 0.000000  loss: 3.9033 (3.4159)  loss_scale: 32768.0000 (38088.6535)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0009  max mem: 6186
Epoch: [25]  [24970/40201]  eta: 2:14:27  lr: 0.000002  min_lr: 0.000000  loss: 3.8505 (3.4160)  loss_scale: 32768.0000 (38086.5228)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0009  max mem: 6186
Epoch: [25]  [24980/40201]  eta: 2:14:22  lr: 0.000002  min_lr: 0.000000  loss: 3.6650 (3.4161)  loss_scale: 32768.0000 (38084.3937)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0008  max mem: 6186
Epoch: [25]  [24990/40201]  eta: 2:14:18  lr: 0.000002  min_lr: 0.000000  loss: 3.4959 (3.4160)  loss_scale: 32768.0000 (38082.2664)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0010  max mem: 6186
[2023-07-24 18:55:51,533] [INFO] [timer.py:181:stop] 0/25000, SamplesPerSec=13.316340031991329
Epoch: [25]  [25000/40201]  eta: 2:14:13  lr: 0.000002  min_lr: 0.000000  loss: 3.1479 (3.4160)  loss_scale: 32768.0000 (38080.1408)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0013  max mem: 6186
Epoch: [25]  [25010/40201]  eta: 2:14:08  lr: 0.000002  min_lr: 0.000000  loss: 3.1658 (3.4158)  loss_scale: 32768.0000 (38078.0169)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0334  max mem: 6186
Epoch: [25]  [25020/40201]  eta: 2:14:04  lr: 0.000002  min_lr: 0.000000  loss: 3.3184 (3.4159)  loss_scale: 32768.0000 (38075.8946)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.0333  max mem: 6186
Epoch: [25]  [25030/40201]  eta: 2:13:59  lr: 0.000002  min_lr: 0.000000  loss: 3.1850 (3.4158)  loss_scale: 32768.0000 (38073.7741)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0011  max mem: 6186
Epoch: [25]  [25040/40201]  eta: 2:13:54  lr: 0.000002  min_lr: 0.000000  loss: 3.1804 (3.4157)  loss_scale: 32768.0000 (38071.6553)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0010  max mem: 6186
Epoch: [25]  [25050/40201]  eta: 2:13:49  lr: 0.000002  min_lr: 0.000000  loss: 3.2459 (3.4157)  loss_scale: 32768.0000 (38069.5381)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0007  max mem: 6186
Epoch: [25]  [25060/40201]  eta: 2:13:44  lr: 0.000002  min_lr: 0.000000  loss: 3.1137 (3.4155)  loss_scale: 32768.0000 (38067.4227)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0007  max mem: 6186
Epoch: [25]  [25070/40201]  eta: 2:13:40  lr: 0.000002  min_lr: 0.000000  loss: 2.9112 (3.4155)  loss_scale: 32768.0000 (38065.3089)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0010  max mem: 6186
Epoch: [25]  [25080/40201]  eta: 2:13:35  lr: 0.000002  min_lr: 0.000000  loss: 3.1634 (3.4155)  loss_scale: 32768.0000 (38063.1968)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0012  max mem: 6186
[2023-07-24 18:56:44,445] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:56:44,445] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:56:44,461] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:56:44,462] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [25090/40201]  eta: 2:13:31  lr: 0.000002  min_lr: 0.000000  loss: 3.6660 (3.4155)  loss_scale: 32768.0000 (38071.5342)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.0012  max mem: 6186
Epoch: [25]  [25100/40201]  eta: 2:13:26  lr: 0.000002  min_lr: 0.000000  loss: 3.4409 (3.4154)  loss_scale: 65536.0000 (38082.4758)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0012  max mem: 6186
[2023-07-24 18:56:57,958] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12552
[2023-07-24 18:56:57,958] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 18:56:57,959] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 18:56:57,962] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12552
[2023-07-24 18:56:57,962] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [25110/40201]  eta: 2:13:21  lr: 0.000002  min_lr: 0.000000  loss: 3.0853 (3.4153)  loss_scale: 65536.0000 (38085.5791)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0011  max mem: 6186
Epoch: [25]  [25120/40201]  eta: 2:13:16  lr: 0.000002  min_lr: 0.000000  loss: 2.9451 (3.4152)  loss_scale: 32768.0000 (38083.4623)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0008  max mem: 6186
Epoch: [25]  [25130/40201]  eta: 2:13:12  lr: 0.000002  min_lr: 0.000000  loss: 3.0034 (3.4152)  loss_scale: 32768.0000 (38081.3472)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0013  max mem: 6186
Epoch: [25]  [25140/40201]  eta: 2:13:07  lr: 0.000002  min_lr: 0.000000  loss: 3.0100 (3.4150)  loss_scale: 32768.0000 (38079.2338)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0014  max mem: 6186
Epoch: [25]  [25150/40201]  eta: 2:13:03  lr: 0.000002  min_lr: 0.000000  loss: 3.0859 (3.4150)  loss_scale: 32768.0000 (38077.1220)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.0011  max mem: 6186
Epoch: [25]  [25160/40201]  eta: 2:12:56  lr: 0.000002  min_lr: 0.000000  loss: 3.1691 (3.4151)  loss_scale: 32768.0000 (38075.0120)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0016  max mem: 6186
Epoch: [25]  [25170/40201]  eta: 2:12:50  lr: 0.000002  min_lr: 0.000000  loss: 3.4717 (3.4152)  loss_scale: 32768.0000 (38072.9036)  weight_decay: 0.0500 (0.0500)  time: 0.3316  data: 0.0016  max mem: 6186
Epoch: [25]  [25180/40201]  eta: 2:12:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5173 (3.4153)  loss_scale: 32768.0000 (38070.7969)  weight_decay: 0.0500 (0.0500)  time: 0.3758  data: 0.0995  max mem: 6186
Epoch: [25]  [25190/40201]  eta: 2:12:38  lr: 0.000002  min_lr: 0.000000  loss: 3.1801 (3.4150)  loss_scale: 32768.0000 (38068.6918)  weight_decay: 0.0500 (0.0500)  time: 0.4225  data: 0.1399  max mem: 6186
Epoch: [25]  [25200/40201]  eta: 2:12:31  lr: 0.000002  min_lr: 0.000000  loss: 3.3573 (3.4151)  loss_scale: 32768.0000 (38066.5885)  weight_decay: 0.0500 (0.0500)  time: 0.3342  data: 0.0584  max mem: 6186
Epoch: [25]  [25210/40201]  eta: 2:12:27  lr: 0.000002  min_lr: 0.000000  loss: 3.4317 (3.4150)  loss_scale: 32768.0000 (38064.4868)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.2255  max mem: 6186
Epoch: [25]  [25220/40201]  eta: 2:12:22  lr: 0.000002  min_lr: 0.000000  loss: 3.8365 (3.4154)  loss_scale: 32768.0000 (38062.3867)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.3270  max mem: 6186
Epoch: [25]  [25230/40201]  eta: 2:12:17  lr: 0.000002  min_lr: 0.000000  loss: 3.8712 (3.4155)  loss_scale: 32768.0000 (38060.2884)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1190  max mem: 6186
Epoch: [25]  [25240/40201]  eta: 2:12:11  lr: 0.000002  min_lr: 0.000000  loss: 3.5096 (3.4156)  loss_scale: 32768.0000 (38058.1917)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0012  max mem: 6186
Epoch: [25]  [25250/40201]  eta: 2:12:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5889 (3.4158)  loss_scale: 32768.0000 (38056.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0010  max mem: 6186
Epoch: [25]  [25260/40201]  eta: 2:12:03  lr: 0.000002  min_lr: 0.000000  loss: 3.4274 (3.4157)  loss_scale: 32768.0000 (38054.0032)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.1070  max mem: 6186
Epoch: [25]  [25270/40201]  eta: 2:11:58  lr: 0.000002  min_lr: 0.000000  loss: 3.3502 (3.4156)  loss_scale: 32768.0000 (38051.9115)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.1070  max mem: 6186
Epoch: [25]  [25280/40201]  eta: 2:11:52  lr: 0.000002  min_lr: 0.000000  loss: 3.3376 (3.4156)  loss_scale: 32768.0000 (38049.8214)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0016  max mem: 6186
Epoch: [25]  [25290/40201]  eta: 2:11:47  lr: 0.000002  min_lr: 0.000000  loss: 3.3886 (3.4156)  loss_scale: 32768.0000 (38047.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0018  max mem: 6186
Epoch: [25]  [25300/40201]  eta: 2:11:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4583 (3.4155)  loss_scale: 32768.0000 (38045.6463)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0008  max mem: 6186
Epoch: [25]  [25310/40201]  eta: 2:11:37  lr: 0.000002  min_lr: 0.000000  loss: 3.1765 (3.4156)  loss_scale: 32768.0000 (38043.5611)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0010  max mem: 6186
Epoch: [25]  [25320/40201]  eta: 2:11:32  lr: 0.000002  min_lr: 0.000000  loss: 3.1765 (3.4156)  loss_scale: 32768.0000 (38041.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0012  max mem: 6186
Epoch: [25]  [25330/40201]  eta: 2:11:27  lr: 0.000002  min_lr: 0.000000  loss: 3.1950 (3.4156)  loss_scale: 32768.0000 (38039.3958)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0011  max mem: 6186
Epoch: [25]  [25340/40201]  eta: 2:11:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7339 (3.4158)  loss_scale: 32768.0000 (38037.3157)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0802  max mem: 6186
Epoch: [25]  [25350/40201]  eta: 2:11:18  lr: 0.000002  min_lr: 0.000000  loss: 3.2856 (3.4156)  loss_scale: 32768.0000 (38035.2371)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0802  max mem: 6186
Epoch: [25]  [25360/40201]  eta: 2:11:13  lr: 0.000002  min_lr: 0.000000  loss: 3.1182 (3.4155)  loss_scale: 32768.0000 (38033.1602)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0008  max mem: 6186
[2023-07-24 18:59:22,805] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:59:22,806] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 18:59:22,813] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 18:59:22,813] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [25370/40201]  eta: 2:11:08  lr: 0.000002  min_lr: 0.000000  loss: 3.2978 (3.4155)  loss_scale: 32768.0000 (38041.4174)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0012  max mem: 6186
Epoch: [25]  [25380/40201]  eta: 2:11:03  lr: 0.000002  min_lr: 0.000000  loss: 3.3754 (3.4154)  loss_scale: 65536.0000 (38052.2501)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0010  max mem: 6186
Epoch: [25]  [25390/40201]  eta: 2:10:57  lr: 0.000002  min_lr: 0.000000  loss: 3.1920 (3.4154)  loss_scale: 65536.0000 (38063.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0010  max mem: 6186
Epoch: [25]  [25400/40201]  eta: 2:10:52  lr: 0.000002  min_lr: 0.000000  loss: 3.3900 (3.4154)  loss_scale: 65536.0000 (38073.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0016  max mem: 6186
Epoch: [25]  [25410/40201]  eta: 2:10:47  lr: 0.000002  min_lr: 0.000000  loss: 3.3904 (3.4152)  loss_scale: 65536.0000 (38084.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0010  max mem: 6186
Epoch: [25]  [25420/40201]  eta: 2:10:42  lr: 0.000002  min_lr: 0.000000  loss: 3.1669 (3.4152)  loss_scale: 65536.0000 (38095.4958)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0008  max mem: 6186
Epoch: [25]  [25430/40201]  eta: 2:10:37  lr: 0.000002  min_lr: 0.000000  loss: 3.5140 (3.4152)  loss_scale: 65536.0000 (38106.2860)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0008  max mem: 6186
Epoch: [25]  [25440/40201]  eta: 2:10:32  lr: 0.000002  min_lr: 0.000000  loss: 3.5140 (3.4151)  loss_scale: 65536.0000 (38117.0677)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0009  max mem: 6186
Epoch: [25]  [25450/40201]  eta: 2:10:27  lr: 0.000002  min_lr: 0.000000  loss: 3.1489 (3.4152)  loss_scale: 65536.0000 (38127.8409)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0011  max mem: 6186
Epoch: [25]  [25460/40201]  eta: 2:10:22  lr: 0.000002  min_lr: 0.000000  loss: 2.9302 (3.4149)  loss_scale: 65536.0000 (38138.6057)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0011  max mem: 6186
Epoch: [25]  [25470/40201]  eta: 2:10:17  lr: 0.000002  min_lr: 0.000000  loss: 2.8890 (3.4147)  loss_scale: 65536.0000 (38149.3620)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0010  max mem: 6186
Epoch: [25]  [25480/40201]  eta: 2:10:12  lr: 0.000002  min_lr: 0.000000  loss: 2.9272 (3.4147)  loss_scale: 65536.0000 (38160.1099)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0007  max mem: 6186
Epoch: [25]  [25490/40201]  eta: 2:10:07  lr: 0.000002  min_lr: 0.000000  loss: 3.6891 (3.4148)  loss_scale: 65536.0000 (38170.8493)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0018  max mem: 6186
Epoch: [25]  [25500/40201]  eta: 2:10:03  lr: 0.000002  min_lr: 0.000000  loss: 3.8346 (3.4149)  loss_scale: 65536.0000 (38181.5803)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.0030  max mem: 6186
Epoch: [25]  [25510/40201]  eta: 2:09:58  lr: 0.000002  min_lr: 0.000000  loss: 3.3867 (3.4147)  loss_scale: 65536.0000 (38192.3029)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.0024  max mem: 6186
Epoch: [25]  [25520/40201]  eta: 2:09:52  lr: 0.000002  min_lr: 0.000000  loss: 3.3567 (3.4146)  loss_scale: 65536.0000 (38203.0171)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0020  max mem: 6186
Epoch: [25]  [25530/40201]  eta: 2:09:48  lr: 0.000002  min_lr: 0.000000  loss: 3.4484 (3.4146)  loss_scale: 65536.0000 (38213.7229)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.2427  max mem: 6186
Epoch: [25]  [25540/40201]  eta: 2:09:41  lr: 0.000002  min_lr: 0.000000  loss: 3.6152 (3.4148)  loss_scale: 65536.0000 (38224.4203)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.3011  max mem: 6186
Epoch: [25]  [25550/40201]  eta: 2:09:36  lr: 0.000002  min_lr: 0.000000  loss: 3.8227 (3.4149)  loss_scale: 65536.0000 (38235.1094)  weight_decay: 0.0500 (0.0500)  time: 0.4191  data: 0.1494  max mem: 6186
Epoch: [25]  [25560/40201]  eta: 2:09:31  lr: 0.000002  min_lr: 0.000000  loss: 3.3225 (3.4148)  loss_scale: 65536.0000 (38245.7901)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.2704  max mem: 6186
Epoch: [25]  [25570/40201]  eta: 2:09:29  lr: 0.000002  min_lr: 0.000000  loss: 3.3066 (3.4147)  loss_scale: 65536.0000 (38256.4624)  weight_decay: 0.0500 (0.0500)  time: 0.8560  data: 0.4418  max mem: 6186
Epoch: [25]  [25580/40201]  eta: 2:09:24  lr: 0.000002  min_lr: 0.000000  loss: 3.3456 (3.4148)  loss_scale: 65536.0000 (38267.1264)  weight_decay: 0.0500 (0.0500)  time: 0.9094  data: 0.2622  max mem: 6186
Epoch: [25]  [25590/40201]  eta: 2:09:20  lr: 0.000002  min_lr: 0.000000  loss: 3.1401 (3.4148)  loss_scale: 65536.0000 (38277.7820)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.0014  max mem: 6186
Epoch: [25]  [25600/40201]  eta: 2:09:16  lr: 0.000002  min_lr: 0.000000  loss: 3.1401 (3.4147)  loss_scale: 65536.0000 (38288.4294)  weight_decay: 0.0500 (0.0500)  time: 0.6883  data: 0.0014  max mem: 6186
Epoch: [25]  [25610/40201]  eta: 2:09:12  lr: 0.000002  min_lr: 0.000000  loss: 3.0220 (3.4145)  loss_scale: 65536.0000 (38299.0684)  weight_decay: 0.0500 (0.0500)  time: 0.7327  data: 0.0017  max mem: 6186
[2023-07-24 19:02:00,366] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:02:00,366] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 19:02:00,381] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:02:00,382] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [25620/40201]  eta: 2:09:07  lr: 0.000002  min_lr: 0.000000  loss: 3.0220 (3.4145)  loss_scale: 65536.0000 (38314.8149)  weight_decay: 0.0500 (0.0500)  time: 0.7414  data: 0.0012  max mem: 6186
[2023-07-24 19:02:01,558] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12810
[2023-07-24 19:02:01,558] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 19:02:01,558] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-07-24 19:02:01,560] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12810
[2023-07-24 19:02:01,560] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [25]  [25630/40201]  eta: 2:09:03  lr: 0.000002  min_lr: 0.000000  loss: 3.4854 (3.4147)  loss_scale: 65536.0000 (38325.4353)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.0017  max mem: 6186
Epoch: [25]  [25640/40201]  eta: 2:08:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4283 (3.4146)  loss_scale: 65536.0000 (38336.0474)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.0016  max mem: 6186
Epoch: [25]  [25650/40201]  eta: 2:08:54  lr: 0.000002  min_lr: 0.000000  loss: 3.0601 (3.4145)  loss_scale: 65536.0000 (38346.6513)  weight_decay: 0.0500 (0.0500)  time: 0.7280  data: 0.0008  max mem: 6186
Epoch: [25]  [25660/40201]  eta: 2:08:50  lr: 0.000002  min_lr: 0.000000  loss: 3.2388 (3.4144)  loss_scale: 65536.0000 (38357.2469)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.0009  max mem: 6186
Epoch: [25]  [25670/40201]  eta: 2:08:46  lr: 0.000002  min_lr: 0.000000  loss: 3.3332 (3.4144)  loss_scale: 65536.0000 (38367.8342)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.0010  max mem: 6186
Epoch: [25]  [25680/40201]  eta: 2:08:42  lr: 0.000002  min_lr: 0.000000  loss: 3.3934 (3.4145)  loss_scale: 65536.0000 (38378.4133)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.0019  max mem: 6186
Epoch: [25]  [25690/40201]  eta: 2:08:38  lr: 0.000002  min_lr: 0.000000  loss: 3.0885 (3.4144)  loss_scale: 65536.0000 (38388.9842)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.0020  max mem: 6186
Epoch: [25]  [25700/40201]  eta: 2:08:33  lr: 0.000002  min_lr: 0.000000  loss: 3.0090 (3.4144)  loss_scale: 65536.0000 (38399.5468)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.0011  max mem: 6186
Epoch: [25]  [25710/40201]  eta: 2:08:28  lr: 0.000002  min_lr: 0.000000  loss: 3.0638 (3.4144)  loss_scale: 65536.0000 (38410.1012)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0013  max mem: 6186
Epoch: [25]  [25720/40201]  eta: 2:08:24  lr: 0.000002  min_lr: 0.000000  loss: 3.3429 (3.4144)  loss_scale: 65536.0000 (38420.6474)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.0011  max mem: 6186
[2023-07-24 19:03:17,994] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12864
[2023-07-24 19:03:17,994] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:03:17,994] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:03:18,006] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 12864
[2023-07-24 19:03:18,006] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [25730/40201]  eta: 2:08:19  lr: 0.000002  min_lr: 0.000000  loss: 3.6271 (3.4146)  loss_scale: 65536.0000 (38428.6385)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0007  max mem: 6186
Epoch: [25]  [25740/40201]  eta: 2:08:15  lr: 0.000002  min_lr: 0.000000  loss: 3.6423 (3.4146)  loss_scale: 32768.0000 (38426.4394)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.0012  max mem: 6186
Epoch: [25]  [25750/40201]  eta: 2:08:11  lr: 0.000002  min_lr: 0.000000  loss: 3.3962 (3.4144)  loss_scale: 32768.0000 (38424.2420)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.0009  max mem: 6186
Epoch: [25]  [25760/40201]  eta: 2:08:06  lr: 0.000002  min_lr: 0.000000  loss: 3.3962 (3.4146)  loss_scale: 32768.0000 (38422.0463)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.0015  max mem: 6186
Epoch: [25]  [25770/40201]  eta: 2:08:02  lr: 0.000002  min_lr: 0.000000  loss: 3.2055 (3.4144)  loss_scale: 32768.0000 (38419.8524)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.0017  max mem: 6186
Epoch: [25]  [25780/40201]  eta: 2:07:58  lr: 0.000002  min_lr: 0.000000  loss: 3.0585 (3.4143)  loss_scale: 32768.0000 (38417.6601)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.0007  max mem: 6186
Epoch: [25]  [25790/40201]  eta: 2:07:53  lr: 0.000002  min_lr: 0.000000  loss: 3.4089 (3.4143)  loss_scale: 32768.0000 (38415.4696)  weight_decay: 0.0500 (0.0500)  time: 0.7224  data: 0.0012  max mem: 6186
Epoch: [25]  [25800/40201]  eta: 2:07:49  lr: 0.000002  min_lr: 0.000000  loss: 3.2308 (3.4141)  loss_scale: 32768.0000 (38413.2807)  weight_decay: 0.0500 (0.0500)  time: 0.7390  data: 0.0011  max mem: 6186
Epoch: [25]  [25810/40201]  eta: 2:07:45  lr: 0.000002  min_lr: 0.000000  loss: 3.1037 (3.4142)  loss_scale: 32768.0000 (38411.0936)  weight_decay: 0.0500 (0.0500)  time: 0.7304  data: 0.0010  max mem: 6186
Epoch: [25]  [25820/40201]  eta: 2:07:41  lr: 0.000002  min_lr: 0.000000  loss: 3.9372 (3.4143)  loss_scale: 32768.0000 (38408.9081)  weight_decay: 0.0500 (0.0500)  time: 0.7224  data: 0.0012  max mem: 6186
Epoch: [25]  [25830/40201]  eta: 2:07:37  lr: 0.000002  min_lr: 0.000000  loss: 3.1809 (3.4142)  loss_scale: 32768.0000 (38406.7243)  weight_decay: 0.0500 (0.0500)  time: 0.7608  data: 0.0011  max mem: 6186
Epoch: [25]  [25840/40201]  eta: 2:07:33  lr: 0.000002  min_lr: 0.000000  loss: 3.0671 (3.4142)  loss_scale: 32768.0000 (38404.5422)  weight_decay: 0.0500 (0.0500)  time: 0.7454  data: 0.0016  max mem: 6186
Epoch: [25]  [25850/40201]  eta: 2:07:28  lr: 0.000002  min_lr: 0.000000  loss: 3.7078 (3.4144)  loss_scale: 32768.0000 (38402.3618)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.0018  max mem: 6186
Epoch: [25]  [25860/40201]  eta: 2:07:22  lr: 0.000002  min_lr: 0.000000  loss: 3.6328 (3.4145)  loss_scale: 32768.0000 (38400.1831)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0015  max mem: 6186
Epoch: [25]  [25870/40201]  eta: 2:07:16  lr: 0.000002  min_lr: 0.000000  loss: 3.3136 (3.4145)  loss_scale: 32768.0000 (38398.0061)  weight_decay: 0.0500 (0.0500)  time: 0.3729  data: 0.0015  max mem: 6186
Epoch: [25]  [25880/40201]  eta: 2:07:10  lr: 0.000002  min_lr: 0.000000  loss: 3.3136 (3.4145)  loss_scale: 32768.0000 (38395.8308)  weight_decay: 0.0500 (0.0500)  time: 0.3666  data: 0.0012  max mem: 6186
Epoch: [25]  [25890/40201]  eta: 2:07:04  lr: 0.000002  min_lr: 0.000000  loss: 3.6907 (3.4146)  loss_scale: 32768.0000 (38393.6571)  weight_decay: 0.0500 (0.0500)  time: 0.3912  data: 0.0014  max mem: 6186
Epoch: [25]  [25900/40201]  eta: 2:06:58  lr: 0.000002  min_lr: 0.000000  loss: 3.6928 (3.4147)  loss_scale: 32768.0000 (38391.4851)  weight_decay: 0.0500 (0.0500)  time: 0.4000  data: 0.0013  max mem: 6186
Epoch: [25]  [25910/40201]  eta: 2:06:52  lr: 0.000002  min_lr: 0.000000  loss: 3.4124 (3.4146)  loss_scale: 32768.0000 (38389.3148)  weight_decay: 0.0500 (0.0500)  time: 0.4403  data: 0.0013  max mem: 6186
Epoch: [25]  [25920/40201]  eta: 2:06:46  lr: 0.000002  min_lr: 0.000000  loss: 3.3855 (3.4147)  loss_scale: 32768.0000 (38387.1462)  weight_decay: 0.0500 (0.0500)  time: 0.4801  data: 0.0011  max mem: 6186
Epoch: [25]  [25930/40201]  eta: 2:06:43  lr: 0.000002  min_lr: 0.000000  loss: 3.3855 (3.4148)  loss_scale: 32768.0000 (38384.9792)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.0007  max mem: 6186
Epoch: [25]  [25940/40201]  eta: 2:06:38  lr: 0.000002  min_lr: 0.000000  loss: 3.2895 (3.4146)  loss_scale: 32768.0000 (38382.8139)  weight_decay: 0.0500 (0.0500)  time: 0.7534  data: 0.0008  max mem: 6186
Epoch: [25]  [25950/40201]  eta: 2:06:34  lr: 0.000002  min_lr: 0.000000  loss: 3.0964 (3.4146)  loss_scale: 32768.0000 (38380.6503)  weight_decay: 0.0500 (0.0500)  time: 0.6872  data: 0.0006  max mem: 6186
Epoch: [25]  [25960/40201]  eta: 2:06:29  lr: 0.000002  min_lr: 0.000000  loss: 3.4877 (3.4147)  loss_scale: 32768.0000 (38378.4883)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.0013  max mem: 6186
Epoch: [25]  [25970/40201]  eta: 2:06:25  lr: 0.000002  min_lr: 0.000000  loss: 3.4877 (3.4147)  loss_scale: 32768.0000 (38376.3281)  weight_decay: 0.0500 (0.0500)  time: 0.7160  data: 0.0019  max mem: 6186
Epoch: [25]  [25980/40201]  eta: 2:06:21  lr: 0.000002  min_lr: 0.000000  loss: 3.1780 (3.4147)  loss_scale: 32768.0000 (38374.1694)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.0013  max mem: 6186
[2023-07-24 19:06:02,583] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:06:02,583] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:06:02,591] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:06:02,591] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [25990/40201]  eta: 2:06:16  lr: 0.000002  min_lr: 0.000000  loss: 3.2801 (3.4146)  loss_scale: 32768.0000 (38377.0554)  weight_decay: 0.0500 (0.0500)  time: 0.7057  data: 0.0006  max mem: 6186
[2023-07-24 19:06:11,253] [INFO] [logging.py:69:log_dist] [Rank 0] step=13000, skipped=71, lr=[5.79203744501038e-08, 5.79203744501038e-08, 7.722716593347173e-08, 7.722716593347173e-08, 1.0296955457796232e-07, 1.0296955457796232e-07, 1.3729273943728308e-07, 1.3729273943728308e-07, 1.8305698591637744e-07, 1.8305698591637744e-07, 2.440759812218366e-07, 2.440759812218366e-07, 3.2543464162911543e-07, 3.2543464162911543e-07, 4.3391285550548727e-07, 4.3391285550548727e-07, 5.785504740073163e-07, 5.785504740073163e-07, 7.714006320097552e-07, 7.714006320097552e-07, 1.028534176013007e-06, 1.028534176013007e-06, 1.3713789013506758e-06, 1.3713789013506758e-06, 1.828505201800901e-06, 1.828505201800901e-06, 2.4380069357345347e-06, 2.4380069357345347e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 19:06:11,255] [INFO] [timer.py:181:stop] 0/26000, SamplesPerSec=13.271780384997307
Epoch: [25]  [26000/40201]  eta: 2:06:12  lr: 0.000002  min_lr: 0.000000  loss: 3.0730 (3.4144)  loss_scale: 65536.0000 (38387.5008)  weight_decay: 0.0500 (0.0500)  time: 0.7089  data: 0.0006  max mem: 6186
Epoch: [25]  [26010/40201]  eta: 2:06:08  lr: 0.000002  min_lr: 0.000000  loss: 3.2606 (3.4147)  loss_scale: 65536.0000 (38397.9381)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.0009  max mem: 6186
Epoch: [25]  [26020/40201]  eta: 2:06:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9393 (3.4148)  loss_scale: 65536.0000 (38408.3674)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.0006  max mem: 6186
Epoch: [25]  [26030/40201]  eta: 2:05:59  lr: 0.000002  min_lr: 0.000000  loss: 3.3962 (3.4147)  loss_scale: 65536.0000 (38418.7887)  weight_decay: 0.0500 (0.0500)  time: 0.7140  data: 0.0009  max mem: 6186
[2023-07-24 19:06:38,197] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13018
[2023-07-24 19:06:38,198] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:06:38,198] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:06:38,212] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13018
[2023-07-24 19:06:38,212] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [26040/40201]  eta: 2:05:55  lr: 0.000002  min_lr: 0.000000  loss: 3.2895 (3.4147)  loss_scale: 65536.0000 (38424.1687)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.0010  max mem: 6186
Epoch: [25]  [26050/40201]  eta: 2:05:51  lr: 0.000002  min_lr: 0.000000  loss: 3.2895 (3.4147)  loss_scale: 32768.0000 (38421.9975)  weight_decay: 0.0500 (0.0500)  time: 0.7648  data: 0.0023  max mem: 6186
Epoch: [25]  [26060/40201]  eta: 2:05:47  lr: 0.000002  min_lr: 0.000000  loss: 3.2188 (3.4146)  loss_scale: 32768.0000 (38419.8279)  weight_decay: 0.0500 (0.0500)  time: 0.7645  data: 0.0028  max mem: 6186
Epoch: [25]  [26070/40201]  eta: 2:05:42  lr: 0.000002  min_lr: 0.000000  loss: 3.1693 (3.4146)  loss_scale: 32768.0000 (38417.6601)  weight_decay: 0.0500 (0.0500)  time: 0.6916  data: 0.0016  max mem: 6186
Epoch: [25]  [26080/40201]  eta: 2:05:37  lr: 0.000002  min_lr: 0.000000  loss: 3.1693 (3.4145)  loss_scale: 32768.0000 (38415.4939)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.0016  max mem: 6186
Epoch: [25]  [26090/40201]  eta: 2:05:33  lr: 0.000002  min_lr: 0.000000  loss: 3.1298 (3.4145)  loss_scale: 32768.0000 (38413.3293)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.0013  max mem: 6186
Epoch: [25]  [26100/40201]  eta: 2:05:27  lr: 0.000002  min_lr: 0.000000  loss: 3.2750 (3.4145)  loss_scale: 32768.0000 (38411.1665)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0010  max mem: 6186
Epoch: [25]  [26110/40201]  eta: 2:05:21  lr: 0.000002  min_lr: 0.000000  loss: 3.0623 (3.4143)  loss_scale: 32768.0000 (38409.0052)  weight_decay: 0.0500 (0.0500)  time: 0.3733  data: 0.0010  max mem: 6186
Epoch: [25]  [26120/40201]  eta: 2:05:15  lr: 0.000002  min_lr: 0.000000  loss: 2.6008 (3.4141)  loss_scale: 32768.0000 (38406.8457)  weight_decay: 0.0500 (0.0500)  time: 0.4755  data: 0.1277  max mem: 6186
Epoch: [25]  [26130/40201]  eta: 2:05:09  lr: 0.000002  min_lr: 0.000000  loss: 3.1597 (3.4140)  loss_scale: 32768.0000 (38404.6878)  weight_decay: 0.0500 (0.0500)  time: 0.4756  data: 0.1283  max mem: 6186
Epoch: [25]  [26140/40201]  eta: 2:05:03  lr: 0.000002  min_lr: 0.000000  loss: 3.2190 (3.4139)  loss_scale: 32768.0000 (38402.5315)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0021  max mem: 6186
Epoch: [25]  [26150/40201]  eta: 2:05:00  lr: 0.000002  min_lr: 0.000000  loss: 3.3862 (3.4139)  loss_scale: 32768.0000 (38400.3769)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0020  max mem: 6186
Epoch: [25]  [26160/40201]  eta: 2:04:56  lr: 0.000002  min_lr: 0.000000  loss: 3.1507 (3.4139)  loss_scale: 32768.0000 (38398.2239)  weight_decay: 0.0500 (0.0500)  time: 0.8177  data: 0.0018  max mem: 6186
Epoch: [25]  [26170/40201]  eta: 2:04:51  lr: 0.000002  min_lr: 0.000000  loss: 3.1986 (3.4138)  loss_scale: 32768.0000 (38396.0726)  weight_decay: 0.0500 (0.0500)  time: 0.7469  data: 0.0015  max mem: 6186
Epoch: [25]  [26180/40201]  eta: 2:04:47  lr: 0.000002  min_lr: 0.000000  loss: 3.3019 (3.4138)  loss_scale: 32768.0000 (38393.9229)  weight_decay: 0.0500 (0.0500)  time: 0.7665  data: 0.0014  max mem: 6186
Epoch: [25]  [26190/40201]  eta: 2:04:43  lr: 0.000002  min_lr: 0.000000  loss: 3.8767 (3.4140)  loss_scale: 32768.0000 (38391.7749)  weight_decay: 0.0500 (0.0500)  time: 0.7487  data: 0.0010  max mem: 6186
Epoch: [25]  [26200/40201]  eta: 2:04:39  lr: 0.000002  min_lr: 0.000000  loss: 3.4557 (3.4140)  loss_scale: 32768.0000 (38389.6285)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.0005  max mem: 6186
Epoch: [25]  [26210/40201]  eta: 2:04:34  lr: 0.000002  min_lr: 0.000000  loss: 3.2339 (3.4139)  loss_scale: 32768.0000 (38387.4837)  weight_decay: 0.0500 (0.0500)  time: 0.7343  data: 0.0008  max mem: 6186
Epoch: [25]  [26220/40201]  eta: 2:04:30  lr: 0.000002  min_lr: 0.000000  loss: 3.2339 (3.4140)  loss_scale: 32768.0000 (38385.3406)  weight_decay: 0.0500 (0.0500)  time: 0.7429  data: 0.0012  max mem: 6186
Epoch: [25]  [26230/40201]  eta: 2:04:26  lr: 0.000002  min_lr: 0.000000  loss: 3.3401 (3.4139)  loss_scale: 32768.0000 (38383.1991)  weight_decay: 0.0500 (0.0500)  time: 0.7231  data: 0.0010  max mem: 6186
Epoch: [25]  [26240/40201]  eta: 2:04:22  lr: 0.000002  min_lr: 0.000000  loss: 3.4805 (3.4139)  loss_scale: 32768.0000 (38381.0593)  weight_decay: 0.0500 (0.0500)  time: 0.7347  data: 0.0011  max mem: 6186
Epoch: [25]  [26250/40201]  eta: 2:04:17  lr: 0.000002  min_lr: 0.000000  loss: 3.5132 (3.4139)  loss_scale: 32768.0000 (38378.9210)  weight_decay: 0.0500 (0.0500)  time: 0.7361  data: 0.0010  max mem: 6186
Epoch: [25]  [26260/40201]  eta: 2:04:13  lr: 0.000002  min_lr: 0.000000  loss: 3.5132 (3.4140)  loss_scale: 32768.0000 (38376.7844)  weight_decay: 0.0500 (0.0500)  time: 0.7293  data: 0.0153  max mem: 6186
Epoch: [25]  [26270/40201]  eta: 2:04:09  lr: 0.000002  min_lr: 0.000000  loss: 3.6855 (3.4140)  loss_scale: 32768.0000 (38374.6495)  weight_decay: 0.0500 (0.0500)  time: 0.7319  data: 0.0153  max mem: 6186
Epoch: [25]  [26280/40201]  eta: 2:04:04  lr: 0.000002  min_lr: 0.000000  loss: 3.2541 (3.4140)  loss_scale: 32768.0000 (38372.5161)  weight_decay: 0.0500 (0.0500)  time: 0.7289  data: 0.0009  max mem: 6186
Epoch: [25]  [26290/40201]  eta: 2:04:00  lr: 0.000002  min_lr: 0.000000  loss: 3.2970 (3.4141)  loss_scale: 32768.0000 (38370.3844)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.0015  max mem: 6186
[2023-07-24 19:09:32,944] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:09:32,945] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:09:32,971] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:09:32,971] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [26300/40201]  eta: 2:03:56  lr: 0.000002  min_lr: 0.000000  loss: 3.3372 (3.4140)  loss_scale: 32768.0000 (38375.7296)  weight_decay: 0.0500 (0.0500)  time: 0.7268  data: 0.0014  max mem: 6186
Epoch: [25]  [26310/40201]  eta: 2:03:51  lr: 0.000002  min_lr: 0.000000  loss: 3.1469 (3.4140)  loss_scale: 65536.0000 (38386.0524)  weight_decay: 0.0500 (0.0500)  time: 0.7165  data: 0.0007  max mem: 6186
Epoch: [25]  [26320/40201]  eta: 2:03:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7462 (3.4141)  loss_scale: 65536.0000 (38396.3673)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.0006  max mem: 6186
Epoch: [25]  [26330/40201]  eta: 2:03:43  lr: 0.000002  min_lr: 0.000000  loss: 3.2670 (3.4140)  loss_scale: 65536.0000 (38406.6744)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.0008  max mem: 6186
Epoch: [25]  [26340/40201]  eta: 2:03:38  lr: 0.000002  min_lr: 0.000000  loss: 3.1586 (3.4140)  loss_scale: 65536.0000 (38416.9737)  weight_decay: 0.0500 (0.0500)  time: 0.7065  data: 0.0007  max mem: 6186
Epoch: [25]  [26350/40201]  eta: 2:03:34  lr: 0.000002  min_lr: 0.000000  loss: 3.2101 (3.4140)  loss_scale: 65536.0000 (38427.2652)  weight_decay: 0.0500 (0.0500)  time: 0.7118  data: 0.0012  max mem: 6186
Epoch: [25]  [26360/40201]  eta: 2:03:29  lr: 0.000002  min_lr: 0.000000  loss: 3.2101 (3.4139)  loss_scale: 65536.0000 (38437.5488)  weight_decay: 0.0500 (0.0500)  time: 0.7099  data: 0.0018  max mem: 6186
Epoch: [25]  [26370/40201]  eta: 2:03:24  lr: 0.000002  min_lr: 0.000000  loss: 3.2627 (3.4140)  loss_scale: 65536.0000 (38447.8247)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0012  max mem: 6186
[2023-07-24 19:10:30,899] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13188
[2023-07-24 19:10:30,899] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:10:30,906] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13188
[2023-07-24 19:10:30,906] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:10:30,906] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [26380/40201]  eta: 2:03:20  lr: 0.000002  min_lr: 0.000000  loss: 3.7239 (3.4141)  loss_scale: 65536.0000 (38453.1243)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0007  max mem: 6186
Epoch: [25]  [26390/40201]  eta: 2:03:15  lr: 0.000002  min_lr: 0.000000  loss: 3.7226 (3.4142)  loss_scale: 32768.0000 (38450.9701)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.0012  max mem: 6186
Epoch: [25]  [26400/40201]  eta: 2:03:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7226 (3.4142)  loss_scale: 32768.0000 (38448.8175)  weight_decay: 0.0500 (0.0500)  time: 0.7205  data: 0.0015  max mem: 6186
Epoch: [25]  [26410/40201]  eta: 2:03:07  lr: 0.000002  min_lr: 0.000000  loss: 3.3300 (3.4142)  loss_scale: 32768.0000 (38446.6666)  weight_decay: 0.0500 (0.0500)  time: 0.7482  data: 0.0016  max mem: 6186
Epoch: [25]  [26420/40201]  eta: 2:03:02  lr: 0.000002  min_lr: 0.000000  loss: 2.9957 (3.4142)  loss_scale: 32768.0000 (38444.5173)  weight_decay: 0.0500 (0.0500)  time: 0.7263  data: 0.0010  max mem: 6186
Epoch: [25]  [26430/40201]  eta: 2:02:58  lr: 0.000002  min_lr: 0.000000  loss: 3.3783 (3.4141)  loss_scale: 32768.0000 (38442.3696)  weight_decay: 0.0500 (0.0500)  time: 0.7331  data: 0.0104  max mem: 6186
Epoch: [25]  [26440/40201]  eta: 2:02:54  lr: 0.000002  min_lr: 0.000000  loss: 3.4770 (3.4141)  loss_scale: 32768.0000 (38440.2236)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.0103  max mem: 6186
Epoch: [25]  [26450/40201]  eta: 2:02:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5250 (3.4141)  loss_scale: 32768.0000 (38438.0792)  weight_decay: 0.0500 (0.0500)  time: 0.9476  data: 0.2576  max mem: 6186
Epoch: [25]  [26460/40201]  eta: 2:02:47  lr: 0.000002  min_lr: 0.000000  loss: 3.1563 (3.4140)  loss_scale: 32768.0000 (38435.9364)  weight_decay: 0.0500 (0.0500)  time: 0.9542  data: 0.2579  max mem: 6186
Epoch: [25]  [26470/40201]  eta: 2:02:43  lr: 0.000002  min_lr: 0.000000  loss: 3.2501 (3.4141)  loss_scale: 32768.0000 (38433.7952)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.0012  max mem: 6186
Epoch: [25]  [26480/40201]  eta: 2:02:37  lr: 0.000002  min_lr: 0.000000  loss: 3.4668 (3.4141)  loss_scale: 32768.0000 (38431.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0011  max mem: 6186
Epoch: [25]  [26490/40201]  eta: 2:02:31  lr: 0.000002  min_lr: 0.000000  loss: 3.4668 (3.4141)  loss_scale: 32768.0000 (38429.5176)  weight_decay: 0.0500 (0.0500)  time: 0.4204  data: 0.0007  max mem: 6186
Epoch: [25]  [26500/40201]  eta: 2:02:24  lr: 0.000002  min_lr: 0.000000  loss: 3.0820 (3.4140)  loss_scale: 32768.0000 (38427.3813)  weight_decay: 0.0500 (0.0500)  time: 0.3477  data: 0.0090  max mem: 6186
Epoch: [25]  [26510/40201]  eta: 2:02:19  lr: 0.000002  min_lr: 0.000000  loss: 3.5179 (3.4142)  loss_scale: 32768.0000 (38425.2466)  weight_decay: 0.0500 (0.0500)  time: 0.3901  data: 0.0554  max mem: 6186
Epoch: [25]  [26520/40201]  eta: 2:02:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4622 (3.4141)  loss_scale: 32768.0000 (38423.1135)  weight_decay: 0.0500 (0.0500)  time: 0.4473  data: 0.0777  max mem: 6186
Epoch: [25]  [26530/40201]  eta: 2:02:07  lr: 0.000002  min_lr: 0.000000  loss: 3.4293 (3.4141)  loss_scale: 32768.0000 (38420.9819)  weight_decay: 0.0500 (0.0500)  time: 0.4193  data: 0.0316  max mem: 6186
Epoch: [25]  [26540/40201]  eta: 2:02:01  lr: 0.000002  min_lr: 0.000000  loss: 3.4140 (3.4141)  loss_scale: 32768.0000 (38418.8520)  weight_decay: 0.0500 (0.0500)  time: 0.3878  data: 0.0032  max mem: 6186
Epoch: [25]  [26550/40201]  eta: 2:01:57  lr: 0.000002  min_lr: 0.000000  loss: 3.4140 (3.4141)  loss_scale: 32768.0000 (38416.7237)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.0030  max mem: 6186
Epoch: [25]  [26560/40201]  eta: 2:01:53  lr: 0.000002  min_lr: 0.000000  loss: 3.1970 (3.4140)  loss_scale: 32768.0000 (38414.5970)  weight_decay: 0.0500 (0.0500)  time: 0.8318  data: 0.0007  max mem: 6186
Epoch: [25]  [26570/40201]  eta: 2:01:48  lr: 0.000002  min_lr: 0.000000  loss: 3.0871 (3.4140)  loss_scale: 32768.0000 (38412.4719)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.0019  max mem: 6186
Epoch: [25]  [26580/40201]  eta: 2:01:44  lr: 0.000002  min_lr: 0.000000  loss: 3.0000 (3.4138)  loss_scale: 32768.0000 (38410.3484)  weight_decay: 0.0500 (0.0500)  time: 0.7378  data: 0.0019  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/EuGXJiVQwCg_000005_000015.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [26590/40201]  eta: 2:01:38  lr: 0.000002  min_lr: 0.000000  loss: 3.1539 (3.4138)  loss_scale: 32768.0000 (38408.2265)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0010  max mem: 6186
Epoch: [25]  [26600/40201]  eta: 2:01:32  lr: 0.000002  min_lr: 0.000000  loss: 3.4001 (3.4138)  loss_scale: 32768.0000 (38406.1062)  weight_decay: 0.0500 (0.0500)  time: 0.3463  data: 0.0012  max mem: 6186
Epoch: [25]  [26610/40201]  eta: 2:01:25  lr: 0.000002  min_lr: 0.000000  loss: 3.3687 (3.4138)  loss_scale: 32768.0000 (38403.9875)  weight_decay: 0.0500 (0.0500)  time: 0.3547  data: 0.0291  max mem: 6186
Epoch: [25]  [26620/40201]  eta: 2:01:20  lr: 0.000002  min_lr: 0.000000  loss: 3.4065 (3.4139)  loss_scale: 32768.0000 (38401.8704)  weight_decay: 0.0500 (0.0500)  time: 0.4257  data: 0.0834  max mem: 6186
Epoch: [25]  [26630/40201]  eta: 2:01:20  lr: 0.000002  min_lr: 0.000000  loss: 3.5524 (3.4138)  loss_scale: 32768.0000 (38399.7549)  weight_decay: 0.0500 (0.0500)  time: 1.0271  data: 0.5307  max mem: 6186
[2023-07-24 19:13:16,906] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:13:16,906] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:13:16,931] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:13:16,932] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [26640/40201]  eta: 2:01:15  lr: 0.000002  min_lr: 0.000000  loss: 3.0568 (3.4137)  loss_scale: 32768.0000 (38405.0208)  weight_decay: 0.0500 (0.0500)  time: 1.1251  data: 0.4766  max mem: 6186
Epoch: [25]  [26650/40201]  eta: 2:01:10  lr: 0.000002  min_lr: 0.000000  loss: 3.0667 (3.4137)  loss_scale: 65536.0000 (38415.2009)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.0012  max mem: 6186
Epoch: [25]  [26660/40201]  eta: 2:01:06  lr: 0.000002  min_lr: 0.000000  loss: 3.3283 (3.4137)  loss_scale: 65536.0000 (38425.3734)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.0013  max mem: 6186
[2023-07-24 19:13:37,574] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13332
[2023-07-24 19:13:37,575] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:13:37,575] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:13:37,576] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13332
[2023-07-24 19:13:37,576] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [26670/40201]  eta: 2:01:01  lr: 0.000002  min_lr: 0.000000  loss: 3.4774 (3.4138)  loss_scale: 65536.0000 (38428.1666)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.0016  max mem: 6186
Epoch: [25]  [26680/40201]  eta: 2:00:57  lr: 0.000002  min_lr: 0.000000  loss: 3.5476 (3.4138)  loss_scale: 32768.0000 (38426.0452)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.0013  max mem: 6186
Epoch: [25]  [26690/40201]  eta: 2:00:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5802 (3.4139)  loss_scale: 32768.0000 (38423.9254)  weight_decay: 0.0500 (0.0500)  time: 0.7024  data: 0.0014  max mem: 6186
Epoch: [25]  [26700/40201]  eta: 2:00:48  lr: 0.000002  min_lr: 0.000000  loss: 3.6076 (3.4140)  loss_scale: 32768.0000 (38421.8071)  weight_decay: 0.0500 (0.0500)  time: 0.6990  data: 0.0025  max mem: 6186
Epoch: [25]  [26710/40201]  eta: 2:00:43  lr: 0.000002  min_lr: 0.000000  loss: 3.5892 (3.4140)  loss_scale: 32768.0000 (38419.6905)  weight_decay: 0.0500 (0.0500)  time: 0.7065  data: 0.0019  max mem: 6186
Epoch: [25]  [26720/40201]  eta: 2:00:39  lr: 0.000002  min_lr: 0.000000  loss: 3.3941 (3.4139)  loss_scale: 32768.0000 (38417.5754)  weight_decay: 0.0500 (0.0500)  time: 0.7060  data: 0.0016  max mem: 6186
Epoch: [25]  [26730/40201]  eta: 2:00:34  lr: 0.000002  min_lr: 0.000000  loss: 3.4360 (3.4140)  loss_scale: 32768.0000 (38415.4619)  weight_decay: 0.0500 (0.0500)  time: 0.7353  data: 0.0013  max mem: 6186
Epoch: [25]  [26740/40201]  eta: 2:00:30  lr: 0.000002  min_lr: 0.000000  loss: 3.3689 (3.4139)  loss_scale: 32768.0000 (38413.3500)  weight_decay: 0.0500 (0.0500)  time: 0.7399  data: 0.0008  max mem: 6186
Epoch: [25]  [26750/40201]  eta: 2:00:25  lr: 0.000002  min_lr: 0.000000  loss: 3.2101 (3.4139)  loss_scale: 32768.0000 (38411.2397)  weight_decay: 0.0500 (0.0500)  time: 0.7017  data: 0.0011  max mem: 6186
Epoch: [25]  [26760/40201]  eta: 2:00:21  lr: 0.000002  min_lr: 0.000000  loss: 3.2101 (3.4138)  loss_scale: 32768.0000 (38409.1309)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.0012  max mem: 6186
Epoch: [25]  [26770/40201]  eta: 2:00:16  lr: 0.000002  min_lr: 0.000000  loss: 3.0027 (3.4138)  loss_scale: 32768.0000 (38407.0237)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.0018  max mem: 6186
Epoch: [25]  [26780/40201]  eta: 2:00:12  lr: 0.000002  min_lr: 0.000000  loss: 3.5285 (3.4140)  loss_scale: 32768.0000 (38404.9181)  weight_decay: 0.0500 (0.0500)  time: 0.7445  data: 0.0017  max mem: 6186
Epoch: [25]  [26790/40201]  eta: 2:00:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5519 (3.4140)  loss_scale: 32768.0000 (38402.8141)  weight_decay: 0.0500 (0.0500)  time: 0.7190  data: 0.0012  max mem: 6186
Epoch: [25]  [26800/40201]  eta: 2:00:03  lr: 0.000002  min_lr: 0.000000  loss: 3.4424 (3.4140)  loss_scale: 32768.0000 (38400.7116)  weight_decay: 0.0500 (0.0500)  time: 0.7093  data: 0.0013  max mem: 6186
Epoch: [25]  [26810/40201]  eta: 1:59:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4424 (3.4140)  loss_scale: 32768.0000 (38398.6107)  weight_decay: 0.0500 (0.0500)  time: 0.7099  data: 0.0012  max mem: 6186
Epoch: [25]  [26820/40201]  eta: 1:59:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5303 (3.4139)  loss_scale: 32768.0000 (38396.5114)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.0015  max mem: 6186
Epoch: [25]  [26830/40201]  eta: 1:59:49  lr: 0.000002  min_lr: 0.000000  loss: 3.4858 (3.4140)  loss_scale: 32768.0000 (38394.4136)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.0014  max mem: 6186
Epoch: [25]  [26840/40201]  eta: 1:59:43  lr: 0.000002  min_lr: 0.000000  loss: 3.1713 (3.4139)  loss_scale: 32768.0000 (38392.3174)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0008  max mem: 6186
Epoch: [25]  [26850/40201]  eta: 1:59:37  lr: 0.000002  min_lr: 0.000000  loss: 3.1549 (3.4140)  loss_scale: 32768.0000 (38390.2228)  weight_decay: 0.0500 (0.0500)  time: 0.3613  data: 0.0012  max mem: 6186
Epoch: [25]  [26860/40201]  eta: 1:59:31  lr: 0.000002  min_lr: 0.000000  loss: 3.2182 (3.4139)  loss_scale: 32768.0000 (38388.1297)  weight_decay: 0.0500 (0.0500)  time: 0.3621  data: 0.0070  max mem: 6186
Epoch: [25]  [26870/40201]  eta: 1:59:25  lr: 0.000002  min_lr: 0.000000  loss: 2.8933 (3.4138)  loss_scale: 32768.0000 (38386.0382)  weight_decay: 0.0500 (0.0500)  time: 0.3882  data: 0.0362  max mem: 6186
Epoch: [25]  [26880/40201]  eta: 1:59:19  lr: 0.000002  min_lr: 0.000000  loss: 3.1302 (3.4139)  loss_scale: 32768.0000 (38383.9482)  weight_decay: 0.0500 (0.0500)  time: 0.4160  data: 0.0588  max mem: 6186
Epoch: [25]  [26890/40201]  eta: 1:59:13  lr: 0.000002  min_lr: 0.000000  loss: 3.5197 (3.4140)  loss_scale: 32768.0000 (38381.8598)  weight_decay: 0.0500 (0.0500)  time: 0.4366  data: 0.0709  max mem: 6186
Epoch: [25]  [26900/40201]  eta: 1:59:11  lr: 0.000002  min_lr: 0.000000  loss: 3.3028 (3.4140)  loss_scale: 32768.0000 (38379.7729)  weight_decay: 0.0500 (0.0500)  time: 0.8345  data: 0.3041  max mem: 6186
Epoch: [25]  [26910/40201]  eta: 1:59:05  lr: 0.000002  min_lr: 0.000000  loss: 3.1674 (3.4140)  loss_scale: 32768.0000 (38377.6876)  weight_decay: 0.0500 (0.0500)  time: 0.8356  data: 0.2621  max mem: 6186
Epoch: [25]  [26920/40201]  eta: 1:58:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4078 (3.4142)  loss_scale: 32768.0000 (38375.6039)  weight_decay: 0.0500 (0.0500)  time: 0.4471  data: 0.0010  max mem: 6186
[2023-07-24 19:16:20,814] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:16:20,814] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:16:20,815] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:16:20,815] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [26930/40201]  eta: 1:58:53  lr: 0.000002  min_lr: 0.000000  loss: 3.5222 (3.4141)  loss_scale: 32768.0000 (38383.2556)  weight_decay: 0.0500 (0.0500)  time: 0.3691  data: 0.0015  max mem: 6186
Epoch: [25]  [26940/40201]  eta: 1:58:50  lr: 0.000002  min_lr: 0.000000  loss: 3.0645 (3.4141)  loss_scale: 65536.0000 (38393.3342)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.0031  max mem: 6186
Epoch: [25]  [26950/40201]  eta: 1:58:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5508 (3.4142)  loss_scale: 65536.0000 (38403.4053)  weight_decay: 0.0500 (0.0500)  time: 0.7079  data: 0.0036  max mem: 6186
Epoch: [25]  [26960/40201]  eta: 1:58:39  lr: 0.000002  min_lr: 0.000000  loss: 3.8019 (3.4144)  loss_scale: 65536.0000 (38413.4689)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0896  max mem: 6186
Epoch: [25]  [26970/40201]  eta: 1:58:35  lr: 0.000002  min_lr: 0.000000  loss: 3.3167 (3.4143)  loss_scale: 65536.0000 (38423.5251)  weight_decay: 0.0500 (0.0500)  time: 0.7107  data: 0.0890  max mem: 6186
Epoch: [25]  [26980/40201]  eta: 1:58:30  lr: 0.000002  min_lr: 0.000000  loss: 2.9724 (3.4142)  loss_scale: 65536.0000 (38433.5738)  weight_decay: 0.0500 (0.0500)  time: 0.7240  data: 0.0024  max mem: 6186
Epoch: [25]  [26990/40201]  eta: 1:58:26  lr: 0.000002  min_lr: 0.000000  loss: 3.0487 (3.4142)  loss_scale: 65536.0000 (38443.6151)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.0022  max mem: 6186
[2023-07-24 19:17:14,198] [INFO] [timer.py:181:stop] 0/27000, SamplesPerSec=13.202769300259813
Epoch: [25]  [27000/40201]  eta: 1:58:22  lr: 0.000002  min_lr: 0.000000  loss: 3.2845 (3.4142)  loss_scale: 65536.0000 (38453.6490)  weight_decay: 0.0500 (0.0500)  time: 0.7911  data: 0.0010  max mem: 6186
[2023-07-24 19:17:20,078] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13503
[2023-07-24 19:17:20,078] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:17:20,082] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13503
[2023-07-24 19:17:20,082] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:17:20,082] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [27010/40201]  eta: 1:58:18  lr: 0.000002  min_lr: 0.000000  loss: 3.3125 (3.4142)  loss_scale: 65536.0000 (38458.8228)  weight_decay: 0.0500 (0.0500)  time: 0.8105  data: 0.0011  max mem: 6186
Epoch: [25]  [27020/40201]  eta: 1:58:13  lr: 0.000002  min_lr: 0.000000  loss: 3.0486 (3.4140)  loss_scale: 32768.0000 (38456.7168)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.0009  max mem: 6186
Epoch: [25]  [27030/40201]  eta: 1:58:08  lr: 0.000002  min_lr: 0.000000  loss: 3.4563 (3.4142)  loss_scale: 32768.0000 (38454.6123)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0010  max mem: 6186
Epoch: [25]  [27040/40201]  eta: 1:58:02  lr: 0.000002  min_lr: 0.000000  loss: 3.7300 (3.4140)  loss_scale: 32768.0000 (38452.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0010  max mem: 6186
Epoch: [25]  [27050/40201]  eta: 1:57:56  lr: 0.000002  min_lr: 0.000000  loss: 3.2852 (3.4141)  loss_scale: 32768.0000 (38450.4079)  weight_decay: 0.0500 (0.0500)  time: 0.4450  data: 0.0008  max mem: 6186
Epoch: [25]  [27060/40201]  eta: 1:57:52  lr: 0.000002  min_lr: 0.000000  loss: 3.2852 (3.4141)  loss_scale: 32768.0000 (38448.3080)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.2442  max mem: 6186
Epoch: [25]  [27070/40201]  eta: 1:57:45  lr: 0.000002  min_lr: 0.000000  loss: 3.4762 (3.4141)  loss_scale: 32768.0000 (38446.2097)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.2985  max mem: 6186
Epoch: [25]  [27080/40201]  eta: 1:57:40  lr: 0.000002  min_lr: 0.000000  loss: 3.4762 (3.4141)  loss_scale: 32768.0000 (38444.1130)  weight_decay: 0.0500 (0.0500)  time: 0.4596  data: 0.1481  max mem: 6186
Epoch: [25]  [27090/40201]  eta: 1:57:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4492 (3.4142)  loss_scale: 32768.0000 (38442.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0938  max mem: 6186
Epoch: [25]  [27100/40201]  eta: 1:57:30  lr: 0.000002  min_lr: 0.000000  loss: 3.8248 (3.4143)  loss_scale: 32768.0000 (38439.9241)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0006  max mem: 6186
Epoch: [25]  [27110/40201]  eta: 1:57:25  lr: 0.000002  min_lr: 0.000000  loss: 3.3793 (3.4143)  loss_scale: 32768.0000 (38437.8320)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0011  max mem: 6186
Epoch: [25]  [27120/40201]  eta: 1:57:20  lr: 0.000002  min_lr: 0.000000  loss: 3.3793 (3.4144)  loss_scale: 32768.0000 (38435.7415)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0014  max mem: 6186
Epoch: [25]  [27130/40201]  eta: 1:57:16  lr: 0.000002  min_lr: 0.000000  loss: 3.7419 (3.4144)  loss_scale: 32768.0000 (38433.6524)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0009  max mem: 6186
Epoch: [25]  [27140/40201]  eta: 1:57:11  lr: 0.000002  min_lr: 0.000000  loss: 3.0865 (3.4143)  loss_scale: 32768.0000 (38431.5649)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0005  max mem: 6186
Epoch: [25]  [27150/40201]  eta: 1:57:06  lr: 0.000002  min_lr: 0.000000  loss: 3.1071 (3.4144)  loss_scale: 32768.0000 (38429.4790)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0013  max mem: 6186
Epoch: [25]  [27160/40201]  eta: 1:57:01  lr: 0.000002  min_lr: 0.000000  loss: 3.8818 (3.4146)  loss_scale: 32768.0000 (38427.3946)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0015  max mem: 6186
Epoch: [25]  [27170/40201]  eta: 1:56:56  lr: 0.000002  min_lr: 0.000000  loss: 3.4573 (3.4145)  loss_scale: 32768.0000 (38425.3117)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0015  max mem: 6186
Epoch: [25]  [27180/40201]  eta: 1:56:51  lr: 0.000002  min_lr: 0.000000  loss: 3.0746 (3.4144)  loss_scale: 32768.0000 (38423.2303)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0019  max mem: 6186
Epoch: [25]  [27190/40201]  eta: 1:56:46  lr: 0.000002  min_lr: 0.000000  loss: 3.0492 (3.4143)  loss_scale: 32768.0000 (38421.1505)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0013  max mem: 6186
Epoch: [25]  [27200/40201]  eta: 1:56:41  lr: 0.000002  min_lr: 0.000000  loss: 3.4474 (3.4145)  loss_scale: 32768.0000 (38419.0722)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0010  max mem: 6186
Epoch: [25]  [27210/40201]  eta: 1:56:36  lr: 0.000002  min_lr: 0.000000  loss: 3.6827 (3.4145)  loss_scale: 32768.0000 (38416.9955)  weight_decay: 0.0500 (0.0500)  time: 0.6858  data: 0.0011  max mem: 6186
Epoch: [25]  [27220/40201]  eta: 1:56:32  lr: 0.000002  min_lr: 0.000000  loss: 3.3941 (3.4145)  loss_scale: 32768.0000 (38414.9202)  weight_decay: 0.0500 (0.0500)  time: 0.6803  data: 0.0013  max mem: 6186
Epoch: [25]  [27230/40201]  eta: 1:56:27  lr: 0.000002  min_lr: 0.000000  loss: 3.0503 (3.4144)  loss_scale: 32768.0000 (38412.8465)  weight_decay: 0.0500 (0.0500)  time: 0.7054  data: 0.0011  max mem: 6186
Epoch: [25]  [27240/40201]  eta: 1:56:22  lr: 0.000002  min_lr: 0.000000  loss: 2.8748 (3.4143)  loss_scale: 32768.0000 (38410.7743)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.0011  max mem: 6186
Epoch: [25]  [27250/40201]  eta: 1:56:17  lr: 0.000002  min_lr: 0.000000  loss: 3.3842 (3.4143)  loss_scale: 32768.0000 (38408.7037)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0013  max mem: 6186
Epoch: [25]  [27260/40201]  eta: 1:56:13  lr: 0.000002  min_lr: 0.000000  loss: 3.1777 (3.4142)  loss_scale: 32768.0000 (38406.6345)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.0010  max mem: 6186
[2023-07-24 19:20:00,401] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:20:00,402] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:20:00,413] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:20:00,413] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [27270/40201]  eta: 1:56:08  lr: 0.000002  min_lr: 0.000000  loss: 3.1777 (3.4142)  loss_scale: 32768.0000 (38411.7763)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0011  max mem: 6186
Epoch: [25]  [27280/40201]  eta: 1:56:03  lr: 0.000002  min_lr: 0.000000  loss: 3.5998 (3.4144)  loss_scale: 65536.0000 (38421.7189)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.0011  max mem: 6186
Epoch: [25]  [27290/40201]  eta: 1:55:59  lr: 0.000002  min_lr: 0.000000  loss: 3.7322 (3.4144)  loss_scale: 65536.0000 (38431.6541)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.0014  max mem: 6186
Epoch: [25]  [27300/40201]  eta: 1:55:54  lr: 0.000002  min_lr: 0.000000  loss: 3.1882 (3.4143)  loss_scale: 65536.0000 (38441.5821)  weight_decay: 0.0500 (0.0500)  time: 0.7162  data: 0.0015  max mem: 6186
Epoch: [25]  [27310/40201]  eta: 1:55:50  lr: 0.000002  min_lr: 0.000000  loss: 3.1882 (3.4143)  loss_scale: 65536.0000 (38451.5028)  weight_decay: 0.0500 (0.0500)  time: 0.7251  data: 0.0011  max mem: 6186
Epoch: [25]  [27320/40201]  eta: 1:55:45  lr: 0.000002  min_lr: 0.000000  loss: 3.7433 (3.4144)  loss_scale: 65536.0000 (38461.4162)  weight_decay: 0.0500 (0.0500)  time: 0.7041  data: 0.0007  max mem: 6186
Epoch: [25]  [27330/40201]  eta: 1:55:40  lr: 0.000002  min_lr: 0.000000  loss: 3.5072 (3.4144)  loss_scale: 65536.0000 (38471.3224)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.0007  max mem: 6186
Epoch: [25]  [27340/40201]  eta: 1:55:35  lr: 0.000002  min_lr: 0.000000  loss: 2.8582 (3.4141)  loss_scale: 65536.0000 (38481.2213)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.0009  max mem: 6186
Epoch: [25]  [27350/40201]  eta: 1:55:30  lr: 0.000002  min_lr: 0.000000  loss: 3.0056 (3.4141)  loss_scale: 65536.0000 (38491.1130)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.0006  max mem: 6186
Epoch: [25]  [27360/40201]  eta: 1:55:26  lr: 0.000002  min_lr: 0.000000  loss: 3.2513 (3.4141)  loss_scale: 65536.0000 (38500.9975)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0006  max mem: 6186
Epoch: [25]  [27370/40201]  eta: 1:55:20  lr: 0.000002  min_lr: 0.000000  loss: 3.1443 (3.4139)  loss_scale: 65536.0000 (38510.8747)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0027  max mem: 6186
Epoch: [25]  [27380/40201]  eta: 1:55:14  lr: 0.000002  min_lr: 0.000000  loss: 2.8908 (3.4138)  loss_scale: 65536.0000 (38520.7448)  weight_decay: 0.0500 (0.0500)  time: 0.4773  data: 0.0026  max mem: 6186
Epoch: [25]  [27390/40201]  eta: 1:55:08  lr: 0.000002  min_lr: 0.000000  loss: 3.0496 (3.4139)  loss_scale: 65536.0000 (38530.6076)  weight_decay: 0.0500 (0.0500)  time: 0.3204  data: 0.0006  max mem: 6186
Epoch: [25]  [27400/40201]  eta: 1:55:05  lr: 0.000002  min_lr: 0.000000  loss: 3.1392 (3.4138)  loss_scale: 65536.0000 (38540.4632)  weight_decay: 0.0500 (0.0500)  time: 0.7233  data: 0.0746  max mem: 6186
Epoch: [25]  [27410/40201]  eta: 1:54:59  lr: 0.000002  min_lr: 0.000000  loss: 3.2931 (3.4138)  loss_scale: 65536.0000 (38550.3116)  weight_decay: 0.0500 (0.0500)  time: 0.8199  data: 0.0748  max mem: 6186
Epoch: [25]  [27420/40201]  eta: 1:54:53  lr: 0.000002  min_lr: 0.000000  loss: 3.3172 (3.4137)  loss_scale: 65536.0000 (38560.1529)  weight_decay: 0.0500 (0.0500)  time: 0.3967  data: 0.0009  max mem: 6186
Epoch: [25]  [27430/40201]  eta: 1:54:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3051 (3.4138)  loss_scale: 65536.0000 (38569.9869)  weight_decay: 0.0500 (0.0500)  time: 0.4789  data: 0.0010  max mem: 6186
Epoch: [25]  [27440/40201]  eta: 1:54:44  lr: 0.000002  min_lr: 0.000000  loss: 3.3974 (3.4138)  loss_scale: 65536.0000 (38579.8139)  weight_decay: 0.0500 (0.0500)  time: 0.7613  data: 0.0011  max mem: 6186
Epoch: [25]  [27450/40201]  eta: 1:54:39  lr: 0.000002  min_lr: 0.000000  loss: 3.3974 (3.4138)  loss_scale: 65536.0000 (38589.6336)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.0011  max mem: 6186
Epoch: [25]  [27460/40201]  eta: 1:54:34  lr: 0.000002  min_lr: 0.000000  loss: 3.2782 (3.4138)  loss_scale: 65536.0000 (38599.4462)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0017  max mem: 6186
Epoch: [25]  [27470/40201]  eta: 1:54:29  lr: 0.000002  min_lr: 0.000000  loss: 3.0719 (3.4137)  loss_scale: 65536.0000 (38609.2516)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0017  max mem: 6186
Epoch: [25]  [27480/40201]  eta: 1:54:23  lr: 0.000002  min_lr: 0.000000  loss: 3.0321 (3.4135)  loss_scale: 65536.0000 (38619.0500)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0010  max mem: 6186
Epoch: [25]  [27490/40201]  eta: 1:54:18  lr: 0.000002  min_lr: 0.000000  loss: 3.1625 (3.4135)  loss_scale: 65536.0000 (38628.8411)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0012  max mem: 6186
Epoch: [25]  [27500/40201]  eta: 1:54:13  lr: 0.000002  min_lr: 0.000000  loss: 3.3257 (3.4135)  loss_scale: 65536.0000 (38638.6252)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0011  max mem: 6186
Epoch: [25]  [27510/40201]  eta: 1:54:07  lr: 0.000002  min_lr: 0.000000  loss: 3.3560 (3.4135)  loss_scale: 65536.0000 (38648.4022)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0005  max mem: 6186
Epoch: [25]  [27520/40201]  eta: 1:54:02  lr: 0.000002  min_lr: 0.000000  loss: 3.2352 (3.4134)  loss_scale: 65536.0000 (38658.1720)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0011  max mem: 6186
[2023-07-24 19:22:38,769] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:22:38,770] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 19:22:38,790] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:22:38,790] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 19:22:39,598] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13761
[2023-07-24 19:22:39,599] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 19:22:39,599] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13761
[2023-07-24 19:22:39,599] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 19:22:39,600] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [27530/40201]  eta: 1:53:57  lr: 0.000002  min_lr: 0.000000  loss: 3.0574 (3.4133)  loss_scale: 65536.0000 (38672.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0013  max mem: 6186
Epoch: [25]  [27540/40201]  eta: 1:53:50  lr: 0.000002  min_lr: 0.000000  loss: 3.0574 (3.4132)  loss_scale: 65536.0000 (38682.4496)  weight_decay: 0.0500 (0.0500)  time: 0.4113  data: 0.0007  max mem: 6186
Epoch: [25]  [27550/40201]  eta: 1:53:45  lr: 0.000002  min_lr: 0.000000  loss: 3.1612 (3.4133)  loss_scale: 65536.0000 (38692.1964)  weight_decay: 0.0500 (0.0500)  time: 0.4235  data: 0.1119  max mem: 6186
Epoch: [25]  [27560/40201]  eta: 1:53:39  lr: 0.000002  min_lr: 0.000000  loss: 3.3910 (3.4133)  loss_scale: 65536.0000 (38701.9362)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.2398  max mem: 6186
Epoch: [25]  [27570/40201]  eta: 1:53:33  lr: 0.000002  min_lr: 0.000000  loss: 3.2144 (3.4132)  loss_scale: 65536.0000 (38711.6689)  weight_decay: 0.0500 (0.0500)  time: 0.3884  data: 0.1329  max mem: 6186
Epoch: [25]  [27580/40201]  eta: 1:53:27  lr: 0.000002  min_lr: 0.000000  loss: 3.3123 (3.4132)  loss_scale: 65536.0000 (38721.3946)  weight_decay: 0.0500 (0.0500)  time: 0.3615  data: 0.1133  max mem: 6186
Epoch: [25]  [27590/40201]  eta: 1:53:24  lr: 0.000002  min_lr: 0.000000  loss: 3.3123 (3.4131)  loss_scale: 65536.0000 (38731.1132)  weight_decay: 0.0500 (0.0500)  time: 0.7835  data: 0.1088  max mem: 6186
Epoch: [25]  [27600/40201]  eta: 1:53:19  lr: 0.000002  min_lr: 0.000000  loss: 3.4468 (3.4132)  loss_scale: 65536.0000 (38740.8248)  weight_decay: 0.0500 (0.0500)  time: 0.8674  data: 0.0005  max mem: 6186
Epoch: [25]  [27610/40201]  eta: 1:53:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6482 (3.4133)  loss_scale: 65536.0000 (38750.5293)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0005  max mem: 6186
Epoch: [25]  [27620/40201]  eta: 1:53:09  lr: 0.000002  min_lr: 0.000000  loss: 3.4200 (3.4132)  loss_scale: 65536.0000 (38760.2268)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0006  max mem: 6186
Epoch: [25]  [27630/40201]  eta: 1:53:03  lr: 0.000002  min_lr: 0.000000  loss: 3.3609 (3.4133)  loss_scale: 65536.0000 (38769.9173)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0006  max mem: 6186
Epoch: [25]  [27640/40201]  eta: 1:52:58  lr: 0.000002  min_lr: 0.000000  loss: 3.6384 (3.4133)  loss_scale: 65536.0000 (38779.6007)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0011  max mem: 6186
[2023-07-24 19:23:47,272] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13822
[2023-07-24 19:23:47,272] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:23:47,275] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13822
[2023-07-24 19:23:47,275] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:23:47,275] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [27650/40201]  eta: 1:52:53  lr: 0.000002  min_lr: 0.000000  loss: 3.1281 (3.4132)  loss_scale: 65536.0000 (38782.1669)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0009  max mem: 6186
Epoch: [25]  [27660/40201]  eta: 1:52:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3319 (3.4132)  loss_scale: 32768.0000 (38779.9926)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0005  max mem: 6186
Epoch: [25]  [27670/40201]  eta: 1:52:43  lr: 0.000002  min_lr: 0.000000  loss: 3.4154 (3.4132)  loss_scale: 32768.0000 (38777.8200)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0006  max mem: 6186
Epoch: [25]  [27680/40201]  eta: 1:52:37  lr: 0.000002  min_lr: 0.000000  loss: 3.2395 (3.4131)  loss_scale: 32768.0000 (38775.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0008  max mem: 6186
Epoch: [25]  [27690/40201]  eta: 1:52:32  lr: 0.000002  min_lr: 0.000000  loss: 3.3968 (3.4132)  loss_scale: 32768.0000 (38773.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0008  max mem: 6186
Epoch: [25]  [27700/40201]  eta: 1:52:25  lr: 0.000002  min_lr: 0.000000  loss: 3.4779 (3.4133)  loss_scale: 32768.0000 (38771.3114)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0009  max mem: 6186
Epoch: [25]  [27710/40201]  eta: 1:52:19  lr: 0.000002  min_lr: 0.000000  loss: 3.2884 (3.4133)  loss_scale: 32768.0000 (38769.1450)  weight_decay: 0.0500 (0.0500)  time: 0.2390  data: 0.0140  max mem: 6186
Epoch: [25]  [27720/40201]  eta: 1:52:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7968 (3.4134)  loss_scale: 32768.0000 (38766.9801)  weight_decay: 0.0500 (0.0500)  time: 0.3979  data: 0.1579  max mem: 6186
Epoch: [25]  [27730/40201]  eta: 1:52:07  lr: 0.000002  min_lr: 0.000000  loss: 3.3374 (3.4132)  loss_scale: 32768.0000 (38764.8168)  weight_decay: 0.0500 (0.0500)  time: 0.4338  data: 0.1853  max mem: 6186
Epoch: [25]  [27740/40201]  eta: 1:52:01  lr: 0.000002  min_lr: 0.000000  loss: 3.0106 (3.4132)  loss_scale: 32768.0000 (38762.6551)  weight_decay: 0.0500 (0.0500)  time: 0.3622  data: 0.1091  max mem: 6186
Epoch: [25]  [27750/40201]  eta: 1:51:55  lr: 0.000002  min_lr: 0.000000  loss: 3.1898 (3.4131)  loss_scale: 32768.0000 (38760.4950)  weight_decay: 0.0500 (0.0500)  time: 0.4304  data: 0.1785  max mem: 6186
Epoch: [25]  [27760/40201]  eta: 1:51:49  lr: 0.000002  min_lr: 0.000000  loss: 3.1898 (3.4130)  loss_scale: 32768.0000 (38758.3364)  weight_decay: 0.0500 (0.0500)  time: 0.4306  data: 0.1646  max mem: 6186
Epoch: [25]  [27770/40201]  eta: 1:51:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5457 (3.4130)  loss_scale: 32768.0000 (38756.1793)  weight_decay: 0.0500 (0.0500)  time: 0.5034  data: 0.0552  max mem: 6186
Epoch: [25]  [27780/40201]  eta: 1:51:39  lr: 0.000002  min_lr: 0.000000  loss: 3.4113 (3.4130)  loss_scale: 32768.0000 (38754.0238)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0026  max mem: 6186
Epoch: [25]  [27790/40201]  eta: 1:51:33  lr: 0.000002  min_lr: 0.000000  loss: 3.4113 (3.4130)  loss_scale: 32768.0000 (38751.8699)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0021  max mem: 6186
Epoch: [25]  [27800/40201]  eta: 1:51:28  lr: 0.000002  min_lr: 0.000000  loss: 3.5176 (3.4131)  loss_scale: 32768.0000 (38749.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0006  max mem: 6186
Epoch: [25]  [27810/40201]  eta: 1:51:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7288 (3.4132)  loss_scale: 32768.0000 (38747.5666)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0011  max mem: 6186
Epoch: [25]  [27820/40201]  eta: 1:51:18  lr: 0.000002  min_lr: 0.000000  loss: 3.4820 (3.4132)  loss_scale: 32768.0000 (38745.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0013  max mem: 6186
Epoch: [25]  [27830/40201]  eta: 1:51:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4820 (3.4133)  loss_scale: 32768.0000 (38743.2696)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0012  max mem: 6186
Epoch: [25]  [27840/40201]  eta: 1:51:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5635 (3.4133)  loss_scale: 32768.0000 (38741.1234)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0012  max mem: 6186
Epoch: [25]  [27850/40201]  eta: 1:51:02  lr: 0.000002  min_lr: 0.000000  loss: 3.3066 (3.4134)  loss_scale: 32768.0000 (38738.9787)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0013  max mem: 6186
Epoch: [25]  [27860/40201]  eta: 1:50:57  lr: 0.000002  min_lr: 0.000000  loss: 3.0993 (3.4133)  loss_scale: 32768.0000 (38736.8356)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0012  max mem: 6186
Epoch: [25]  [27870/40201]  eta: 1:50:53  lr: 0.000002  min_lr: 0.000000  loss: 3.1224 (3.4133)  loss_scale: 32768.0000 (38734.6940)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.0899  max mem: 6186
Epoch: [25]  [27880/40201]  eta: 1:50:47  lr: 0.000002  min_lr: 0.000000  loss: 3.3619 (3.4133)  loss_scale: 32768.0000 (38732.5539)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.0899  max mem: 6186
Epoch: [25]  [27890/40201]  eta: 1:50:42  lr: 0.000002  min_lr: 0.000000  loss: 3.2675 (3.4132)  loss_scale: 32768.0000 (38730.4154)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0013  max mem: 6186
Epoch: [25]  [27900/40201]  eta: 1:50:37  lr: 0.000002  min_lr: 0.000000  loss: 3.3712 (3.4132)  loss_scale: 32768.0000 (38728.2784)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0021  max mem: 6186
[2023-07-24 19:26:04,831] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:26:04,831] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:26:04,834] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:26:04,834] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:26:07,799] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13954
[2023-07-24 19:26:07,799] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:26:07,799] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:26:07,801] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 13954
[2023-07-24 19:26:07,801] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [27910/40201]  eta: 1:50:32  lr: 0.000002  min_lr: 0.000000  loss: 3.3712 (3.4133)  loss_scale: 32768.0000 (38733.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0017  max mem: 6186
Epoch: [25]  [27920/40201]  eta: 1:50:27  lr: 0.000002  min_lr: 0.000000  loss: 3.6746 (3.4135)  loss_scale: 32768.0000 (38731.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0008  max mem: 6186
Epoch: [25]  [27930/40201]  eta: 1:50:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5232 (3.4134)  loss_scale: 32768.0000 (38728.9157)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0006  max mem: 6186
Epoch: [25]  [27940/40201]  eta: 1:50:16  lr: 0.000002  min_lr: 0.000000  loss: 3.3859 (3.4135)  loss_scale: 32768.0000 (38726.7823)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0008  max mem: 6186
Epoch: [25]  [27950/40201]  eta: 1:50:11  lr: 0.000002  min_lr: 0.000000  loss: 3.3859 (3.4135)  loss_scale: 32768.0000 (38724.6504)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0008  max mem: 6186
Epoch: [25]  [27960/40201]  eta: 1:50:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5072 (3.4135)  loss_scale: 32768.0000 (38722.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0005  max mem: 6186
Epoch: [25]  [27970/40201]  eta: 1:50:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6604 (3.4138)  loss_scale: 32768.0000 (38720.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0011  max mem: 6186
Epoch: [25]  [27980/40201]  eta: 1:49:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7356 (3.4139)  loss_scale: 32768.0000 (38718.2640)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0012  max mem: 6186
Epoch: [25]  [27990/40201]  eta: 1:49:51  lr: 0.000002  min_lr: 0.000000  loss: 3.4567 (3.4139)  loss_scale: 32768.0000 (38716.1382)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.1410  max mem: 6186
[2023-07-24 19:26:59,745] [INFO] [logging.py:69:log_dist] [Rank 0] step=14000, skipped=78, lr=[5.665129837980307e-08, 5.665129837980307e-08, 7.55350645064041e-08, 7.55350645064041e-08, 1.0071341934187213e-07, 1.0071341934187213e-07, 1.342845591224962e-07, 1.342845591224962e-07, 1.790460788299949e-07, 1.790460788299949e-07, 2.3872810510665985e-07, 2.3872810510665985e-07, 3.1830414014221315e-07, 3.1830414014221315e-07, 4.2440552018961756e-07, 4.2440552018961756e-07, 5.6587402691949e-07, 5.6587402691949e-07, 7.544987025593201e-07, 7.544987025593201e-07, 1.0059982700790935e-06, 1.0059982700790935e-06, 1.3413310267721246e-06, 1.3413310267721246e-06, 1.7884413690294993e-06, 1.7884413690294993e-06, 2.3845884920393326e-06, 2.3845884920393326e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 19:26:59,746] [INFO] [timer.py:181:stop] 0/28000, SamplesPerSec=13.185635980154764
Epoch: [25]  [28000/40201]  eta: 1:49:45  lr: 0.000002  min_lr: 0.000000  loss: 3.3152 (3.4138)  loss_scale: 32768.0000 (38714.0139)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1411  max mem: 6186
Epoch: [25]  [28010/40201]  eta: 1:49:38  lr: 0.000002  min_lr: 0.000000  loss: 3.0998 (3.4138)  loss_scale: 32768.0000 (38711.8912)  weight_decay: 0.0500 (0.0500)  time: 0.3464  data: 0.0008  max mem: 6186
Epoch: [25]  [28020/40201]  eta: 1:49:33  lr: 0.000002  min_lr: 0.000000  loss: 3.8897 (3.4140)  loss_scale: 32768.0000 (38709.7700)  weight_decay: 0.0500 (0.0500)  time: 0.4474  data: 0.1338  max mem: 6186
Epoch: [25]  [28030/40201]  eta: 1:49:28  lr: 0.000002  min_lr: 0.000000  loss: 3.8897 (3.4140)  loss_scale: 32768.0000 (38707.6502)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1659  max mem: 6186
Epoch: [25]  [28040/40201]  eta: 1:49:22  lr: 0.000002  min_lr: 0.000000  loss: 3.3652 (3.4140)  loss_scale: 32768.0000 (38705.5320)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0331  max mem: 6186
Epoch: [25]  [28050/40201]  eta: 1:49:17  lr: 0.000002  min_lr: 0.000000  loss: 3.1921 (3.4139)  loss_scale: 32768.0000 (38703.4154)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0011  max mem: 6186
Epoch: [25]  [28060/40201]  eta: 1:49:13  lr: 0.000002  min_lr: 0.000000  loss: 3.0535 (3.4137)  loss_scale: 32768.0000 (38701.3002)  weight_decay: 0.0500 (0.0500)  time: 0.7255  data: 0.0005  max mem: 6186
Epoch: [25]  [28070/40201]  eta: 1:49:08  lr: 0.000002  min_lr: 0.000000  loss: 3.0940 (3.4137)  loss_scale: 32768.0000 (38699.1865)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.0005  max mem: 6186
Epoch: [25]  [28080/40201]  eta: 1:49:03  lr: 0.000002  min_lr: 0.000000  loss: 3.0291 (3.4136)  loss_scale: 32768.0000 (38697.0743)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.0006  max mem: 6186
Epoch: [25]  [28090/40201]  eta: 1:48:58  lr: 0.000002  min_lr: 0.000000  loss: 3.1290 (3.4136)  loss_scale: 32768.0000 (38694.9637)  weight_decay: 0.0500 (0.0500)  time: 0.6950  data: 0.0007  max mem: 6186
Epoch: [25]  [28100/40201]  eta: 1:48:54  lr: 0.000002  min_lr: 0.000000  loss: 3.4161 (3.4135)  loss_scale: 32768.0000 (38692.8545)  weight_decay: 0.0500 (0.0500)  time: 0.7382  data: 0.0008  max mem: 6186
Epoch: [25]  [28110/40201]  eta: 1:48:49  lr: 0.000002  min_lr: 0.000000  loss: 3.4648 (3.4135)  loss_scale: 32768.0000 (38690.7468)  weight_decay: 0.0500 (0.0500)  time: 0.7354  data: 0.0010  max mem: 6186
Epoch: [25]  [28120/40201]  eta: 1:48:45  lr: 0.000002  min_lr: 0.000000  loss: 3.4648 (3.4136)  loss_scale: 32768.0000 (38688.6407)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.0016  max mem: 6186
Epoch: [25]  [28130/40201]  eta: 1:48:40  lr: 0.000002  min_lr: 0.000000  loss: 3.3299 (3.4135)  loss_scale: 32768.0000 (38686.5360)  weight_decay: 0.0500 (0.0500)  time: 0.7044  data: 0.0013  max mem: 6186
Epoch: [25]  [28140/40201]  eta: 1:48:35  lr: 0.000002  min_lr: 0.000000  loss: 3.0223 (3.4133)  loss_scale: 32768.0000 (38684.4328)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.0006  max mem: 6186
Epoch: [25]  [28150/40201]  eta: 1:48:31  lr: 0.000002  min_lr: 0.000000  loss: 3.1555 (3.4135)  loss_scale: 32768.0000 (38682.3311)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.0008  max mem: 6186
Epoch: [25]  [28160/40201]  eta: 1:48:26  lr: 0.000002  min_lr: 0.000000  loss: 3.8019 (3.4135)  loss_scale: 32768.0000 (38680.2310)  weight_decay: 0.0500 (0.0500)  time: 0.7121  data: 0.0005  max mem: 6186
[2023-07-24 19:28:48,724] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:28:48,724] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:28:48,733] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:28:48,733] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [28170/40201]  eta: 1:48:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5123 (3.4135)  loss_scale: 32768.0000 (38682.7850)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0012  max mem: 6186
Epoch: [25]  [28180/40201]  eta: 1:48:16  lr: 0.000002  min_lr: 0.000000  loss: 3.1988 (3.4135)  loss_scale: 65536.0000 (38692.3138)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0014  max mem: 6186
Epoch: [25]  [28190/40201]  eta: 1:48:11  lr: 0.000002  min_lr: 0.000000  loss: 3.0524 (3.4133)  loss_scale: 65536.0000 (38701.8359)  weight_decay: 0.0500 (0.0500)  time: 0.6875  data: 0.0014  max mem: 6186
Epoch: [25]  [28200/40201]  eta: 1:48:07  lr: 0.000002  min_lr: 0.000000  loss: 3.0524 (3.4132)  loss_scale: 65536.0000 (38711.3512)  weight_decay: 0.0500 (0.0500)  time: 0.7151  data: 0.0013  max mem: 6186
Epoch: [25]  [28210/40201]  eta: 1:48:02  lr: 0.000002  min_lr: 0.000000  loss: 3.2476 (3.4132)  loss_scale: 65536.0000 (38720.8598)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.0013  max mem: 6186
Epoch: [25]  [28220/40201]  eta: 1:47:57  lr: 0.000002  min_lr: 0.000000  loss: 3.2476 (3.4131)  loss_scale: 65536.0000 (38730.3616)  weight_decay: 0.0500 (0.0500)  time: 0.7051  data: 0.0017  max mem: 6186
Epoch: [25]  [28230/40201]  eta: 1:47:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5368 (3.4134)  loss_scale: 65536.0000 (38739.8568)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0011  max mem: 6186
Epoch: [25]  [28240/40201]  eta: 1:47:47  lr: 0.000002  min_lr: 0.000000  loss: 4.3085 (3.4135)  loss_scale: 65536.0000 (38749.3451)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.0013  max mem: 6186
Epoch: [25]  [28250/40201]  eta: 1:47:42  lr: 0.000002  min_lr: 0.000000  loss: 3.8162 (3.4136)  loss_scale: 65536.0000 (38758.8268)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0030  max mem: 6186
Epoch: [25]  [28260/40201]  eta: 1:47:38  lr: 0.000002  min_lr: 0.000000  loss: 3.6806 (3.4136)  loss_scale: 65536.0000 (38768.3018)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.0027  max mem: 6186
Epoch: [25]  [28270/40201]  eta: 1:47:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6544 (3.4138)  loss_scale: 65536.0000 (38777.7700)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.0010  max mem: 6186
Epoch: [25]  [28280/40201]  eta: 1:47:28  lr: 0.000002  min_lr: 0.000000  loss: 3.5366 (3.4139)  loss_scale: 65536.0000 (38787.2316)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0008  max mem: 6186
Epoch: [25]  [28290/40201]  eta: 1:47:23  lr: 0.000002  min_lr: 0.000000  loss: 3.3005 (3.4137)  loss_scale: 65536.0000 (38796.6864)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.0006  max mem: 6186
Epoch: [25]  [28300/40201]  eta: 1:47:18  lr: 0.000002  min_lr: 0.000000  loss: 3.0843 (3.4136)  loss_scale: 65536.0000 (38806.1346)  weight_decay: 0.0500 (0.0500)  time: 0.6682  data: 0.0007  max mem: 6186
Epoch: [25]  [28310/40201]  eta: 1:47:13  lr: 0.000002  min_lr: 0.000000  loss: 3.2189 (3.4136)  loss_scale: 65536.0000 (38815.5761)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.0011  max mem: 6186
Epoch: [25]  [28320/40201]  eta: 1:47:08  lr: 0.000002  min_lr: 0.000000  loss: 3.2842 (3.4135)  loss_scale: 65536.0000 (38825.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0015  max mem: 6186
Epoch: [25]  [28330/40201]  eta: 1:47:04  lr: 0.000002  min_lr: 0.000000  loss: 3.0225 (3.4133)  loss_scale: 65536.0000 (38834.4392)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.0010  max mem: 6186
Epoch: [25]  [28340/40201]  eta: 1:46:58  lr: 0.000002  min_lr: 0.000000  loss: 3.0562 (3.4133)  loss_scale: 65536.0000 (38843.8607)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0004  max mem: 6186
Epoch: [25]  [28350/40201]  eta: 1:46:52  lr: 0.000002  min_lr: 0.000000  loss: 3.2544 (3.4133)  loss_scale: 65536.0000 (38853.2756)  weight_decay: 0.0500 (0.0500)  time: 0.4444  data: 0.0014  max mem: 6186
Epoch: [25]  [28360/40201]  eta: 1:46:46  lr: 0.000002  min_lr: 0.000000  loss: 3.4580 (3.4134)  loss_scale: 65536.0000 (38862.6838)  weight_decay: 0.0500 (0.0500)  time: 0.3506  data: 0.0022  max mem: 6186
Epoch: [25]  [28370/40201]  eta: 1:46:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4893 (3.4135)  loss_scale: 65536.0000 (38872.0854)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0018  max mem: 6186
Epoch: [25]  [28380/40201]  eta: 1:46:36  lr: 0.000002  min_lr: 0.000000  loss: 3.3530 (3.4134)  loss_scale: 65536.0000 (38881.4804)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0016  max mem: 6186
Epoch: [25]  [28390/40201]  eta: 1:46:30  lr: 0.000002  min_lr: 0.000000  loss: 3.3197 (3.4134)  loss_scale: 65536.0000 (38890.8688)  weight_decay: 0.0500 (0.0500)  time: 0.4118  data: 0.0023  max mem: 6186
Epoch: [25]  [28400/40201]  eta: 1:46:25  lr: 0.000002  min_lr: 0.000000  loss: 3.2639 (3.4133)  loss_scale: 65536.0000 (38900.2506)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0023  max mem: 6186
Epoch: [25]  [28410/40201]  eta: 1:46:21  lr: 0.000002  min_lr: 0.000000  loss: 2.7627 (3.4132)  loss_scale: 65536.0000 (38909.6257)  weight_decay: 0.0500 (0.0500)  time: 0.7190  data: 0.0009  max mem: 6186
Epoch: [25]  [28420/40201]  eta: 1:46:16  lr: 0.000002  min_lr: 0.000000  loss: 3.3628 (3.4133)  loss_scale: 65536.0000 (38918.9943)  weight_decay: 0.0500 (0.0500)  time: 0.7206  data: 0.0010  max mem: 6186
[2023-07-24 19:31:32,716] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:31:32,717] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 19:31:32,719] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:31:32,719] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-07-24 19:31:34,415] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14212
[2023-07-24 19:31:34,415] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-07-24 19:31:34,415] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-07-24 19:31:34,425] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14212
[2023-07-24 19:31:34,425] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [25]  [28430/40201]  eta: 1:46:11  lr: 0.000002  min_lr: 0.000000  loss: 3.6641 (3.4133)  loss_scale: 65536.0000 (38932.9664)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.0020  max mem: 6186
Epoch: [25]  [28440/40201]  eta: 1:46:06  lr: 0.000002  min_lr: 0.000000  loss: 3.4139 (3.4133)  loss_scale: 65536.0000 (38942.3202)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0016  max mem: 6186
Epoch: [25]  [28450/40201]  eta: 1:46:02  lr: 0.000002  min_lr: 0.000000  loss: 3.4139 (3.4133)  loss_scale: 65536.0000 (38951.6674)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.0535  max mem: 6186
Epoch: [25]  [28460/40201]  eta: 1:45:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6817 (3.4133)  loss_scale: 65536.0000 (38961.0080)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.0537  max mem: 6186
[2023-07-24 19:32:03,207] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14233
[2023-07-24 19:32:03,207] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:32:03,207] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14233
[2023-07-24 19:32:03,207] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:32:03,208] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [28470/40201]  eta: 1:45:52  lr: 0.000002  min_lr: 0.000000  loss: 3.1486 (3.4133)  loss_scale: 65536.0000 (38965.7383)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.0017  max mem: 6186
Epoch: [25]  [28480/40201]  eta: 1:45:47  lr: 0.000002  min_lr: 0.000000  loss: 2.8554 (3.4131)  loss_scale: 32768.0000 (38963.5622)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.0017  max mem: 6186
Epoch: [25]  [28490/40201]  eta: 1:45:42  lr: 0.000002  min_lr: 0.000000  loss: 2.8927 (3.4129)  loss_scale: 32768.0000 (38961.3877)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.0014  max mem: 6186
Epoch: [25]  [28500/40201]  eta: 1:45:38  lr: 0.000002  min_lr: 0.000000  loss: 3.0157 (3.4129)  loss_scale: 32768.0000 (38959.2146)  weight_decay: 0.0500 (0.0500)  time: 0.7113  data: 0.0009  max mem: 6186
Epoch: [25]  [28510/40201]  eta: 1:45:33  lr: 0.000002  min_lr: 0.000000  loss: 3.4029 (3.4131)  loss_scale: 32768.0000 (38957.0431)  weight_decay: 0.0500 (0.0500)  time: 0.7191  data: 0.0008  max mem: 6186
Epoch: [25]  [28520/40201]  eta: 1:45:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6687 (3.4131)  loss_scale: 32768.0000 (38954.8731)  weight_decay: 0.0500 (0.0500)  time: 0.7011  data: 0.0015  max mem: 6186
Epoch: [25]  [28530/40201]  eta: 1:45:23  lr: 0.000002  min_lr: 0.000000  loss: 3.1071 (3.4129)  loss_scale: 32768.0000 (38952.7046)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.0016  max mem: 6186
Epoch: [25]  [28540/40201]  eta: 1:45:18  lr: 0.000002  min_lr: 0.000000  loss: 3.1888 (3.4131)  loss_scale: 32768.0000 (38950.5377)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0014  max mem: 6186
Epoch: [25]  [28550/40201]  eta: 1:45:14  lr: 0.000002  min_lr: 0.000000  loss: 3.1888 (3.4131)  loss_scale: 32768.0000 (38948.3722)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.0012  max mem: 6186
Epoch: [25]  [28560/40201]  eta: 1:45:09  lr: 0.000002  min_lr: 0.000000  loss: 3.1964 (3.4131)  loss_scale: 32768.0000 (38946.2083)  weight_decay: 0.0500 (0.0500)  time: 0.7171  data: 0.0015  max mem: 6186
Epoch: [25]  [28570/40201]  eta: 1:45:04  lr: 0.000002  min_lr: 0.000000  loss: 3.3985 (3.4132)  loss_scale: 32768.0000 (38944.0459)  weight_decay: 0.0500 (0.0500)  time: 0.6904  data: 0.0015  max mem: 6186
Epoch: [25]  [28580/40201]  eta: 1:44:59  lr: 0.000002  min_lr: 0.000000  loss: 3.6471 (3.4132)  loss_scale: 32768.0000 (38941.8850)  weight_decay: 0.0500 (0.0500)  time: 0.7031  data: 0.0011  max mem: 6186
Epoch: [25]  [28590/40201]  eta: 1:44:55  lr: 0.000002  min_lr: 0.000000  loss: 3.3770 (3.4131)  loss_scale: 32768.0000 (38939.7256)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.0019  max mem: 6186
Epoch: [25]  [28600/40201]  eta: 1:44:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3670 (3.4132)  loss_scale: 32768.0000 (38937.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0016  max mem: 6186
Epoch: [25]  [28610/40201]  eta: 1:44:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4857 (3.4133)  loss_scale: 32768.0000 (38935.4114)  weight_decay: 0.0500 (0.0500)  time: 0.3522  data: 0.0022  max mem: 6186
Epoch: [25]  [28620/40201]  eta: 1:44:36  lr: 0.000002  min_lr: 0.000000  loss: 3.6072 (3.4134)  loss_scale: 32768.0000 (38933.2566)  weight_decay: 0.0500 (0.0500)  time: 0.3501  data: 0.0200  max mem: 6186
Epoch: [25]  [28630/40201]  eta: 1:44:31  lr: 0.000002  min_lr: 0.000000  loss: 3.6229 (3.4135)  loss_scale: 32768.0000 (38931.1032)  weight_decay: 0.0500 (0.0500)  time: 0.4616  data: 0.0240  max mem: 6186
Epoch: [25]  [28640/40201]  eta: 1:44:25  lr: 0.000002  min_lr: 0.000000  loss: 3.5237 (3.4134)  loss_scale: 32768.0000 (38928.9514)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0062  max mem: 6186
Epoch: [25]  [28650/40201]  eta: 1:44:19  lr: 0.000002  min_lr: 0.000000  loss: 3.1771 (3.4133)  loss_scale: 32768.0000 (38926.8010)  weight_decay: 0.0500 (0.0500)  time: 0.4642  data: 0.0010  max mem: 6186
Epoch: [25]  [28660/40201]  eta: 1:44:15  lr: 0.000002  min_lr: 0.000000  loss: 3.1771 (3.4134)  loss_scale: 32768.0000 (38924.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0013  max mem: 6186
Epoch: [25]  [28670/40201]  eta: 1:44:10  lr: 0.000002  min_lr: 0.000000  loss: 3.0754 (3.4132)  loss_scale: 32768.0000 (38922.5048)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.0014  max mem: 6186
Epoch: [25]  [28680/40201]  eta: 1:44:05  lr: 0.000002  min_lr: 0.000000  loss: 3.2370 (3.4133)  loss_scale: 32768.0000 (38920.3590)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.0010  max mem: 6186
Epoch: [25]  [28690/40201]  eta: 1:44:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5379 (3.4133)  loss_scale: 32768.0000 (38918.2146)  weight_decay: 0.0500 (0.0500)  time: 0.7229  data: 0.0008  max mem: 6186
Epoch: [25]  [28700/40201]  eta: 1:43:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7626 (3.4135)  loss_scale: 32768.0000 (38916.0718)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.0015  max mem: 6186
Epoch: [25]  [28710/40201]  eta: 1:43:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7892 (3.4136)  loss_scale: 32768.0000 (38913.9304)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.0012  max mem: 6186
Epoch: [25]  [28720/40201]  eta: 1:43:46  lr: 0.000002  min_lr: 0.000000  loss: 3.4211 (3.4135)  loss_scale: 32768.0000 (38911.7905)  weight_decay: 0.0500 (0.0500)  time: 0.7120  data: 0.0008  max mem: 6186
[2023-07-24 19:34:47,269] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:34:47,270] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:34:47,295] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:34:47,296] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [28730/40201]  eta: 1:43:41  lr: 0.000002  min_lr: 0.000000  loss: 3.0257 (3.4134)  loss_scale: 32768.0000 (38916.4952)  weight_decay: 0.0500 (0.0500)  time: 0.6868  data: 0.0015  max mem: 6186
[2023-07-24 19:34:52,811] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14366
[2023-07-24 19:34:52,811] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:34:52,824] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14366
[2023-07-24 19:34:52,824] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:34:52,824] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [28740/40201]  eta: 1:43:36  lr: 0.000002  min_lr: 0.000000  loss: 2.7634 (3.4133)  loss_scale: 32768.0000 (38916.6362)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0021  max mem: 6186
Epoch: [25]  [28750/40201]  eta: 1:43:31  lr: 0.000002  min_lr: 0.000000  loss: 3.1894 (3.4134)  loss_scale: 32768.0000 (38914.4976)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0021  max mem: 6186
Epoch: [25]  [28760/40201]  eta: 1:43:26  lr: 0.000002  min_lr: 0.000000  loss: 3.6109 (3.4134)  loss_scale: 32768.0000 (38912.3605)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.0018  max mem: 6186
Epoch: [25]  [28770/40201]  eta: 1:43:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5842 (3.4135)  loss_scale: 32768.0000 (38910.2249)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.0142  max mem: 6186
Epoch: [25]  [28780/40201]  eta: 1:43:16  lr: 0.000002  min_lr: 0.000000  loss: 3.5589 (3.4135)  loss_scale: 32768.0000 (38908.0908)  weight_decay: 0.0500 (0.0500)  time: 0.7090  data: 0.0135  max mem: 6186
Epoch: [25]  [28790/40201]  eta: 1:43:12  lr: 0.000002  min_lr: 0.000000  loss: 3.2013 (3.4135)  loss_scale: 32768.0000 (38905.9581)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.0007  max mem: 6186
Epoch: [25]  [28800/40201]  eta: 1:43:07  lr: 0.000002  min_lr: 0.000000  loss: 3.6823 (3.4135)  loss_scale: 32768.0000 (38903.8270)  weight_decay: 0.0500 (0.0500)  time: 0.7038  data: 0.0007  max mem: 6186
Epoch: [25]  [28810/40201]  eta: 1:43:02  lr: 0.000002  min_lr: 0.000000  loss: 3.0036 (3.4135)  loss_scale: 32768.0000 (38901.6973)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0007  max mem: 6186
Epoch: [25]  [28820/40201]  eta: 1:42:57  lr: 0.000002  min_lr: 0.000000  loss: 3.1174 (3.4134)  loss_scale: 32768.0000 (38899.5691)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.1882  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/efTAWmCkLKE_000418_000428.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [28830/40201]  eta: 1:42:51  lr: 0.000002  min_lr: 0.000000  loss: 3.3958 (3.4135)  loss_scale: 32768.0000 (38897.4423)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.2291  max mem: 6186
Epoch: [25]  [28840/40201]  eta: 1:42:45  lr: 0.000002  min_lr: 0.000000  loss: 3.1277 (3.4134)  loss_scale: 32768.0000 (38895.3171)  weight_decay: 0.0500 (0.0500)  time: 0.3815  data: 0.0545  max mem: 6186
Epoch: [25]  [28850/40201]  eta: 1:42:39  lr: 0.000002  min_lr: 0.000000  loss: 2.9252 (3.4134)  loss_scale: 32768.0000 (38893.1933)  weight_decay: 0.0500 (0.0500)  time: 0.3603  data: 0.0205  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/fNFXTBUF3nY_000230_000240.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [28860/40201]  eta: 1:42:33  lr: 0.000002  min_lr: 0.000000  loss: 3.3036 (3.4133)  loss_scale: 32768.0000 (38891.0710)  weight_decay: 0.0500 (0.0500)  time: 0.4578  data: 0.0949  max mem: 6186
Epoch: [25]  [28870/40201]  eta: 1:42:28  lr: 0.000002  min_lr: 0.000000  loss: 3.4275 (3.4133)  loss_scale: 32768.0000 (38888.9502)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0883  max mem: 6186
Epoch: [25]  [28880/40201]  eta: 1:42:24  lr: 0.000002  min_lr: 0.000000  loss: 3.2138 (3.4132)  loss_scale: 32768.0000 (38886.8308)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0023  max mem: 6186
Epoch: [25]  [28890/40201]  eta: 1:42:19  lr: 0.000002  min_lr: 0.000000  loss: 2.8946 (3.4131)  loss_scale: 32768.0000 (38884.7129)  weight_decay: 0.0500 (0.0500)  time: 0.7236  data: 0.0022  max mem: 6186
Epoch: [25]  [28900/40201]  eta: 1:42:14  lr: 0.000002  min_lr: 0.000000  loss: 2.8267 (3.4130)  loss_scale: 32768.0000 (38882.5964)  weight_decay: 0.0500 (0.0500)  time: 0.6916  data: 0.0013  max mem: 6186
Epoch: [25]  [28910/40201]  eta: 1:42:09  lr: 0.000002  min_lr: 0.000000  loss: 2.9373 (3.4131)  loss_scale: 32768.0000 (38880.4815)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.0012  max mem: 6186
Epoch: [25]  [28920/40201]  eta: 1:42:04  lr: 0.000002  min_lr: 0.000000  loss: 3.4384 (3.4132)  loss_scale: 32768.0000 (38878.3680)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.0012  max mem: 6186
Epoch: [25]  [28930/40201]  eta: 1:41:59  lr: 0.000002  min_lr: 0.000000  loss: 3.3610 (3.4132)  loss_scale: 32768.0000 (38876.2559)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.0008  max mem: 6186
Epoch: [25]  [28940/40201]  eta: 1:41:53  lr: 0.000002  min_lr: 0.000000  loss: 3.3610 (3.4133)  loss_scale: 32768.0000 (38874.1453)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0013  max mem: 6186
Epoch: [25]  [28950/40201]  eta: 1:41:47  lr: 0.000002  min_lr: 0.000000  loss: 3.0519 (3.4131)  loss_scale: 32768.0000 (38872.0362)  weight_decay: 0.0500 (0.0500)  time: 0.3864  data: 0.0027  max mem: 6186
Epoch: [25]  [28960/40201]  eta: 1:41:41  lr: 0.000002  min_lr: 0.000000  loss: 3.2234 (3.4132)  loss_scale: 32768.0000 (38869.9285)  weight_decay: 0.0500 (0.0500)  time: 0.3530  data: 0.0030  max mem: 6186
Epoch: [25]  [28970/40201]  eta: 1:41:35  lr: 0.000002  min_lr: 0.000000  loss: 3.2263 (3.4132)  loss_scale: 32768.0000 (38867.8223)  weight_decay: 0.0500 (0.0500)  time: 0.3447  data: 0.0150  max mem: 6186
Epoch: [25]  [28980/40201]  eta: 1:41:29  lr: 0.000002  min_lr: 0.000000  loss: 3.7552 (3.4135)  loss_scale: 32768.0000 (38865.7175)  weight_decay: 0.0500 (0.0500)  time: 0.4334  data: 0.0878  max mem: 6186
Epoch: [25]  [28990/40201]  eta: 1:41:23  lr: 0.000002  min_lr: 0.000000  loss: 3.8706 (3.4136)  loss_scale: 32768.0000 (38863.6142)  weight_decay: 0.0500 (0.0500)  time: 0.4390  data: 0.0751  max mem: 6186
[2023-07-24 19:37:20,369] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:37:20,369] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:37:20,369] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:37:20,370] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:37:24,157] [INFO] [timer.py:181:stop] 0/29000, SamplesPerSec=13.14869165939925
Epoch: [25]  [29000/40201]  eta: 1:41:17  lr: 0.000002  min_lr: 0.000000  loss: 3.7407 (3.4137)  loss_scale: 32768.0000 (38872.8113)  weight_decay: 0.0500 (0.0500)  time: 0.3871  data: 0.0378  max mem: 6186
Epoch: [25]  [29010/40201]  eta: 1:41:12  lr: 0.000002  min_lr: 0.000000  loss: 3.3211 (3.4137)  loss_scale: 65536.0000 (38882.0020)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0437  max mem: 6186
Epoch: [25]  [29020/40201]  eta: 1:41:08  lr: 0.000002  min_lr: 0.000000  loss: 3.0801 (3.4136)  loss_scale: 65536.0000 (38891.1864)  weight_decay: 0.0500 (0.0500)  time: 0.7078  data: 0.0072  max mem: 6186
Epoch: [25]  [29030/40201]  eta: 1:41:03  lr: 0.000002  min_lr: 0.000000  loss: 3.4227 (3.4136)  loss_scale: 65536.0000 (38900.3644)  weight_decay: 0.0500 (0.0500)  time: 0.7359  data: 0.0010  max mem: 6186
Epoch: [25]  [29040/40201]  eta: 1:40:58  lr: 0.000002  min_lr: 0.000000  loss: 3.7968 (3.4137)  loss_scale: 65536.0000 (38909.5362)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.0007  max mem: 6186
Epoch: [25]  [29050/40201]  eta: 1:40:53  lr: 0.000002  min_lr: 0.000000  loss: 3.4160 (3.4137)  loss_scale: 65536.0000 (38918.7016)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.0010  max mem: 6186
Epoch: [25]  [29060/40201]  eta: 1:40:49  lr: 0.000002  min_lr: 0.000000  loss: 3.3261 (3.4139)  loss_scale: 65536.0000 (38927.8607)  weight_decay: 0.0500 (0.0500)  time: 0.6987  data: 0.0012  max mem: 6186
Epoch: [25]  [29070/40201]  eta: 1:40:44  lr: 0.000002  min_lr: 0.000000  loss: 3.6038 (3.4139)  loss_scale: 65536.0000 (38937.0135)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.0011  max mem: 6186
Epoch: [25]  [29080/40201]  eta: 1:40:39  lr: 0.000002  min_lr: 0.000000  loss: 3.5834 (3.4139)  loss_scale: 65536.0000 (38946.1600)  weight_decay: 0.0500 (0.0500)  time: 0.6798  data: 0.0009  max mem: 6186
Epoch: [25]  [29090/40201]  eta: 1:40:34  lr: 0.000002  min_lr: 0.000000  loss: 3.5332 (3.4138)  loss_scale: 65536.0000 (38955.3003)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.0006  max mem: 6186
Epoch: [25]  [29100/40201]  eta: 1:40:29  lr: 0.000002  min_lr: 0.000000  loss: 3.5641 (3.4140)  loss_scale: 65536.0000 (38964.4342)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0013  max mem: 6186
Epoch: [25]  [29110/40201]  eta: 1:40:24  lr: 0.000002  min_lr: 0.000000  loss: 3.4687 (3.4140)  loss_scale: 65536.0000 (38973.5619)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.0014  max mem: 6186
[2023-07-24 19:38:46,067] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14558
[2023-07-24 19:38:46,067] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:38:46,067] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:38:46,076] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14558
[2023-07-24 19:38:46,077] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [29120/40201]  eta: 1:40:19  lr: 0.000002  min_lr: 0.000000  loss: 3.3496 (3.4138)  loss_scale: 65536.0000 (38978.1823)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.0007  max mem: 6186
Epoch: [25]  [29130/40201]  eta: 1:40:15  lr: 0.000002  min_lr: 0.000000  loss: 3.0121 (3.4138)  loss_scale: 32768.0000 (38976.0505)  weight_decay: 0.0500 (0.0500)  time: 0.7247  data: 0.0013  max mem: 6186
Epoch: [25]  [29140/40201]  eta: 1:40:10  lr: 0.000002  min_lr: 0.000000  loss: 3.1535 (3.4138)  loss_scale: 32768.0000 (38973.9202)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0021  max mem: 6186
Epoch: [25]  [29150/40201]  eta: 1:40:03  lr: 0.000002  min_lr: 0.000000  loss: 3.4465 (3.4138)  loss_scale: 32768.0000 (38971.7913)  weight_decay: 0.0500 (0.0500)  time: 0.4693  data: 0.0020  max mem: 6186
Epoch: [25]  [29160/40201]  eta: 1:39:57  lr: 0.000002  min_lr: 0.000000  loss: 3.4690 (3.4140)  loss_scale: 32768.0000 (38969.6639)  weight_decay: 0.0500 (0.0500)  time: 0.3176  data: 0.0012  max mem: 6186
Epoch: [25]  [29170/40201]  eta: 1:39:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7082 (3.4140)  loss_scale: 32768.0000 (38967.5379)  weight_decay: 0.0500 (0.0500)  time: 0.3043  data: 0.0013  max mem: 6186
Epoch: [25]  [29180/40201]  eta: 1:39:45  lr: 0.000002  min_lr: 0.000000  loss: 2.9683 (3.4139)  loss_scale: 32768.0000 (38965.4134)  weight_decay: 0.0500 (0.0500)  time: 0.3644  data: 0.0025  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/nfjWfoyGApo_000220_000230.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [29190/40201]  eta: 1:39:39  lr: 0.000002  min_lr: 0.000000  loss: 3.2748 (3.4139)  loss_scale: 32768.0000 (38963.2903)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0022  max mem: 6186
Epoch: [25]  [29200/40201]  eta: 1:39:34  lr: 0.000002  min_lr: 0.000000  loss: 3.2005 (3.4138)  loss_scale: 32768.0000 (38961.1687)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0273  max mem: 6186
Epoch: [25]  [29210/40201]  eta: 1:39:29  lr: 0.000002  min_lr: 0.000000  loss: 2.9476 (3.4137)  loss_scale: 32768.0000 (38959.0486)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0495  max mem: 6186
Epoch: [25]  [29220/40201]  eta: 1:39:24  lr: 0.000002  min_lr: 0.000000  loss: 3.0669 (3.4136)  loss_scale: 32768.0000 (38956.9299)  weight_decay: 0.0500 (0.0500)  time: 0.6758  data: 0.0233  max mem: 6186
Epoch: [25]  [29230/40201]  eta: 1:39:19  lr: 0.000002  min_lr: 0.000000  loss: 3.2575 (3.4135)  loss_scale: 32768.0000 (38954.8126)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.0021  max mem: 6186
Epoch: [25]  [29240/40201]  eta: 1:39:14  lr: 0.000002  min_lr: 0.000000  loss: 3.4994 (3.4137)  loss_scale: 32768.0000 (38952.6968)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.0033  max mem: 6186
Epoch: [25]  [29250/40201]  eta: 1:39:09  lr: 0.000002  min_lr: 0.000000  loss: 3.6142 (3.4136)  loss_scale: 32768.0000 (38950.5825)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0019  max mem: 6186
Epoch: [25]  [29260/40201]  eta: 1:39:04  lr: 0.000002  min_lr: 0.000000  loss: 3.4485 (3.4136)  loss_scale: 32768.0000 (38948.4696)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.0012  max mem: 6186
Epoch: [25]  [29270/40201]  eta: 1:39:00  lr: 0.000002  min_lr: 0.000000  loss: 3.4485 (3.4136)  loss_scale: 32768.0000 (38946.3581)  weight_decay: 0.0500 (0.0500)  time: 0.7308  data: 0.0019  max mem: 6186
Epoch: [25]  [29280/40201]  eta: 1:38:55  lr: 0.000002  min_lr: 0.000000  loss: 3.1184 (3.4135)  loss_scale: 32768.0000 (38944.2481)  weight_decay: 0.0500 (0.0500)  time: 0.7359  data: 0.0018  max mem: 6186
Epoch: [25]  [29290/40201]  eta: 1:38:50  lr: 0.000002  min_lr: 0.000000  loss: 3.4275 (3.4136)  loss_scale: 32768.0000 (38942.1395)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.0015  max mem: 6186
Epoch: [25]  [29300/40201]  eta: 1:38:45  lr: 0.000002  min_lr: 0.000000  loss: 3.2805 (3.4135)  loss_scale: 32768.0000 (38940.0324)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.0010  max mem: 6186
Epoch: [25]  [29310/40201]  eta: 1:38:40  lr: 0.000002  min_lr: 0.000000  loss: 3.2544 (3.4136)  loss_scale: 32768.0000 (38937.9266)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.0017  max mem: 6186
Epoch: [25]  [29320/40201]  eta: 1:38:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4844 (3.4138)  loss_scale: 32768.0000 (38935.8224)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.0016  max mem: 6186
Epoch: [25]  [29330/40201]  eta: 1:38:30  lr: 0.000002  min_lr: 0.000000  loss: 3.4844 (3.4140)  loss_scale: 32768.0000 (38933.7195)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.0007  max mem: 6186
Epoch: [25]  [29340/40201]  eta: 1:38:26  lr: 0.000002  min_lr: 0.000000  loss: 3.3428 (3.4140)  loss_scale: 32768.0000 (38931.6181)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.0008  max mem: 6186
Epoch: [25]  [29350/40201]  eta: 1:38:21  lr: 0.000002  min_lr: 0.000000  loss: 3.1918 (3.4140)  loss_scale: 32768.0000 (38929.5182)  weight_decay: 0.0500 (0.0500)  time: 0.7112  data: 0.0010  max mem: 6186
Epoch: [25]  [29360/40201]  eta: 1:38:16  lr: 0.000002  min_lr: 0.000000  loss: 3.1107 (3.4138)  loss_scale: 32768.0000 (38927.4196)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.0017  max mem: 6186
Epoch: [25]  [29370/40201]  eta: 1:38:11  lr: 0.000002  min_lr: 0.000000  loss: 3.1136 (3.4137)  loss_scale: 32768.0000 (38925.3225)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.0016  max mem: 6186
[2023-07-24 19:41:25,740] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:41:25,740] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:41:25,742] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:41:25,742] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [29380/40201]  eta: 1:38:05  lr: 0.000002  min_lr: 0.000000  loss: 3.2510 (3.4136)  loss_scale: 32768.0000 (38929.9185)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0007  max mem: 6186
Epoch: [25]  [29390/40201]  eta: 1:37:59  lr: 0.000002  min_lr: 0.000000  loss: 2.6358 (3.4134)  loss_scale: 65536.0000 (38938.9710)  weight_decay: 0.0500 (0.0500)  time: 0.3982  data: 0.0011  max mem: 6186
Epoch: [25]  [29400/40201]  eta: 1:37:53  lr: 0.000002  min_lr: 0.000000  loss: 3.1416 (3.4135)  loss_scale: 65536.0000 (38948.0173)  weight_decay: 0.0500 (0.0500)  time: 0.3961  data: 0.0016  max mem: 6186
Epoch: [25]  [29410/40201]  eta: 1:37:47  lr: 0.000002  min_lr: 0.000000  loss: 3.5535 (3.4134)  loss_scale: 65536.0000 (38957.0574)  weight_decay: 0.0500 (0.0500)  time: 0.4071  data: 0.0602  max mem: 6186
Epoch: [25]  [29420/40201]  eta: 1:37:41  lr: 0.000002  min_lr: 0.000000  loss: 3.1842 (3.4134)  loss_scale: 65536.0000 (38966.0914)  weight_decay: 0.0500 (0.0500)  time: 0.4370  data: 0.1127  max mem: 6186
Epoch: [25]  [29430/40201]  eta: 1:37:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4697 (3.4134)  loss_scale: 65536.0000 (38975.1193)  weight_decay: 0.0500 (0.0500)  time: 0.3916  data: 0.0558  max mem: 6186
Epoch: [25]  [29440/40201]  eta: 1:37:30  lr: 0.000002  min_lr: 0.000000  loss: 3.2258 (3.4134)  loss_scale: 65536.0000 (38984.1410)  weight_decay: 0.0500 (0.0500)  time: 0.4600  data: 0.0447  max mem: 6186
Epoch: [25]  [29450/40201]  eta: 1:37:25  lr: 0.000002  min_lr: 0.000000  loss: 3.2258 (3.4134)  loss_scale: 65536.0000 (38993.1566)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0434  max mem: 6186
Epoch: [25]  [29460/40201]  eta: 1:37:20  lr: 0.000002  min_lr: 0.000000  loss: 3.2475 (3.4134)  loss_scale: 65536.0000 (39002.1661)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0022  max mem: 6186
Epoch: [25]  [29470/40201]  eta: 1:37:15  lr: 0.000002  min_lr: 0.000000  loss: 3.6474 (3.4135)  loss_scale: 65536.0000 (39011.1695)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0015  max mem: 6186
Epoch: [25]  [29480/40201]  eta: 1:37:09  lr: 0.000002  min_lr: 0.000000  loss: 3.6712 (3.4135)  loss_scale: 65536.0000 (39020.1668)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0016  max mem: 6186
Epoch: [25]  [29490/40201]  eta: 1:37:04  lr: 0.000002  min_lr: 0.000000  loss: 3.2156 (3.4134)  loss_scale: 65536.0000 (39029.1579)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0016  max mem: 6186
Epoch: [25]  [29500/40201]  eta: 1:36:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4491 (3.4136)  loss_scale: 65536.0000 (39038.1430)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0009  max mem: 6186
Epoch: [25]  [29510/40201]  eta: 1:36:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5835 (3.4136)  loss_scale: 65536.0000 (39047.1220)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0008  max mem: 6186
Epoch: [25]  [29520/40201]  eta: 1:36:48  lr: 0.000002  min_lr: 0.000000  loss: 3.4005 (3.4135)  loss_scale: 65536.0000 (39056.0948)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0010  max mem: 6186
Epoch: [25]  [29530/40201]  eta: 1:36:43  lr: 0.000002  min_lr: 0.000000  loss: 3.2714 (3.4135)  loss_scale: 65536.0000 (39065.0617)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0013  max mem: 6186
Epoch: [25]  [29540/40201]  eta: 1:36:38  lr: 0.000002  min_lr: 0.000000  loss: 3.5799 (3.4137)  loss_scale: 65536.0000 (39074.0224)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0011  max mem: 6186
Epoch: [25]  [29550/40201]  eta: 1:36:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6028 (3.4137)  loss_scale: 65536.0000 (39082.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0010  max mem: 6186
Epoch: [25]  [29560/40201]  eta: 1:36:27  lr: 0.000002  min_lr: 0.000000  loss: 3.2607 (3.4137)  loss_scale: 65536.0000 (39091.9257)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0009  max mem: 6186
Epoch: [25]  [29570/40201]  eta: 1:36:22  lr: 0.000002  min_lr: 0.000000  loss: 3.4162 (3.4138)  loss_scale: 65536.0000 (39100.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0005  max mem: 6186
Epoch: [25]  [29580/40201]  eta: 1:36:17  lr: 0.000002  min_lr: 0.000000  loss: 3.8107 (3.4139)  loss_scale: 65536.0000 (39109.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0005  max mem: 6186
Epoch: [25]  [29590/40201]  eta: 1:36:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7519 (3.4139)  loss_scale: 65536.0000 (39118.7353)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0010  max mem: 6186
Epoch: [25]  [29600/40201]  eta: 1:36:06  lr: 0.000002  min_lr: 0.000000  loss: 3.0271 (3.4138)  loss_scale: 65536.0000 (39127.6597)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0010  max mem: 6186
[2023-07-24 19:43:33,648] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14802
[2023-07-24 19:43:33,648] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:43:33,649] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14802
[2023-07-24 19:43:33,649] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:43:33,650] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [29610/40201]  eta: 1:36:01  lr: 0.000002  min_lr: 0.000000  loss: 2.8634 (3.4138)  loss_scale: 65536.0000 (39129.9385)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0007  max mem: 6186
Epoch: [25]  [29620/40201]  eta: 1:35:56  lr: 0.000002  min_lr: 0.000000  loss: 3.1318 (3.4136)  loss_scale: 32768.0000 (39127.7907)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0012  max mem: 6186
Epoch: [25]  [29630/40201]  eta: 1:35:51  lr: 0.000002  min_lr: 0.000000  loss: 3.2502 (3.4136)  loss_scale: 32768.0000 (39125.6444)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0016  max mem: 6186
Epoch: [25]  [29640/40201]  eta: 1:35:46  lr: 0.000002  min_lr: 0.000000  loss: 3.2686 (3.4137)  loss_scale: 32768.0000 (39123.4995)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.0022  max mem: 6186
Epoch: [25]  [29650/40201]  eta: 1:35:42  lr: 0.000002  min_lr: 0.000000  loss: 3.2686 (3.4136)  loss_scale: 32768.0000 (39121.3560)  weight_decay: 0.0500 (0.0500)  time: 0.8305  data: 0.1465  max mem: 6186
Epoch: [25]  [29660/40201]  eta: 1:35:38  lr: 0.000002  min_lr: 0.000000  loss: 3.2508 (3.4136)  loss_scale: 32768.0000 (39119.2141)  weight_decay: 0.0500 (0.0500)  time: 0.8864  data: 0.1454  max mem: 6186
Epoch: [25]  [29670/40201]  eta: 1:35:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6395 (3.4137)  loss_scale: 32768.0000 (39117.0735)  weight_decay: 0.0500 (0.0500)  time: 0.7616  data: 0.0005  max mem: 6186
Epoch: [25]  [29680/40201]  eta: 1:35:27  lr: 0.000002  min_lr: 0.000000  loss: 3.6990 (3.4138)  loss_scale: 32768.0000 (39114.9344)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.0005  max mem: 6186
Epoch: [25]  [29690/40201]  eta: 1:35:21  lr: 0.000002  min_lr: 0.000000  loss: 3.7445 (3.4139)  loss_scale: 32768.0000 (39112.7967)  weight_decay: 0.0500 (0.0500)  time: 0.4478  data: 0.0004  max mem: 6186
Epoch: [25]  [29700/40201]  eta: 1:35:16  lr: 0.000002  min_lr: 0.000000  loss: 3.6500 (3.4139)  loss_scale: 32768.0000 (39110.6605)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.2022  max mem: 6186
Epoch: [25]  [29710/40201]  eta: 1:35:10  lr: 0.000002  min_lr: 0.000000  loss: 3.3630 (3.4140)  loss_scale: 32768.0000 (39108.5257)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.2072  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/GajaQD6qRkw_000057_000067.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/GajaQD6qRkw_000057_000067.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [29720/40201]  eta: 1:35:05  lr: 0.000002  min_lr: 0.000000  loss: 3.3630 (3.4140)  loss_scale: 32768.0000 (39106.3924)  weight_decay: 0.0500 (0.0500)  time: 0.4041  data: 0.0334  max mem: 6186
Epoch: [25]  [29730/40201]  eta: 1:34:59  lr: 0.000002  min_lr: 0.000000  loss: 3.1550 (3.4140)  loss_scale: 32768.0000 (39104.2605)  weight_decay: 0.0500 (0.0500)  time: 0.4415  data: 0.1184  max mem: 6186
Epoch: [25]  [29740/40201]  eta: 1:34:54  lr: 0.000002  min_lr: 0.000000  loss: 3.3555 (3.4138)  loss_scale: 32768.0000 (39102.1300)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1446  max mem: 6186
Epoch: [25]  [29750/40201]  eta: 1:34:49  lr: 0.000002  min_lr: 0.000000  loss: 3.2310 (3.4137)  loss_scale: 32768.0000 (39100.0009)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.0551  max mem: 6186
Epoch: [25]  [29760/40201]  eta: 1:34:44  lr: 0.000002  min_lr: 0.000000  loss: 3.0905 (3.4136)  loss_scale: 32768.0000 (39097.8733)  weight_decay: 0.0500 (0.0500)  time: 0.7207  data: 0.0016  max mem: 6186
Epoch: [25]  [29770/40201]  eta: 1:34:39  lr: 0.000002  min_lr: 0.000000  loss: 3.0905 (3.4137)  loss_scale: 32768.0000 (39095.7471)  weight_decay: 0.0500 (0.0500)  time: 0.7199  data: 0.0017  max mem: 6186
Epoch: [25]  [29780/40201]  eta: 1:34:35  lr: 0.000002  min_lr: 0.000000  loss: 3.3762 (3.4137)  loss_scale: 32768.0000 (39093.6224)  weight_decay: 0.0500 (0.0500)  time: 0.6952  data: 0.0010  max mem: 6186
Epoch: [25]  [29790/40201]  eta: 1:34:30  lr: 0.000002  min_lr: 0.000000  loss: 3.4262 (3.4137)  loss_scale: 32768.0000 (39091.4990)  weight_decay: 0.0500 (0.0500)  time: 0.7274  data: 0.0012  max mem: 6186
Epoch: [25]  [29800/40201]  eta: 1:34:25  lr: 0.000002  min_lr: 0.000000  loss: 3.4922 (3.4138)  loss_scale: 32768.0000 (39089.3771)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.0016  max mem: 6186
Epoch: [25]  [29810/40201]  eta: 1:34:20  lr: 0.000002  min_lr: 0.000000  loss: 3.8754 (3.4140)  loss_scale: 32768.0000 (39087.2567)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.0012  max mem: 6186
Epoch: [25]  [29820/40201]  eta: 1:34:16  lr: 0.000002  min_lr: 0.000000  loss: 3.6772 (3.4140)  loss_scale: 32768.0000 (39085.1376)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.0020  max mem: 6186
Epoch: [25]  [29830/40201]  eta: 1:34:11  lr: 0.000002  min_lr: 0.000000  loss: 3.1654 (3.4138)  loss_scale: 32768.0000 (39083.0199)  weight_decay: 0.0500 (0.0500)  time: 0.7568  data: 0.0031  max mem: 6186
Epoch: [25]  [29840/40201]  eta: 1:34:06  lr: 0.000002  min_lr: 0.000000  loss: 2.9969 (3.4138)  loss_scale: 32768.0000 (39080.9037)  weight_decay: 0.0500 (0.0500)  time: 0.7291  data: 0.0021  max mem: 6186
Epoch: [25]  [29850/40201]  eta: 1:34:01  lr: 0.000002  min_lr: 0.000000  loss: 3.0356 (3.4138)  loss_scale: 32768.0000 (39078.7889)  weight_decay: 0.0500 (0.0500)  time: 0.7164  data: 0.0008  max mem: 6186
Epoch: [25]  [29860/40201]  eta: 1:33:56  lr: 0.000002  min_lr: 0.000000  loss: 3.2723 (3.4138)  loss_scale: 32768.0000 (39076.6755)  weight_decay: 0.0500 (0.0500)  time: 0.7269  data: 0.0005  max mem: 6186
[2023-07-24 19:46:27,285] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:46:27,285] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:46:27,291] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:46:27,292] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [29870/40201]  eta: 1:33:51  lr: 0.000002  min_lr: 0.000000  loss: 3.1418 (3.4136)  loss_scale: 32768.0000 (39083.3394)  weight_decay: 0.0500 (0.0500)  time: 0.7409  data: 0.0003  max mem: 6186
[2023-07-24 19:46:35,370] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14937
[2023-07-24 19:46:35,370] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:46:35,370] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:46:35,383] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 14937
[2023-07-24 19:46:35,383] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [29880/40201]  eta: 1:33:46  lr: 0.000002  min_lr: 0.000000  loss: 2.8210 (3.4135)  loss_scale: 65536.0000 (39085.6124)  weight_decay: 0.0500 (0.0500)  time: 0.6869  data: 0.0005  max mem: 6186
Epoch: [25]  [29890/40201]  eta: 1:33:41  lr: 0.000002  min_lr: 0.000000  loss: 3.2879 (3.4136)  loss_scale: 32768.0000 (39083.4988)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.0004  max mem: 6186
Epoch: [25]  [29900/40201]  eta: 1:33:37  lr: 0.000002  min_lr: 0.000000  loss: 3.5344 (3.4136)  loss_scale: 32768.0000 (39081.3867)  weight_decay: 0.0500 (0.0500)  time: 0.7329  data: 0.0017  max mem: 6186
Epoch: [25]  [29910/40201]  eta: 1:33:32  lr: 0.000002  min_lr: 0.000000  loss: 3.5752 (3.4137)  loss_scale: 32768.0000 (39079.2760)  weight_decay: 0.0500 (0.0500)  time: 0.7530  data: 0.0021  max mem: 6186
Epoch: [25]  [29920/40201]  eta: 1:33:27  lr: 0.000002  min_lr: 0.000000  loss: 3.5718 (3.4137)  loss_scale: 32768.0000 (39077.1667)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.0011  max mem: 6186
Epoch: [25]  [29930/40201]  eta: 1:33:22  lr: 0.000002  min_lr: 0.000000  loss: 2.9787 (3.4136)  loss_scale: 32768.0000 (39075.0588)  weight_decay: 0.0500 (0.0500)  time: 0.7268  data: 0.0007  max mem: 6186
Epoch: [25]  [29940/40201]  eta: 1:33:17  lr: 0.000002  min_lr: 0.000000  loss: 3.3130 (3.4137)  loss_scale: 32768.0000 (39072.9523)  weight_decay: 0.0500 (0.0500)  time: 0.7597  data: 0.0008  max mem: 6186
Epoch: [25]  [29950/40201]  eta: 1:33:13  lr: 0.000002  min_lr: 0.000000  loss: 3.6903 (3.4138)  loss_scale: 32768.0000 (39070.8472)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.0010  max mem: 6186
Epoch: [25]  [29960/40201]  eta: 1:33:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7272 (3.4138)  loss_scale: 32768.0000 (39068.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0006  max mem: 6186
Epoch: [25]  [29970/40201]  eta: 1:33:01  lr: 0.000002  min_lr: 0.000000  loss: 3.5216 (3.4139)  loss_scale: 32768.0000 (39066.6412)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0011  max mem: 6186
Epoch: [25]  [29980/40201]  eta: 1:32:56  lr: 0.000002  min_lr: 0.000000  loss: 3.3110 (3.4139)  loss_scale: 32768.0000 (39064.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.2101  max mem: 6186
Epoch: [25]  [29990/40201]  eta: 1:32:50  lr: 0.000002  min_lr: 0.000000  loss: 3.3110 (3.4139)  loss_scale: 32768.0000 (39062.4409)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.2898  max mem: 6186
[2023-07-24 19:47:53,580] [INFO] [logging.py:69:log_dist] [Rank 0] step=15000, skipped=84, lr=[5.539581345132665e-08, 5.539581345132665e-08, 7.386108460176887e-08, 7.386108460176887e-08, 9.848144613569183e-08, 9.848144613569183e-08, 1.313085948475891e-07, 1.313085948475891e-07, 1.7507812646345213e-07, 1.7507812646345213e-07, 2.334375019512695e-07, 2.334375019512695e-07, 3.1125000260169265e-07, 3.1125000260169265e-07, 4.150000034689236e-07, 4.150000034689236e-07, 5.533333379585647e-07, 5.533333379585647e-07, 7.377777839447531e-07, 7.377777839447531e-07, 9.837037119263373e-07, 9.837037119263373e-07, 1.3116049492351164e-06, 1.3116049492351164e-06, 1.7488065989801553e-06, 1.7488065989801553e-06, 2.3317421319735404e-06, 2.3317421319735404e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-24 19:47:53,580] [INFO] [timer.py:181:stop] 0/30000, SamplesPerSec=13.107140160778112
Epoch: [25]  [30000/40201]  eta: 1:32:44  lr: 0.000002  min_lr: 0.000000  loss: 3.3775 (3.4140)  loss_scale: 32768.0000 (39060.3428)  weight_decay: 0.0500 (0.0500)  time: 0.4537  data: 0.0809  max mem: 6186
Epoch: [25]  [30010/40201]  eta: 1:32:39  lr: 0.000002  min_lr: 0.000000  loss: 3.3350 (3.4141)  loss_scale: 32768.0000 (39058.2461)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0014  max mem: 6186
Epoch: [25]  [30020/40201]  eta: 1:32:34  lr: 0.000002  min_lr: 0.000000  loss: 3.3350 (3.4141)  loss_scale: 32768.0000 (39056.1508)  weight_decay: 0.0500 (0.0500)  time: 0.6945  data: 0.0017  max mem: 6186
Epoch: [25]  [30030/40201]  eta: 1:32:29  lr: 0.000002  min_lr: 0.000000  loss: 3.5547 (3.4141)  loss_scale: 32768.0000 (39054.0569)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.0013  max mem: 6186
Epoch: [25]  [30040/40201]  eta: 1:32:25  lr: 0.000002  min_lr: 0.000000  loss: 2.8221 (3.4139)  loss_scale: 32768.0000 (39051.9644)  weight_decay: 0.0500 (0.0500)  time: 0.7440  data: 0.0010  max mem: 6186
Epoch: [25]  [30050/40201]  eta: 1:32:20  lr: 0.000002  min_lr: 0.000000  loss: 2.9482 (3.4138)  loss_scale: 32768.0000 (39049.8733)  weight_decay: 0.0500 (0.0500)  time: 0.7599  data: 0.0013  max mem: 6186
Epoch: [25]  [30060/40201]  eta: 1:32:15  lr: 0.000002  min_lr: 0.000000  loss: 3.2341 (3.4139)  loss_scale: 32768.0000 (39047.7836)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.0011  max mem: 6186
Epoch: [25]  [30070/40201]  eta: 1:32:11  lr: 0.000002  min_lr: 0.000000  loss: 3.2341 (3.4138)  loss_scale: 32768.0000 (39045.6953)  weight_decay: 0.0500 (0.0500)  time: 0.7614  data: 0.0008  max mem: 6186
Epoch: [25]  [30080/40201]  eta: 1:32:06  lr: 0.000002  min_lr: 0.000000  loss: 3.1454 (3.4137)  loss_scale: 32768.0000 (39043.6084)  weight_decay: 0.0500 (0.0500)  time: 0.8128  data: 0.0014  max mem: 6186
Epoch: [25]  [30090/40201]  eta: 1:32:01  lr: 0.000002  min_lr: 0.000000  loss: 2.8802 (3.4137)  loss_scale: 32768.0000 (39041.5228)  weight_decay: 0.0500 (0.0500)  time: 0.7985  data: 0.0016  max mem: 6186
Epoch: [25]  [30100/40201]  eta: 1:31:56  lr: 0.000002  min_lr: 0.000000  loss: 3.1208 (3.4137)  loss_scale: 32768.0000 (39039.4387)  weight_decay: 0.0500 (0.0500)  time: 0.7306  data: 0.0011  max mem: 6186
Epoch: [25]  [30110/40201]  eta: 1:31:52  lr: 0.000002  min_lr: 0.000000  loss: 3.1208 (3.4136)  loss_scale: 32768.0000 (39037.3559)  weight_decay: 0.0500 (0.0500)  time: 0.7204  data: 0.0005  max mem: 6186
Epoch: [25]  [30120/40201]  eta: 1:31:47  lr: 0.000002  min_lr: 0.000000  loss: 3.2328 (3.4136)  loss_scale: 32768.0000 (39035.2745)  weight_decay: 0.0500 (0.0500)  time: 0.7601  data: 0.0004  max mem: 6186
Epoch: [25]  [30130/40201]  eta: 1:31:42  lr: 0.000002  min_lr: 0.000000  loss: 3.2775 (3.4137)  loss_scale: 32768.0000 (39033.1945)  weight_decay: 0.0500 (0.0500)  time: 0.7392  data: 0.0011  max mem: 6186
[2023-07-24 19:49:32,865] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:49:32,865] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:49:32,894] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:49:32,894] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [30140/40201]  eta: 1:31:37  lr: 0.000002  min_lr: 0.000000  loss: 3.2775 (3.4136)  loss_scale: 32768.0000 (39039.8131)  weight_decay: 0.0500 (0.0500)  time: 0.6805  data: 0.0015  max mem: 6186
Epoch: [25]  [30150/40201]  eta: 1:31:32  lr: 0.000002  min_lr: 0.000000  loss: 3.3043 (3.4136)  loss_scale: 65536.0000 (39048.6010)  weight_decay: 0.0500 (0.0500)  time: 0.6847  data: 0.0007  max mem: 6186
Epoch: [25]  [30160/40201]  eta: 1:31:27  lr: 0.000002  min_lr: 0.000000  loss: 3.3043 (3.4135)  loss_scale: 65536.0000 (39057.3830)  weight_decay: 0.0500 (0.0500)  time: 0.7453  data: 0.0007  max mem: 6186
Epoch: [25]  [30170/40201]  eta: 1:31:23  lr: 0.000002  min_lr: 0.000000  loss: 3.4952 (3.4135)  loss_scale: 65536.0000 (39066.1592)  weight_decay: 0.0500 (0.0500)  time: 0.7973  data: 0.0007  max mem: 6186
Epoch: [25]  [30180/40201]  eta: 1:31:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5711 (3.4135)  loss_scale: 65536.0000 (39074.9295)  weight_decay: 0.0500 (0.0500)  time: 0.8009  data: 0.0003  max mem: 6186
Epoch: [25]  [30190/40201]  eta: 1:31:13  lr: 0.000002  min_lr: 0.000000  loss: 3.5711 (3.4135)  loss_scale: 65536.0000 (39083.6941)  weight_decay: 0.0500 (0.0500)  time: 0.7394  data: 0.0004  max mem: 6186
Epoch: [25]  [30200/40201]  eta: 1:31:08  lr: 0.000002  min_lr: 0.000000  loss: 3.2717 (3.4135)  loss_scale: 65536.0000 (39092.4528)  weight_decay: 0.0500 (0.0500)  time: 0.6990  data: 0.0016  max mem: 6186
Epoch: [25]  [30210/40201]  eta: 1:31:03  lr: 0.000002  min_lr: 0.000000  loss: 3.2797 (3.4135)  loss_scale: 65536.0000 (39101.2058)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.0015  max mem: 6186
Epoch: [25]  [30220/40201]  eta: 1:30:58  lr: 0.000002  min_lr: 0.000000  loss: 3.5443 (3.4136)  loss_scale: 65536.0000 (39109.9529)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.0013  max mem: 6186
Epoch: [25]  [30230/40201]  eta: 1:30:54  lr: 0.000002  min_lr: 0.000000  loss: 3.4056 (3.4136)  loss_scale: 65536.0000 (39118.6943)  weight_decay: 0.0500 (0.0500)  time: 0.7596  data: 0.0013  max mem: 6186
Epoch: [25]  [30240/40201]  eta: 1:30:49  lr: 0.000002  min_lr: 0.000000  loss: 3.1425 (3.4135)  loss_scale: 65536.0000 (39127.4299)  weight_decay: 0.0500 (0.0500)  time: 0.7594  data: 0.0006  max mem: 6186
Epoch: [25]  [30250/40201]  eta: 1:30:44  lr: 0.000002  min_lr: 0.000000  loss: 3.1425 (3.4136)  loss_scale: 65536.0000 (39136.1597)  weight_decay: 0.0500 (0.0500)  time: 0.7133  data: 0.0014  max mem: 6186
[2023-07-24 19:51:05,251] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 15129
[2023-07-24 19:51:05,251] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:51:05,251] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-24 19:51:05,270] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 15129
[2023-07-24 19:51:05,270] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [30260/40201]  eta: 1:30:39  lr: 0.000002  min_lr: 0.000000  loss: 3.4545 (3.4137)  loss_scale: 65536.0000 (39142.7181)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.0011  max mem: 6186
Epoch: [25]  [30270/40201]  eta: 1:30:34  lr: 0.000002  min_lr: 0.000000  loss: 3.4297 (3.4137)  loss_scale: 32768.0000 (39140.6122)  weight_decay: 0.0500 (0.0500)  time: 0.7022  data: 0.0004  max mem: 6186
Epoch: [25]  [30280/40201]  eta: 1:30:29  lr: 0.000002  min_lr: 0.000000  loss: 3.0149 (3.4136)  loss_scale: 32768.0000 (39138.5077)  weight_decay: 0.0500 (0.0500)  time: 0.7666  data: 0.0005  max mem: 6186
Epoch: [25]  [30290/40201]  eta: 1:30:24  lr: 0.000002  min_lr: 0.000000  loss: 3.2485 (3.4137)  loss_scale: 32768.0000 (39136.4046)  weight_decay: 0.0500 (0.0500)  time: 0.7562  data: 0.0006  max mem: 6186
Epoch: [25]  [30300/40201]  eta: 1:30:19  lr: 0.000002  min_lr: 0.000000  loss: 3.7503 (3.4138)  loss_scale: 32768.0000 (39134.3029)  weight_decay: 0.0500 (0.0500)  time: 0.7086  data: 0.0006  max mem: 6186
Epoch: [25]  [30310/40201]  eta: 1:30:15  lr: 0.000002  min_lr: 0.000000  loss: 3.2368 (3.4138)  loss_scale: 32768.0000 (39132.2026)  weight_decay: 0.0500 (0.0500)  time: 0.7240  data: 0.0006  max mem: 6186
Epoch: [25]  [30320/40201]  eta: 1:30:10  lr: 0.000002  min_lr: 0.000000  loss: 3.2191 (3.4138)  loss_scale: 32768.0000 (39130.1036)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.0014  max mem: 6186
Epoch: [25]  [30330/40201]  eta: 1:30:05  lr: 0.000002  min_lr: 0.000000  loss: 3.4517 (3.4139)  loss_scale: 32768.0000 (39128.0061)  weight_decay: 0.0500 (0.0500)  time: 0.7544  data: 0.0015  max mem: 6186
Epoch: [25]  [30340/40201]  eta: 1:30:00  lr: 0.000002  min_lr: 0.000000  loss: 3.4517 (3.4139)  loss_scale: 32768.0000 (39125.9099)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.0010  max mem: 6186
Epoch: [25]  [30350/40201]  eta: 1:29:55  lr: 0.000002  min_lr: 0.000000  loss: 3.3467 (3.4139)  loss_scale: 32768.0000 (39123.8151)  weight_decay: 0.0500 (0.0500)  time: 0.7566  data: 0.0007  max mem: 6186
Epoch: [25]  [30360/40201]  eta: 1:29:50  lr: 0.000002  min_lr: 0.000000  loss: 3.3726 (3.4138)  loss_scale: 32768.0000 (39121.7217)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.0005  max mem: 6186
Epoch: [25]  [30370/40201]  eta: 1:29:45  lr: 0.000002  min_lr: 0.000000  loss: 3.3016 (3.4138)  loss_scale: 32768.0000 (39119.6296)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.0011  max mem: 6186
Epoch: [25]  [30380/40201]  eta: 1:29:41  lr: 0.000002  min_lr: 0.000000  loss: 3.2484 (3.4138)  loss_scale: 32768.0000 (39117.5390)  weight_decay: 0.0500 (0.0500)  time: 0.7516  data: 0.0009  max mem: 6186
Epoch: [25]  [30390/40201]  eta: 1:29:36  lr: 0.000002  min_lr: 0.000000  loss: 3.0196 (3.4137)  loss_scale: 32768.0000 (39115.4497)  weight_decay: 0.0500 (0.0500)  time: 0.7441  data: 0.0003  max mem: 6186
Epoch: [25]  [30400/40201]  eta: 1:29:31  lr: 0.000002  min_lr: 0.000000  loss: 3.0196 (3.4136)  loss_scale: 32768.0000 (39113.3618)  weight_decay: 0.0500 (0.0500)  time: 0.7393  data: 0.0011  max mem: 6186
Epoch: [25]  [30410/40201]  eta: 1:29:26  lr: 0.000002  min_lr: 0.000000  loss: 3.3103 (3.4136)  loss_scale: 32768.0000 (39111.2753)  weight_decay: 0.0500 (0.0500)  time: 0.7477  data: 0.0014  max mem: 6186
Epoch: [25]  [30420/40201]  eta: 1:29:21  lr: 0.000002  min_lr: 0.000000  loss: 3.7074 (3.4136)  loss_scale: 32768.0000 (39109.1901)  weight_decay: 0.0500 (0.0500)  time: 0.7621  data: 0.0008  max mem: 6186
Epoch: [25]  [30430/40201]  eta: 1:29:16  lr: 0.000002  min_lr: 0.000000  loss: 3.9667 (3.4138)  loss_scale: 32768.0000 (39107.1063)  weight_decay: 0.0500 (0.0500)  time: 0.7516  data: 0.0007  max mem: 6186
Epoch: [25]  [30440/40201]  eta: 1:29:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7243 (3.4139)  loss_scale: 32768.0000 (39105.0239)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.0015  max mem: 6186
Epoch: [25]  [30450/40201]  eta: 1:29:06  lr: 0.000002  min_lr: 0.000000  loss: 3.2859 (3.4138)  loss_scale: 32768.0000 (39102.9428)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.0015  max mem: 6186
Epoch: [25]  [30460/40201]  eta: 1:29:02  lr: 0.000002  min_lr: 0.000000  loss: 3.6975 (3.4140)  loss_scale: 32768.0000 (39100.8631)  weight_decay: 0.0500 (0.0500)  time: 0.7208  data: 0.0005  max mem: 6186
Epoch: [25]  [30470/40201]  eta: 1:28:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6267 (3.4140)  loss_scale: 32768.0000 (39098.7848)  weight_decay: 0.0500 (0.0500)  time: 0.7396  data: 0.0009  max mem: 6186
Epoch: [25]  [30480/40201]  eta: 1:28:52  lr: 0.000002  min_lr: 0.000000  loss: 3.4664 (3.4141)  loss_scale: 32768.0000 (39096.7079)  weight_decay: 0.0500 (0.0500)  time: 0.7224  data: 0.0015  max mem: 6186
Epoch: [25]  [30490/40201]  eta: 1:28:46  lr: 0.000002  min_lr: 0.000000  loss: 3.5980 (3.4142)  loss_scale: 32768.0000 (39094.6323)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0018  max mem: 6186
Epoch: [25]  [30500/40201]  eta: 1:28:41  lr: 0.000002  min_lr: 0.000000  loss: 3.3010 (3.4141)  loss_scale: 32768.0000 (39092.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.1186  max mem: 6186
Epoch: [25]  [30510/40201]  eta: 1:28:35  lr: 0.000002  min_lr: 0.000000  loss: 3.3073 (3.4142)  loss_scale: 32768.0000 (39090.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.2269  max mem: 6186
[2023-07-24 19:54:06,252] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:54:06,252] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:54:06,257] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:54:06,258] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [30520/40201]  eta: 1:28:29  lr: 0.000002  min_lr: 0.000000  loss: 3.2583 (3.4141)  loss_scale: 32768.0000 (39092.7081)  weight_decay: 0.0500 (0.0500)  time: 0.4703  data: 0.1274  max mem: 6186
Epoch: [25]  [30530/40201]  eta: 1:28:23  lr: 0.000002  min_lr: 0.000000  loss: 3.1900 (3.4141)  loss_scale: 65536.0000 (39101.3692)  weight_decay: 0.0500 (0.0500)  time: 0.4644  data: 0.0786  max mem: 6186
Epoch: [25]  [30540/40201]  eta: 1:28:19  lr: 0.000002  min_lr: 0.000000  loss: 3.1354 (3.4140)  loss_scale: 65536.0000 (39110.0247)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.1451  max mem: 6186
Epoch: [25]  [30550/40201]  eta: 1:28:14  lr: 0.000002  min_lr: 0.000000  loss: 3.3266 (3.4141)  loss_scale: 65536.0000 (39118.6745)  weight_decay: 0.0500 (0.0500)  time: 0.7818  data: 0.0855  max mem: 6186
[2023-07-24 19:54:28,609] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 15275
[2023-07-24 19:54:28,609] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:54:28,626] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 15275
[2023-07-24 19:54:28,626] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-24 19:54:28,626] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [30560/40201]  eta: 1:28:09  lr: 0.000002  min_lr: 0.000000  loss: 3.4247 (3.4140)  loss_scale: 32768.0000 (39116.5964)  weight_decay: 0.0500 (0.0500)  time: 0.7082  data: 0.0015  max mem: 6186
Epoch: [25]  [30570/40201]  eta: 1:28:04  lr: 0.000002  min_lr: 0.000000  loss: 3.3618 (3.4139)  loss_scale: 32768.0000 (39114.5198)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.0010  max mem: 6186
Epoch: [25]  [30580/40201]  eta: 1:27:59  lr: 0.000002  min_lr: 0.000000  loss: 2.7752 (3.4137)  loss_scale: 32768.0000 (39112.4445)  weight_decay: 0.0500 (0.0500)  time: 0.7318  data: 0.0005  max mem: 6186
Epoch: [25]  [30590/40201]  eta: 1:27:54  lr: 0.000002  min_lr: 0.000000  loss: 3.1837 (3.4139)  loss_scale: 32768.0000 (39110.3705)  weight_decay: 0.0500 (0.0500)  time: 0.7782  data: 0.0004  max mem: 6186
Epoch: [25]  [30600/40201]  eta: 1:27:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7116 (3.4139)  loss_scale: 32768.0000 (39108.2979)  weight_decay: 0.0500 (0.0500)  time: 0.7680  data: 0.0004  max mem: 6186
Epoch: [25]  [30610/40201]  eta: 1:27:44  lr: 0.000002  min_lr: 0.000000  loss: 3.3000 (3.4138)  loss_scale: 32768.0000 (39106.2267)  weight_decay: 0.0500 (0.0500)  time: 0.7142  data: 0.0009  max mem: 6186
Epoch: [25]  [30620/40201]  eta: 1:27:39  lr: 0.000002  min_lr: 0.000000  loss: 3.2086 (3.4138)  loss_scale: 32768.0000 (39104.1568)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.0009  max mem: 6186
Epoch: [25]  [30630/40201]  eta: 1:27:35  lr: 0.000002  min_lr: 0.000000  loss: 3.6356 (3.4139)  loss_scale: 32768.0000 (39102.0882)  weight_decay: 0.0500 (0.0500)  time: 0.7518  data: 0.0005  max mem: 6186
Epoch: [25]  [30640/40201]  eta: 1:27:30  lr: 0.000002  min_lr: 0.000000  loss: 3.4977 (3.4140)  loss_scale: 32768.0000 (39100.0210)  weight_decay: 0.0500 (0.0500)  time: 0.7744  data: 0.0007  max mem: 6186
Epoch: [25]  [30650/40201]  eta: 1:27:25  lr: 0.000002  min_lr: 0.000000  loss: 3.3892 (3.4139)  loss_scale: 32768.0000 (39097.9552)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.0004  max mem: 6186
Epoch: [25]  [30660/40201]  eta: 1:27:20  lr: 0.000002  min_lr: 0.000000  loss: 3.2823 (3.4140)  loss_scale: 32768.0000 (39095.8907)  weight_decay: 0.0500 (0.0500)  time: 0.6951  data: 0.0007  max mem: 6186
Epoch: [25]  [30670/40201]  eta: 1:27:15  lr: 0.000002  min_lr: 0.000000  loss: 3.2306 (3.4138)  loss_scale: 32768.0000 (39093.8275)  weight_decay: 0.0500 (0.0500)  time: 0.7434  data: 0.0007  max mem: 6186
Epoch: [25]  [30680/40201]  eta: 1:27:10  lr: 0.000002  min_lr: 0.000000  loss: 3.0031 (3.4139)  loss_scale: 32768.0000 (39091.7657)  weight_decay: 0.0500 (0.0500)  time: 0.7659  data: 0.0007  max mem: 6186
Epoch: [25]  [30690/40201]  eta: 1:27:05  lr: 0.000002  min_lr: 0.000000  loss: 3.3307 (3.4138)  loss_scale: 32768.0000 (39089.7053)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.0012  max mem: 6186
Epoch: [25]  [30700/40201]  eta: 1:26:59  lr: 0.000002  min_lr: 0.000000  loss: 3.3665 (3.4139)  loss_scale: 32768.0000 (39087.6461)  weight_decay: 0.0500 (0.0500)  time: 0.4819  data: 0.0013  max mem: 6186
Epoch: [25]  [30710/40201]  eta: 1:26:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6916 (3.4140)  loss_scale: 32768.0000 (39085.5884)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0011  max mem: 6186
Epoch: [25]  [30720/40201]  eta: 1:26:47  lr: 0.000002  min_lr: 0.000000  loss: 3.1779 (3.4139)  loss_scale: 32768.0000 (39083.5319)  weight_decay: 0.0500 (0.0500)  time: 0.3853  data: 0.0017  max mem: 6186
Epoch: [25]  [30730/40201]  eta: 1:26:41  lr: 0.000002  min_lr: 0.000000  loss: 3.0370 (3.4139)  loss_scale: 32768.0000 (39081.4768)  weight_decay: 0.0500 (0.0500)  time: 0.3905  data: 0.0028  max mem: 6186
Epoch: [25]  [30740/40201]  eta: 1:26:35  lr: 0.000002  min_lr: 0.000000  loss: 3.3101 (3.4139)  loss_scale: 32768.0000 (39079.4231)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0024  max mem: 6186
Epoch: [25]  [30750/40201]  eta: 1:26:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5446 (3.4140)  loss_scale: 32768.0000 (39077.3706)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0012  max mem: 6186
Epoch: [25]  [30760/40201]  eta: 1:26:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6385 (3.4140)  loss_scale: 32768.0000 (39075.3195)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0007  max mem: 6186
Epoch: [25]  [30770/40201]  eta: 1:26:19  lr: 0.000002  min_lr: 0.000000  loss: 3.2431 (3.4140)  loss_scale: 32768.0000 (39073.2698)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0009  max mem: 6186
Epoch: [25]  [30780/40201]  eta: 1:26:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4054 (3.4140)  loss_scale: 32768.0000 (39071.2213)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0009  max mem: 6186
Epoch: [25]  [30790/40201]  eta: 1:26:08  lr: 0.000002  min_lr: 0.000000  loss: 3.4054 (3.4140)  loss_scale: 32768.0000 (39069.1742)  weight_decay: 0.0500 (0.0500)  time: 0.4554  data: 0.0024  max mem: 6186
Epoch: [25]  [30800/40201]  eta: 1:26:02  lr: 0.000002  min_lr: 0.000000  loss: 3.4984 (3.4141)  loss_scale: 32768.0000 (39067.1285)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0031  max mem: 6186
[2023-07-24 19:57:09,730] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:57:09,730] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-24 19:57:09,749] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-24 19:57:09,749] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [30810/40201]  eta: 1:25:57  lr: 0.000002  min_lr: 0.000000  loss: 3.5803 (3.4141)  loss_scale: 32768.0000 (39067.2111)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0016  max mem: 6186
Epoch: [25]  [30820/40201]  eta: 1:25:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6184 (3.4144)  loss_scale: 65536.0000 (39075.7990)  weight_decay: 0.0500 (0.0500)  time: 0.8148  data: 0.0090  max mem: 6186
Epoch: [25]  [30830/40201]  eta: 1:25:48  lr: 0.000002  min_lr: 0.000000  loss: 3.9170 (3.4145)  loss_scale: 65536.0000 (39084.3813)  weight_decay: 0.0500 (0.0500)  time: 0.7769  data: 0.0086  max mem: 6186
Epoch: [25]  [30840/40201]  eta: 1:25:43  lr: 0.000002  min_lr: 0.000000  loss: 3.4221 (3.4144)  loss_scale: 65536.0000 (39092.9581)  weight_decay: 0.0500 (0.0500)  time: 0.7423  data: 0.0003  max mem: 6186
Epoch: [25]  [30850/40201]  eta: 1:25:38  lr: 0.000002  min_lr: 0.000000  loss: 3.2575 (3.4145)  loss_scale: 65536.0000 (39101.5293)  weight_decay: 0.0500 (0.0500)  time: 0.7421  data: 0.0005  max mem: 6186
Epoch: [25]  [30860/40201]  eta: 1:25:33  lr: 0.000002  min_lr: 0.000000  loss: 3.2048 (3.4143)  loss_scale: 65536.0000 (39110.0949)  weight_decay: 0.0500 (0.0500)  time: 0.7287  data: 0.0008  max mem: 6186
/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3462982 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3462983 closing signal SIGHUP
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 843, in _invoke_run
    time.sleep(monitor_interval)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3462978 got signal: 1
/usr/bin/python: No module named torch.distributed
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=3, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/data/i5O/kinetics-dataset/annotations', data_root='/data/i5O/kinetics400/train/', data_set='Kinetics-400', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=30, eval=False, eval_data_path=None, eval_log_name='log_eval', fc_drop_rate=0.0, finetune='/data/i5O/finetuned/july24/checkpoint-24/mp_rank_00_model_states.pt', gpu=0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', lr=0.001, merge_test=False, min_lr=0.001, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_small_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=400, no_save_best_ckpt=False, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='OUTPUT/mvd_vit_small_with_vit_base_teacher_k400_epoch_400/finetune_on_k400', pin_mem=True, rank=0, recount=1, remode='pixel', remove_pos_emb=False, reprob=0.25, resplit=False, resume='', resume_best=False, sampling_rate=4, save_ckpt=True, save_ckpt_freq=1, seed=0, short_side_size=224, smoothing=0.1, start_epoch=25, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=2, use_checkpoint=False, use_cls_token=False, use_mean_pooling=True, warmup_epochs=0, warmup_lr=0.001, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=2)
Patch size = (16, 16)
                                                        0    1
0       /data/i5O/kinetics400/train/-3B32lodo2M_000059...    0
1       /data/i5O/kinetics400/train/-7kbO0v4hag_000107...    0
2       /data/i5O/kinetics400/train/-bwYZwnwb8E_000013...    0
3       /data/i5O/kinetics400/train/-Cv3NwxG_8g_000087...    0
4       /data/i5O/kinetics400/train/-hLv_HL6UhY_000151...    0
...                                                   ...  ...
241202  /data/i5O/kinetics400/train/_GRX1r0JV30_000039...  399
241203  /data/i5O/kinetics400/train/_JuIL8GGX0A_000150...  399
241204  /data/i5O/kinetics400/train/_Jun6C7ICps_000037...  399
241205  /data/i5O/kinetics400/train/_SyAhrLns4k_000200...  399
241206  /data/i5O/kinetics400/train/_URmIF4eHg4_000014...  399

[241207 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
                                                       0    1
0      /data/i5O/kinetics400/val/0wR5jVB-WPk_000417_0...    0
1      /data/i5O/kinetics400/val/3caPS4FHFF8_000036_0...    0
2      /data/i5O/kinetics400/val/3yaoNwz99xM_000062_0...    0
3      /data/i5O/kinetics400/val/6IbvOJxXnOo_000047_0...    0
4      /data/i5O/kinetics400/val/6_4kjPiQr7w_000191_0...    0
...                                                  ...  ...
19874  /data/i5O/kinetics400/val/w5hbJLVhZDI_000093_0...  399
19875  /data/i5O/kinetics400/val/xDd6uIBeMEA_000001_0...  399
19876  /data/i5O/kinetics400/val/XWvGn7eI04A_000012_0...  399
19877  /data/i5O/kinetics400/val/yGdQwxP5koA_000083_0...  399
19878  /data/i5O/kinetics400/val/ZVDR2od1gn8_000037_0...  399

[19879 rows x 2 columns]
Number of the class = 400
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f4f481a5e20>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Load ckpt from /data/i5O/finetuned/july24/checkpoint-24/mp_rank_00_model_states.pt
Load state_dict by model_key = module
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=400, bias=True)
)
number of params: 22033936
LR = 0.00004688
Batch size = 12
Update frequent = 2
Number of training examples = 241207
Number of training training per epoch = 20100
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-07-25 15:30:58,700] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.8, git-hash=unknown, git-branch=unknown
[2023-07-25 15:30:58,710] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups
[2023-07-25 15:30:58,710] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1
[2023-07-25 15:30:58,752] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1
[2023-07-25 15:30:58,763] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0, 1]
[2023-07-25 15:30:58,764] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]
[2023-07-25 15:30:58,774] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [1]
[2023-07-25 15:30:58,912] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.4 does not match the version torch was compiled with 11.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py38_cu111/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.9867630004882812 seconds
[2023-07-25 15:31:00,883] [INFO] [engine.py:1108:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-07-25 15:31:00,892] [INFO] [engine.py:1116:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2023-07-25 15:31:00,893] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-07-25 15:31:00,904] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-07-25 15:31:00,904] [INFO] [engine.py:808:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-07-25 15:31:00,904] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-07-25 15:31:00,904] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-25 15:31:00,904] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   amp_params ................... False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   dump_state ................... False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'min_scale': 1}
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2023-07-25 15:31:00,905] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   global_rank .................. 0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 2
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 128
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   optimizer_name ............... adam
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   pld_params ................... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   steps_per_print .............. 1000
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   train_batch_size ............. 12
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  3
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   world_size ................... 2
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2023-07-25 15:31:00,906] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 0, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-07-25 15:31:00,907] [INFO] [config.py:1063:print]   zero_enabled ................. False
[2023-07-25 15:31:00,907] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0
[2023-07-25 15:31:00,907] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 3, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
Using /root/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.3068900108337402 seconds
model.gradient_accumulation_steps() = 2
Use step level LR scheduler!
Set warmup steps = 0
Set warmup steps = 0
len(lr_schedule_values) = 603000
np.unique(lr_schedule_values) = [4.6875e-05]
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 30 epochs
Epoch: [25]  [    0/40201]  eta: 1 day, 22:14:13  lr: 0.000047  min_lr: 0.000001  loss: 3.9815 (3.9815)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1405  data: 3.7591  max mem: 6151
Epoch: [25]  [   10/40201]  eta: 10:06:24  lr: 0.000047  min_lr: 0.000001  loss: 3.6440 (3.6172)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9053  data: 0.3437  max mem: 6185
Epoch: [25]  [   20/40201]  eta: 8:06:26  lr: 0.000047  min_lr: 0.000001  loss: 3.2421 (3.5470)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0013  max mem: 6186
Epoch: [25]  [   30/40201]  eta: 7:18:45  lr: 0.000047  min_lr: 0.000001  loss: 3.5725 (3.7103)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0004  max mem: 6186
Epoch: [25]  [   40/40201]  eta: 6:55:12  lr: 0.000047  min_lr: 0.000001  loss: 3.3968 (3.5988)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0004  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/SZtj2TEWiHc_000195_000205.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [   50/40201]  eta: 6:40:49  lr: 0.000047  min_lr: 0.000001  loss: 3.2531 (3.5271)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0011  max mem: 6186
Epoch: [25]  [   60/40201]  eta: 6:32:37  lr: 0.000047  min_lr: 0.000001  loss: 2.8793 (3.4276)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0011  max mem: 6186
Epoch: [25]  [   70/40201]  eta: 6:26:14  lr: 0.000047  min_lr: 0.000001  loss: 2.8793 (3.3979)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0006  max mem: 6186
Epoch: [25]  [   80/40201]  eta: 6:21:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2169 (3.4473)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0005  max mem: 6186
Epoch: [25]  [   90/40201]  eta: 6:17:33  lr: 0.000047  min_lr: 0.000001  loss: 3.8710 (3.4564)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0005  max mem: 6186
Epoch: [25]  [  100/40201]  eta: 6:14:34  lr: 0.000047  min_lr: 0.000001  loss: 3.4801 (3.4496)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0005  max mem: 6186
Epoch: [25]  [  110/40201]  eta: 6:12:05  lr: 0.000047  min_lr: 0.000001  loss: 3.4801 (3.4860)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0005  max mem: 6186
Epoch: [25]  [  120/40201]  eta: 6:09:23  lr: 0.000047  min_lr: 0.000001  loss: 3.5283 (3.4655)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0011  max mem: 6186
Epoch: [25]  [  130/40201]  eta: 6:07:15  lr: 0.000047  min_lr: 0.000001  loss: 3.3020 (3.4753)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0011  max mem: 6186
Epoch: [25]  [  140/40201]  eta: 6:05:36  lr: 0.000047  min_lr: 0.000001  loss: 3.2781 (3.4611)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0012  max mem: 6186
Epoch: [25]  [  150/40201]  eta: 6:04:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2678 (3.4544)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0012  max mem: 6186
Epoch: [25]  [  160/40201]  eta: 6:02:53  lr: 0.000047  min_lr: 0.000001  loss: 3.4037 (3.4838)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0011  max mem: 6186
Epoch: [25]  [  170/40201]  eta: 6:01:34  lr: 0.000047  min_lr: 0.000001  loss: 3.4037 (3.4807)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0011  max mem: 6186
Epoch: [25]  [  180/40201]  eta: 6:00:25  lr: 0.000047  min_lr: 0.000001  loss: 3.1478 (3.4742)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0005  max mem: 6186
Epoch: [25]  [  190/40201]  eta: 5:59:21  lr: 0.000047  min_lr: 0.000001  loss: 3.5009 (3.4827)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0005  max mem: 6186
Epoch: [25]  [  200/40201]  eta: 5:58:27  lr: 0.000047  min_lr: 0.000001  loss: 3.6223 (3.4808)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0005  max mem: 6186
Epoch: [25]  [  210/40201]  eta: 5:57:49  lr: 0.000047  min_lr: 0.000001  loss: 3.6731 (3.4967)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0005  max mem: 6186
Epoch: [25]  [  220/40201]  eta: 5:57:19  lr: 0.000047  min_lr: 0.000001  loss: 3.3739 (3.4781)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0005  max mem: 6186
Epoch: [25]  [  230/40201]  eta: 5:56:34  lr: 0.000047  min_lr: 0.000001  loss: 2.8357 (3.4514)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0004  max mem: 6186
Epoch: [25]  [  240/40201]  eta: 5:56:29  lr: 0.000047  min_lr: 0.000001  loss: 2.8927 (3.4446)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0004  max mem: 6186
Epoch: [25]  [  250/40201]  eta: 5:55:58  lr: 0.000047  min_lr: 0.000001  loss: 3.2609 (3.4318)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0005  max mem: 6186
[2023-07-25 15:33:22,406] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:33:22,406] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-07-25 15:33:22,408] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:33:22,409] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [25]  [  260/40201]  eta: 5:55:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2062 (3.4323)  loss_scale: 128.0000 (129.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5223  data: 0.0005  max mem: 6186
Epoch: [25]  [  270/40201]  eta: 5:54:55  lr: 0.000047  min_lr: 0.000001  loss: 3.2044 (3.4322)  loss_scale: 256.0000 (134.6125)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0005  max mem: 6186
Epoch: [25]  [  280/40201]  eta: 5:54:28  lr: 0.000047  min_lr: 0.000001  loss: 3.5526 (3.4393)  loss_scale: 256.0000 (138.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0005  max mem: 6186
Epoch: [25]  [  290/40201]  eta: 5:53:53  lr: 0.000047  min_lr: 0.000001  loss: 3.9119 (3.4529)  loss_scale: 256.0000 (142.9553)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0005  max mem: 6186
Epoch: [25]  [  300/40201]  eta: 5:53:48  lr: 0.000047  min_lr: 0.000001  loss: 3.7138 (3.4526)  loss_scale: 256.0000 (146.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0005  max mem: 6186
Epoch: [25]  [  310/40201]  eta: 5:53:27  lr: 0.000047  min_lr: 0.000001  loss: 3.6422 (3.4501)  loss_scale: 256.0000 (150.2251)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0005  max mem: 6186
Epoch: [25]  [  320/40201]  eta: 5:53:13  lr: 0.000047  min_lr: 0.000001  loss: 3.6654 (3.4653)  loss_scale: 256.0000 (153.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0012  max mem: 6186
Epoch: [25]  [  330/40201]  eta: 5:52:43  lr: 0.000047  min_lr: 0.000001  loss: 3.4132 (3.4606)  loss_scale: 256.0000 (156.6163)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0018  max mem: 6186
Epoch: [25]  [  340/40201]  eta: 5:52:23  lr: 0.000047  min_lr: 0.000001  loss: 3.7656 (3.4701)  loss_scale: 256.0000 (159.5308)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0018  max mem: 6186
Epoch: [25]  [  350/40201]  eta: 5:52:01  lr: 0.000047  min_lr: 0.000001  loss: 3.8566 (3.4805)  loss_scale: 256.0000 (162.2792)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0011  max mem: 6186
Epoch: [25]  [  360/40201]  eta: 5:51:49  lr: 0.000047  min_lr: 0.000001  loss: 3.6138 (3.4860)  loss_scale: 256.0000 (164.8753)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0005  max mem: 6186
Epoch: [25]  [  370/40201]  eta: 5:51:42  lr: 0.000047  min_lr: 0.000001  loss: 3.7230 (3.4943)  loss_scale: 256.0000 (167.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0005  max mem: 6186
Epoch: [25]  [  380/40201]  eta: 5:51:29  lr: 0.000047  min_lr: 0.000001  loss: 3.7600 (3.5129)  loss_scale: 256.0000 (169.6588)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0005  max mem: 6186
Epoch: [25]  [  390/40201]  eta: 5:51:10  lr: 0.000047  min_lr: 0.000001  loss: 3.4273 (3.5008)  loss_scale: 256.0000 (171.8670)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0005  max mem: 6186
Epoch: [25]  [  400/40201]  eta: 5:50:49  lr: 0.000047  min_lr: 0.000001  loss: 3.2219 (3.5032)  loss_scale: 256.0000 (173.9651)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0005  max mem: 6186
Epoch: [25]  [  410/40201]  eta: 5:50:31  lr: 0.000047  min_lr: 0.000001  loss: 3.4629 (3.5063)  loss_scale: 256.0000 (175.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0005  max mem: 6186
Epoch: [25]  [  420/40201]  eta: 5:50:14  lr: 0.000047  min_lr: 0.000001  loss: 3.5708 (3.5145)  loss_scale: 256.0000 (177.8622)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0005  max mem: 6186
Epoch: [25]  [  430/40201]  eta: 5:49:51  lr: 0.000047  min_lr: 0.000001  loss: 3.5950 (3.5115)  loss_scale: 256.0000 (179.6752)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0009  max mem: 6186
Epoch: [25]  [  440/40201]  eta: 5:49:38  lr: 0.000047  min_lr: 0.000001  loss: 3.4334 (3.5123)  loss_scale: 256.0000 (181.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0009  max mem: 6186
Epoch: [25]  [  450/40201]  eta: 5:49:43  lr: 0.000047  min_lr: 0.000001  loss: 3.4334 (3.5081)  loss_scale: 256.0000 (183.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0005  max mem: 6186
Epoch: [25]  [  460/40201]  eta: 5:49:22  lr: 0.000047  min_lr: 0.000001  loss: 3.3657 (3.5003)  loss_scale: 256.0000 (184.6421)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0005  max mem: 6186
Epoch: [25]  [  470/40201]  eta: 5:48:58  lr: 0.000047  min_lr: 0.000001  loss: 3.2227 (3.4947)  loss_scale: 256.0000 (186.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0005  max mem: 6186
Epoch: [25]  [  480/40201]  eta: 5:48:39  lr: 0.000047  min_lr: 0.000001  loss: 3.0398 (3.4889)  loss_scale: 256.0000 (187.6091)  weight_decay: 0.0500 (0.0500)  time: 0.5082  data: 0.0005  max mem: 6186
Epoch: [25]  [  490/40201]  eta: 5:48:32  lr: 0.000047  min_lr: 0.000001  loss: 3.3805 (3.4964)  loss_scale: 256.0000 (189.0020)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0013  max mem: 6186
Epoch: [25]  [  500/40201]  eta: 5:48:20  lr: 0.000047  min_lr: 0.000001  loss: 3.3805 (3.4924)  loss_scale: 256.0000 (190.3393)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0013  max mem: 6186
Epoch: [25]  [  510/40201]  eta: 5:48:10  lr: 0.000047  min_lr: 0.000001  loss: 3.2406 (3.4959)  loss_scale: 256.0000 (191.6243)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0005  max mem: 6186
[2023-07-25 15:35:34,993] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:35:34,993] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-07-25 15:35:35,007] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:35:35,007] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [25]  [  520/40201]  eta: 5:47:54  lr: 0.000047  min_lr: 0.000001  loss: 3.5399 (3.4980)  loss_scale: 256.0000 (196.7908)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0005  max mem: 6186
Epoch: [25]  [  530/40201]  eta: 5:47:36  lr: 0.000047  min_lr: 0.000001  loss: 3.1166 (3.4899)  loss_scale: 512.0000 (202.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0005  max mem: 6186
Epoch: [25]  [  540/40201]  eta: 5:47:24  lr: 0.000047  min_lr: 0.000001  loss: 3.0729 (3.4854)  loss_scale: 512.0000 (208.4436)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0005  max mem: 6186
Epoch: [25]  [  550/40201]  eta: 5:47:04  lr: 0.000047  min_lr: 0.000001  loss: 3.1676 (3.4947)  loss_scale: 512.0000 (213.9528)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0005  max mem: 6186
Epoch: [25]  [  560/40201]  eta: 5:46:52  lr: 0.000047  min_lr: 0.000001  loss: 3.8231 (3.5007)  loss_scale: 512.0000 (219.2656)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0004  max mem: 6186
Epoch: [25]  [  570/40201]  eta: 5:46:43  lr: 0.000047  min_lr: 0.000001  loss: 3.8231 (3.5060)  loss_scale: 512.0000 (224.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0005  max mem: 6186
Epoch: [25]  [  580/40201]  eta: 5:46:53  lr: 0.000047  min_lr: 0.000001  loss: 3.4049 (3.4980)  loss_scale: 512.0000 (229.3425)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0005  max mem: 6186
Epoch: [25]  [  590/40201]  eta: 5:46:46  lr: 0.000047  min_lr: 0.000001  loss: 3.1405 (3.4950)  loss_scale: 512.0000 (234.1252)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0005  max mem: 6186
Epoch: [25]  [  600/40201]  eta: 5:46:45  lr: 0.000047  min_lr: 0.000001  loss: 3.3162 (3.4967)  loss_scale: 512.0000 (238.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0005  max mem: 6186
Epoch: [25]  [  610/40201]  eta: 5:46:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2997 (3.4926)  loss_scale: 512.0000 (243.2209)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0005  max mem: 6186
Epoch: [25]  [  620/40201]  eta: 5:46:28  lr: 0.000047  min_lr: 0.000001  loss: 3.1102 (3.4918)  loss_scale: 512.0000 (247.5491)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0005  max mem: 6186
Epoch: [25]  [  630/40201]  eta: 5:46:16  lr: 0.000047  min_lr: 0.000001  loss: 3.2792 (3.4891)  loss_scale: 512.0000 (251.7401)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0013  max mem: 6186
Epoch: [25]  [  640/40201]  eta: 5:46:03  lr: 0.000047  min_lr: 0.000001  loss: 3.4453 (3.4894)  loss_scale: 512.0000 (255.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0013  max mem: 6186
Epoch: [25]  [  650/40201]  eta: 5:45:50  lr: 0.000047  min_lr: 0.000001  loss: 3.4640 (3.4885)  loss_scale: 512.0000 (259.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0005  max mem: 6186
Epoch: [25]  [  660/40201]  eta: 5:45:36  lr: 0.000047  min_lr: 0.000001  loss: 3.4640 (3.4873)  loss_scale: 512.0000 (263.5522)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0005  max mem: 6186
Epoch: [25]  [  670/40201]  eta: 5:45:02  lr: 0.000047  min_lr: 0.000001  loss: 3.2311 (3.4833)  loss_scale: 512.0000 (267.2548)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0005  max mem: 6186
Epoch: [25]  [  680/40201]  eta: 5:41:49  lr: 0.000047  min_lr: 0.000001  loss: 3.0621 (3.4769)  loss_scale: 512.0000 (270.8488)  weight_decay: 0.0500 (0.0500)  time: 0.3379  data: 0.0005  max mem: 6186
Epoch: [25]  [  690/40201]  eta: 5:38:39  lr: 0.000047  min_lr: 0.000001  loss: 3.3793 (3.4771)  loss_scale: 512.0000 (274.3386)  weight_decay: 0.0500 (0.0500)  time: 0.1979  data: 0.0013  max mem: 6186
Epoch: [25]  [  700/40201]  eta: 5:39:49  lr: 0.000047  min_lr: 0.000001  loss: 3.5000 (3.4801)  loss_scale: 512.0000 (277.7290)  weight_decay: 0.0500 (0.0500)  time: 0.4211  data: 0.0925  max mem: 6186
Epoch: [25]  [  710/40201]  eta: 5:38:00  lr: 0.000047  min_lr: 0.000001  loss: 3.8991 (3.4841)  loss_scale: 512.0000 (281.0239)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.1406  max mem: 6186
Epoch: [25]  [  720/40201]  eta: 5:37:21  lr: 0.000047  min_lr: 0.000001  loss: 3.2666 (3.4817)  loss_scale: 512.0000 (284.2275)  weight_decay: 0.0500 (0.0500)  time: 0.3910  data: 0.1769  max mem: 6186
Epoch: [25]  [  730/40201]  eta: 5:35:20  lr: 0.000047  min_lr: 0.000001  loss: 3.2666 (3.4835)  loss_scale: 512.0000 (287.3434)  weight_decay: 0.0500 (0.0500)  time: 0.3744  data: 0.1533  max mem: 6186
Epoch: [25]  [  740/40201]  eta: 5:35:20  lr: 0.000047  min_lr: 0.000001  loss: 3.5697 (3.4878)  loss_scale: 512.0000 (290.3752)  weight_decay: 0.0500 (0.0500)  time: 0.4085  data: 0.0257  max mem: 6186
Epoch: [25]  [  750/40201]  eta: 5:35:27  lr: 0.000047  min_lr: 0.000001  loss: 3.5697 (3.4859)  loss_scale: 512.0000 (293.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0005  max mem: 6186
Epoch: [25]  [  760/40201]  eta: 5:35:27  lr: 0.000047  min_lr: 0.000001  loss: 3.4272 (3.4849)  loss_scale: 512.0000 (296.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0004  max mem: 6186
[2023-07-25 15:37:37,699] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:37:37,699] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-07-25 15:37:37,710] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:37:37,710] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [25]  [  770/40201]  eta: 5:35:30  lr: 0.000047  min_lr: 0.000001  loss: 3.3693 (3.4842)  loss_scale: 512.0000 (300.3268)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0004  max mem: 6186
Epoch: [25]  [  780/40201]  eta: 5:35:28  lr: 0.000047  min_lr: 0.000001  loss: 3.4046 (3.4849)  loss_scale: 1024.0000 (309.5928)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0014  max mem: 6186
Epoch: [25]  [  790/40201]  eta: 5:35:21  lr: 0.000047  min_lr: 0.000001  loss: 3.4046 (3.4889)  loss_scale: 1024.0000 (318.6245)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0014  max mem: 6186
Epoch: [25]  [  800/40201]  eta: 5:35:55  lr: 0.000047  min_lr: 0.000001  loss: 3.6594 (3.4900)  loss_scale: 1024.0000 (327.4307)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0426  max mem: 6186
Epoch: [25]  [  810/40201]  eta: 5:36:00  lr: 0.000047  min_lr: 0.000001  loss: 3.1566 (3.4791)  loss_scale: 1024.0000 (336.0197)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0425  max mem: 6186
Epoch: [25]  [  820/40201]  eta: 5:36:04  lr: 0.000047  min_lr: 0.000001  loss: 3.3168 (3.4849)  loss_scale: 1024.0000 (344.3995)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0004  max mem: 6186
Epoch: [25]  [  830/40201]  eta: 5:36:10  lr: 0.000047  min_lr: 0.000001  loss: 3.5758 (3.4819)  loss_scale: 1024.0000 (352.5776)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0005  max mem: 6186
Epoch: [25]  [  840/40201]  eta: 5:36:06  lr: 0.000047  min_lr: 0.000001  loss: 3.0975 (3.4755)  loss_scale: 1024.0000 (360.5612)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0005  max mem: 6186
Epoch: [25]  [  850/40201]  eta: 5:36:08  lr: 0.000047  min_lr: 0.000001  loss: 2.9896 (3.4711)  loss_scale: 1024.0000 (368.3572)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0005  max mem: 6186
Epoch: [25]  [  860/40201]  eta: 5:35:56  lr: 0.000047  min_lr: 0.000001  loss: 3.3005 (3.4739)  loss_scale: 1024.0000 (375.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0005  max mem: 6186
Epoch: [25]  [  870/40201]  eta: 5:35:49  lr: 0.000047  min_lr: 0.000001  loss: 3.5248 (3.4733)  loss_scale: 1024.0000 (383.4122)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0004  max mem: 6186
Epoch: [25]  [  880/40201]  eta: 5:35:41  lr: 0.000047  min_lr: 0.000001  loss: 3.6240 (3.4700)  loss_scale: 1024.0000 (390.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0004  max mem: 6186
Epoch: [25]  [  890/40201]  eta: 5:35:47  lr: 0.000047  min_lr: 0.000001  loss: 3.3061 (3.4655)  loss_scale: 1024.0000 (397.7912)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0005  max mem: 6186
Epoch: [25]  [  900/40201]  eta: 5:35:44  lr: 0.000047  min_lr: 0.000001  loss: 3.3061 (3.4656)  loss_scale: 1024.0000 (404.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0012  max mem: 6186
Epoch: [25]  [  910/40201]  eta: 5:35:44  lr: 0.000047  min_lr: 0.000001  loss: 3.3598 (3.4667)  loss_scale: 1024.0000 (411.5390)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0012  max mem: 6186
Epoch: [25]  [  920/40201]  eta: 5:35:45  lr: 0.000047  min_lr: 0.000001  loss: 3.2452 (3.4628)  loss_scale: 1024.0000 (418.1889)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0012  max mem: 6186
Epoch: [25]  [  930/40201]  eta: 5:35:46  lr: 0.000047  min_lr: 0.000001  loss: 3.4621 (3.4674)  loss_scale: 1024.0000 (424.6960)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0022  max mem: 6186
Epoch: [25]  [  940/40201]  eta: 5:33:48  lr: 0.000047  min_lr: 0.000001  loss: 3.8275 (3.4685)  loss_scale: 1024.0000 (431.0648)  weight_decay: 0.0500 (0.0500)  time: 0.3854  data: 0.0014  max mem: 6186
Epoch: [25]  [  950/40201]  eta: 5:31:28  lr: 0.000047  min_lr: 0.000001  loss: 3.6617 (3.4683)  loss_scale: 1024.0000 (437.2997)  weight_decay: 0.0500 (0.0500)  time: 0.2127  data: 0.0011  max mem: 6186
Epoch: [25]  [  960/40201]  eta: 5:30:03  lr: 0.000047  min_lr: 0.000001  loss: 3.6622 (3.4704)  loss_scale: 1024.0000 (443.4048)  weight_decay: 0.0500 (0.0500)  time: 0.2465  data: 0.0271  max mem: 6186
Epoch: [25]  [  970/40201]  eta: 5:29:29  lr: 0.000047  min_lr: 0.000001  loss: 3.3267 (3.4675)  loss_scale: 1024.0000 (449.3841)  weight_decay: 0.0500 (0.0500)  time: 0.3727  data: 0.0562  max mem: 6186
Epoch: [25]  [  980/40201]  eta: 5:27:53  lr: 0.000047  min_lr: 0.000001  loss: 3.1786 (3.4680)  loss_scale: 1024.0000 (455.2416)  weight_decay: 0.0500 (0.0500)  time: 0.3543  data: 0.0302  max mem: 6186
Epoch: [25]  [  990/40201]  eta: 5:26:34  lr: 0.000047  min_lr: 0.000001  loss: 3.3767 (3.4647)  loss_scale: 1024.0000 (460.9808)  weight_decay: 0.0500 (0.0500)  time: 0.2943  data: 0.0129  max mem: 6186
[2023-07-25 15:39:24,684] [INFO] [timer.py:181:stop] 0/1000, SamplesPerSec=13.495006676485383
Epoch: [25]  [ 1000/40201]  eta: 5:26:35  lr: 0.000047  min_lr: 0.000001  loss: 3.3372 (3.4666)  loss_scale: 1024.0000 (466.6054)  weight_decay: 0.0500 (0.0500)  time: 0.4149  data: 0.0128  max mem: 6186
Epoch: [25]  [ 1010/40201]  eta: 5:26:33  lr: 0.000047  min_lr: 0.000001  loss: 3.7673 (3.4685)  loss_scale: 1024.0000 (472.1187)  weight_decay: 0.0500 (0.0500)  time: 0.5117  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1020/40201]  eta: 5:26:31  lr: 0.000047  min_lr: 0.000001  loss: 3.3732 (3.4672)  loss_scale: 1024.0000 (477.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0004  max mem: 6186
[2023-07-25 15:39:37,878] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:39:37,878] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-07-25 15:39:37,882] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:39:37,882] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [25]  [ 1030/40201]  eta: 5:26:37  lr: 0.000047  min_lr: 0.000001  loss: 3.1189 (3.4639)  loss_scale: 1024.0000 (488.7837)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1040/40201]  eta: 5:26:40  lr: 0.000047  min_lr: 0.000001  loss: 3.3433 (3.4657)  loss_scale: 2048.0000 (503.7618)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1050/40201]  eta: 5:26:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2254 (3.4613)  loss_scale: 2048.0000 (518.4548)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1060/40201]  eta: 5:26:38  lr: 0.000047  min_lr: 0.000001  loss: 3.3302 (3.4634)  loss_scale: 2048.0000 (532.8709)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1070/40201]  eta: 5:26:35  lr: 0.000047  min_lr: 0.000001  loss: 3.6726 (3.4678)  loss_scale: 2048.0000 (547.0177)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1080/40201]  eta: 5:26:38  lr: 0.000047  min_lr: 0.000001  loss: 3.6076 (3.4656)  loss_scale: 2048.0000 (560.9029)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1090/40201]  eta: 5:26:41  lr: 0.000047  min_lr: 0.000001  loss: 3.1906 (3.4633)  loss_scale: 2048.0000 (574.5335)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1100/40201]  eta: 5:26:50  lr: 0.000047  min_lr: 0.000001  loss: 3.0576 (3.4625)  loss_scale: 2048.0000 (587.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1110/40201]  eta: 5:26:54  lr: 0.000047  min_lr: 0.000001  loss: 3.3393 (3.4646)  loss_scale: 2048.0000 (601.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1120/40201]  eta: 5:26:51  lr: 0.000047  min_lr: 0.000001  loss: 3.8789 (3.4703)  loss_scale: 2048.0000 (613.9661)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1130/40201]  eta: 5:26:50  lr: 0.000047  min_lr: 0.000001  loss: 3.6955 (3.4712)  loss_scale: 2048.0000 (626.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1140/40201]  eta: 5:26:49  lr: 0.000047  min_lr: 0.000001  loss: 3.5584 (3.4720)  loss_scale: 2048.0000 (639.1025)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1150/40201]  eta: 5:26:50  lr: 0.000047  min_lr: 0.000001  loss: 3.5383 (3.4720)  loss_scale: 2048.0000 (651.3432)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1160/40201]  eta: 5:26:48  lr: 0.000047  min_lr: 0.000001  loss: 3.3189 (3.4723)  loss_scale: 2048.0000 (663.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1170/40201]  eta: 5:26:49  lr: 0.000047  min_lr: 0.000001  loss: 3.1599 (3.4695)  loss_scale: 2048.0000 (675.1973)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1180/40201]  eta: 5:26:48  lr: 0.000047  min_lr: 0.000001  loss: 3.5670 (3.4734)  loss_scale: 2048.0000 (686.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5166  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1190/40201]  eta: 5:26:44  lr: 0.000047  min_lr: 0.000001  loss: 3.5843 (3.4738)  loss_scale: 2048.0000 (698.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1200/40201]  eta: 5:26:42  lr: 0.000047  min_lr: 0.000001  loss: 3.5404 (3.4753)  loss_scale: 2048.0000 (709.4888)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1210/40201]  eta: 5:25:50  lr: 0.000047  min_lr: 0.000001  loss: 3.3774 (3.4728)  loss_scale: 2048.0000 (720.5417)  weight_decay: 0.0500 (0.0500)  time: 0.4339  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1220/40201]  eta: 5:24:32  lr: 0.000047  min_lr: 0.000001  loss: 3.0519 (3.4706)  loss_scale: 2048.0000 (731.4136)  weight_decay: 0.0500 (0.0500)  time: 0.3141  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1230/40201]  eta: 5:24:30  lr: 0.000047  min_lr: 0.000001  loss: 3.0641 (3.4691)  loss_scale: 2048.0000 (742.1089)  weight_decay: 0.0500 (0.0500)  time: 0.3915  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1240/40201]  eta: 5:23:26  lr: 0.000047  min_lr: 0.000001  loss: 3.5589 (3.4716)  loss_scale: 2048.0000 (752.6317)  weight_decay: 0.0500 (0.0500)  time: 0.4108  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1250/40201]  eta: 5:22:05  lr: 0.000047  min_lr: 0.000001  loss: 3.5589 (3.4712)  loss_scale: 2048.0000 (762.9864)  weight_decay: 0.0500 (0.0500)  time: 0.2828  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1260/40201]  eta: 5:21:28  lr: 0.000047  min_lr: 0.000001  loss: 3.1399 (3.4683)  loss_scale: 2048.0000 (773.1768)  weight_decay: 0.0500 (0.0500)  time: 0.3224  data: 0.1024  max mem: 6186
Epoch: [25]  [ 1270/40201]  eta: 5:19:53  lr: 0.000047  min_lr: 0.000001  loss: 3.1399 (3.4648)  loss_scale: 2048.0000 (783.2069)  weight_decay: 0.0500 (0.0500)  time: 0.2959  data: 0.1091  max mem: 6186
Epoch: [25]  [ 1280/40201]  eta: 5:20:07  lr: 0.000047  min_lr: 0.000001  loss: 3.3344 (3.4662)  loss_scale: 2048.0000 (793.0804)  weight_decay: 0.0500 (0.0500)  time: 0.3791  data: 0.0562  max mem: 6186
[2023-07-25 15:41:37,538] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:41:37,538] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-07-25 15:41:37,554] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:41:37,554] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [25]  [ 1290/40201]  eta: 5:20:12  lr: 0.000047  min_lr: 0.000001  loss: 3.7991 (3.4674)  loss_scale: 2048.0000 (818.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0495  max mem: 6186
Epoch: [25]  [ 1300/40201]  eta: 5:20:16  lr: 0.000047  min_lr: 0.000001  loss: 3.6656 (3.4688)  loss_scale: 4096.0000 (843.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0010  max mem: 6186
Epoch: [25]  [ 1310/40201]  eta: 5:20:20  lr: 0.000047  min_lr: 0.000001  loss: 3.2528 (3.4683)  loss_scale: 4096.0000 (868.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0010  max mem: 6186
Epoch: [25]  [ 1320/40201]  eta: 5:20:17  lr: 0.000047  min_lr: 0.000001  loss: 3.4711 (3.4695)  loss_scale: 4096.0000 (893.0931)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1330/40201]  eta: 5:20:20  lr: 0.000047  min_lr: 0.000001  loss: 3.2288 (3.4682)  loss_scale: 4096.0000 (917.1570)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0103  max mem: 6186
Epoch: [25]  [ 1340/40201]  eta: 5:20:17  lr: 0.000047  min_lr: 0.000001  loss: 3.1098 (3.4676)  loss_scale: 4096.0000 (940.8620)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0102  max mem: 6186
Epoch: [25]  [ 1350/40201]  eta: 5:20:17  lr: 0.000047  min_lr: 0.000001  loss: 3.5166 (3.4679)  loss_scale: 4096.0000 (964.2161)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1360/40201]  eta: 5:20:15  lr: 0.000047  min_lr: 0.000001  loss: 3.5503 (3.4680)  loss_scale: 4096.0000 (987.2270)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1370/40201]  eta: 5:20:24  lr: 0.000047  min_lr: 0.000001  loss: 3.6424 (3.4720)  loss_scale: 4096.0000 (1009.9023)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1380/40201]  eta: 5:20:22  lr: 0.000047  min_lr: 0.000001  loss: 3.7913 (3.4716)  loss_scale: 4096.0000 (1032.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1390/40201]  eta: 5:20:24  lr: 0.000047  min_lr: 0.000001  loss: 3.3665 (3.4735)  loss_scale: 4096.0000 (1054.2746)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1400/40201]  eta: 5:20:21  lr: 0.000047  min_lr: 0.000001  loss: 3.3916 (3.4723)  loss_scale: 4096.0000 (1075.9857)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1410/40201]  eta: 5:20:21  lr: 0.000047  min_lr: 0.000001  loss: 3.5732 (3.4741)  loss_scale: 4096.0000 (1097.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1420/40201]  eta: 5:20:19  lr: 0.000047  min_lr: 0.000001  loss: 3.3476 (3.4709)  loss_scale: 4096.0000 (1118.4912)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1430/40201]  eta: 5:20:21  lr: 0.000047  min_lr: 0.000001  loss: 3.2822 (3.4707)  loss_scale: 4096.0000 (1139.2984)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1440/40201]  eta: 5:20:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5078 (3.4696)  loss_scale: 4096.0000 (1159.8168)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1450/40201]  eta: 5:19:43  lr: 0.000047  min_lr: 0.000001  loss: 3.3844 (3.4690)  loss_scale: 4096.0000 (1180.0524)  weight_decay: 0.0500 (0.0500)  time: 0.4432  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1460/40201]  eta: 5:18:16  lr: 0.000047  min_lr: 0.000001  loss: 3.4352 (3.4688)  loss_scale: 4096.0000 (1200.0110)  weight_decay: 0.0500 (0.0500)  time: 0.2835  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1470/40201]  eta: 5:18:35  lr: 0.000047  min_lr: 0.000001  loss: 3.2601 (3.4675)  loss_scale: 4096.0000 (1219.6982)  weight_decay: 0.0500 (0.0500)  time: 0.3852  data: 0.1959  max mem: 6186
Epoch: [25]  [ 1480/40201]  eta: 5:17:23  lr: 0.000047  min_lr: 0.000001  loss: 2.9902 (3.4675)  loss_scale: 4096.0000 (1239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.4116  data: 0.2191  max mem: 6186
Epoch: [25]  [ 1490/40201]  eta: 5:17:01  lr: 0.000047  min_lr: 0.000001  loss: 2.9613 (3.4659)  loss_scale: 4096.0000 (1258.2803)  weight_decay: 0.0500 (0.0500)  time: 0.3302  data: 0.1257  max mem: 6186
Epoch: [25]  [ 1500/40201]  eta: 5:16:06  lr: 0.000047  min_lr: 0.000001  loss: 2.8333 (3.4617)  loss_scale: 4096.0000 (1277.1859)  weight_decay: 0.0500 (0.0500)  time: 0.3602  data: 0.1560  max mem: 6186
Epoch: [25]  [ 1510/40201]  eta: 5:15:33  lr: 0.000047  min_lr: 0.000001  loss: 3.0183 (3.4627)  loss_scale: 4096.0000 (1295.8412)  weight_decay: 0.0500 (0.0500)  time: 0.3396  data: 0.1057  max mem: 6186
Epoch: [25]  [ 1520/40201]  eta: 5:15:40  lr: 0.000047  min_lr: 0.000001  loss: 3.7140 (3.4637)  loss_scale: 4096.0000 (1314.2512)  weight_decay: 0.0500 (0.0500)  time: 0.4592  data: 0.0569  max mem: 6186
Epoch: [25]  [ 1530/40201]  eta: 5:15:42  lr: 0.000047  min_lr: 0.000001  loss: 3.3220 (3.4618)  loss_scale: 4096.0000 (1332.4206)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0052  max mem: 6186
[2023-07-25 15:43:38,469] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:43:38,469] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-07-25 15:43:38,488] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:43:38,488] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [25]  [ 1540/40201]  eta: 5:15:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1454 (3.4606)  loss_scale: 4096.0000 (1360.9864)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1550/40201]  eta: 5:15:44  lr: 0.000047  min_lr: 0.000001  loss: 3.4598 (3.4606)  loss_scale: 8192.0000 (1405.0290)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1560/40201]  eta: 5:15:45  lr: 0.000047  min_lr: 0.000001  loss: 3.2298 (3.4592)  loss_scale: 8192.0000 (1448.5074)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1570/40201]  eta: 5:15:44  lr: 0.000047  min_lr: 0.000001  loss: 3.1542 (3.4584)  loss_scale: 8192.0000 (1491.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1580/40201]  eta: 5:15:43  lr: 0.000047  min_lr: 0.000001  loss: 3.1542 (3.4574)  loss_scale: 8192.0000 (1533.8140)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1590/40201]  eta: 5:14:47  lr: 0.000047  min_lr: 0.000001  loss: 3.2965 (3.4578)  loss_scale: 8192.0000 (1575.6631)  weight_decay: 0.0500 (0.0500)  time: 0.3934  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1600/40201]  eta: 5:13:31  lr: 0.000047  min_lr: 0.000001  loss: 3.3803 (3.4588)  loss_scale: 8192.0000 (1616.9894)  weight_decay: 0.0500 (0.0500)  time: 0.2377  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1610/40201]  eta: 5:12:50  lr: 0.000047  min_lr: 0.000001  loss: 3.6072 (3.4605)  loss_scale: 8192.0000 (1657.8026)  weight_decay: 0.0500 (0.0500)  time: 0.2642  data: 0.0287  max mem: 6186
Epoch: [25]  [ 1620/40201]  eta: 5:12:07  lr: 0.000047  min_lr: 0.000001  loss: 3.7816 (3.4616)  loss_scale: 8192.0000 (1698.1123)  weight_decay: 0.0500 (0.0500)  time: 0.3295  data: 0.0416  max mem: 6186
Epoch: [25]  [ 1630/40201]  eta: 5:11:40  lr: 0.000047  min_lr: 0.000001  loss: 3.2745 (3.4606)  loss_scale: 8192.0000 (1737.9277)  weight_decay: 0.0500 (0.0500)  time: 0.3598  data: 0.0682  max mem: 6186
Epoch: [25]  [ 1640/40201]  eta: 5:11:48  lr: 0.000047  min_lr: 0.000001  loss: 3.5156 (3.4646)  loss_scale: 8192.0000 (1777.2578)  weight_decay: 0.0500 (0.0500)  time: 0.4659  data: 0.1639  max mem: 6186
Epoch: [25]  [ 1650/40201]  eta: 5:11:15  lr: 0.000047  min_lr: 0.000001  loss: 3.5224 (3.4623)  loss_scale: 8192.0000 (1816.1114)  weight_decay: 0.0500 (0.0500)  time: 0.4507  data: 0.1091  max mem: 6186
Epoch: [25]  [ 1660/40201]  eta: 5:11:17  lr: 0.000047  min_lr: 0.000001  loss: 2.9026 (3.4611)  loss_scale: 8192.0000 (1854.4973)  weight_decay: 0.0500 (0.0500)  time: 0.4407  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1670/40201]  eta: 5:11:46  lr: 0.000047  min_lr: 0.000001  loss: 3.2027 (3.4596)  loss_scale: 8192.0000 (1892.4237)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0004  max mem: 6186
Epoch: [25]  [ 1680/40201]  eta: 5:12:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2027 (3.4571)  loss_scale: 8192.0000 (1929.8989)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0005  max mem: 6186
Epoch: [25]  [ 1690/40201]  eta: 5:12:54  lr: 0.000047  min_lr: 0.000001  loss: 3.2940 (3.4601)  loss_scale: 8192.0000 (1966.9308)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1700/40201]  eta: 5:14:18  lr: 0.000047  min_lr: 0.000001  loss: 3.6474 (3.4615)  loss_scale: 8192.0000 (2003.5273)  weight_decay: 0.0500 (0.0500)  time: 0.7743  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1710/40201]  eta: 5:14:48  lr: 0.000047  min_lr: 0.000001  loss: 3.3300 (3.4606)  loss_scale: 8192.0000 (2039.6961)  weight_decay: 0.0500 (0.0500)  time: 0.7625  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1720/40201]  eta: 5:15:34  lr: 0.000047  min_lr: 0.000001  loss: 3.0168 (3.4566)  loss_scale: 8192.0000 (2075.4445)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1730/40201]  eta: 5:15:57  lr: 0.000047  min_lr: 0.000001  loss: 3.4192 (3.4587)  loss_scale: 8192.0000 (2110.7799)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0009  max mem: 6186
Epoch: [25]  [ 1740/40201]  eta: 5:16:40  lr: 0.000047  min_lr: 0.000001  loss: 3.5931 (3.4585)  loss_scale: 8192.0000 (2145.7094)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1750/40201]  eta: 5:17:05  lr: 0.000047  min_lr: 0.000001  loss: 3.6866 (3.4608)  loss_scale: 8192.0000 (2180.2399)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.0003  max mem: 6186
Epoch: [25]  [ 1760/40201]  eta: 5:17:47  lr: 0.000047  min_lr: 0.000001  loss: 3.6866 (3.4625)  loss_scale: 8192.0000 (2214.3782)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1770/40201]  eta: 5:18:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5594 (3.4646)  loss_scale: 8192.0000 (2248.1310)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1780/40201]  eta: 5:18:29  lr: 0.000047  min_lr: 0.000001  loss: 3.5594 (3.4643)  loss_scale: 8192.0000 (2281.5048)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0018  max mem: 6186
Epoch: [25]  [ 1790/40201]  eta: 5:18:56  lr: 0.000047  min_lr: 0.000001  loss: 3.1284 (3.4644)  loss_scale: 8192.0000 (2314.5059)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0026  max mem: 6186
[2023-07-25 15:45:58,865] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:45:58,865] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-07-25 15:45:58,867] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:45:58,867] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [25]  [ 1800/40201]  eta: 5:21:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1284 (3.4635)  loss_scale: 8192.0000 (2383.5292)  weight_decay: 0.0500 (0.0500)  time: 0.9803  data: 0.3273  max mem: 6186
Epoch: [25]  [ 1810/40201]  eta: 5:22:24  lr: 0.000047  min_lr: 0.000001  loss: 3.4742 (3.4639)  loss_scale: 16384.0000 (2460.8371)  weight_decay: 0.0500 (0.0500)  time: 1.0132  data: 0.3265  max mem: 6186
Epoch: [25]  [ 1820/40201]  eta: 5:22:52  lr: 0.000047  min_lr: 0.000001  loss: 3.4534 (3.4629)  loss_scale: 16384.0000 (2537.2960)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1830/40201]  eta: 5:23:07  lr: 0.000047  min_lr: 0.000001  loss: 3.3879 (3.4641)  loss_scale: 16384.0000 (2612.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1840/40201]  eta: 5:23:31  lr: 0.000047  min_lr: 0.000001  loss: 3.3699 (3.4634)  loss_scale: 16384.0000 (2687.7219)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1850/40201]  eta: 5:23:57  lr: 0.000047  min_lr: 0.000001  loss: 3.3077 (3.4625)  loss_scale: 16384.0000 (2761.7158)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.0006  max mem: 6186
Epoch: [25]  [ 1860/40201]  eta: 5:24:12  lr: 0.000047  min_lr: 0.000001  loss: 3.3739 (3.4631)  loss_scale: 16384.0000 (2834.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1870/40201]  eta: 5:23:44  lr: 0.000047  min_lr: 0.000001  loss: 3.3545 (3.4618)  loss_scale: 16384.0000 (2907.3308)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1880/40201]  eta: 5:23:00  lr: 0.000047  min_lr: 0.000001  loss: 3.4669 (3.4652)  loss_scale: 16384.0000 (2978.9771)  weight_decay: 0.0500 (0.0500)  time: 0.3559  data: 0.0008  max mem: 6186
Epoch: [25]  [ 1890/40201]  eta: 5:23:53  lr: 0.000047  min_lr: 0.000001  loss: 3.4302 (3.4649)  loss_scale: 16384.0000 (3049.8657)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.2573  max mem: 6186
Epoch: [25]  [ 1900/40201]  eta: 5:23:32  lr: 0.000047  min_lr: 0.000001  loss: 3.2194 (3.4640)  loss_scale: 16384.0000 (3120.0084)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.3189  max mem: 6186
Epoch: [25]  [ 1910/40201]  eta: 5:22:52  lr: 0.000047  min_lr: 0.000001  loss: 3.2938 (3.4638)  loss_scale: 16384.0000 (3189.4171)  weight_decay: 0.0500 (0.0500)  time: 0.3811  data: 0.0901  max mem: 6186
Epoch: [25]  [ 1920/40201]  eta: 5:22:34  lr: 0.000047  min_lr: 0.000001  loss: 3.0801 (3.4619)  loss_scale: 16384.0000 (3258.1031)  weight_decay: 0.0500 (0.0500)  time: 0.3874  data: 0.0766  max mem: 6186
Epoch: [25]  [ 1930/40201]  eta: 5:24:10  lr: 0.000047  min_lr: 0.000001  loss: 3.0995 (3.4620)  loss_scale: 16384.0000 (3326.0777)  weight_decay: 0.0500 (0.0500)  time: 0.7284  data: 0.2332  max mem: 6186
Epoch: [25]  [ 1940/40201]  eta: 5:24:39  lr: 0.000047  min_lr: 0.000001  loss: 3.2825 (3.4613)  loss_scale: 16384.0000 (3393.3519)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.1847  max mem: 6186
Epoch: [25]  [ 1950/40201]  eta: 5:25:12  lr: 0.000047  min_lr: 0.000001  loss: 3.3123 (3.4621)  loss_scale: 16384.0000 (3459.9364)  weight_decay: 0.0500 (0.0500)  time: 0.6916  data: 0.0019  max mem: 6186
Epoch: [25]  [ 1960/40201]  eta: 5:25:41  lr: 0.000047  min_lr: 0.000001  loss: 3.3045 (3.4598)  loss_scale: 16384.0000 (3525.8419)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.0021  max mem: 6186
Epoch: [25]  [ 1970/40201]  eta: 5:25:52  lr: 0.000047  min_lr: 0.000001  loss: 2.7757 (3.4593)  loss_scale: 16384.0000 (3591.0786)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0011  max mem: 6186
Epoch: [25]  [ 1980/40201]  eta: 5:26:40  lr: 0.000047  min_lr: 0.000001  loss: 3.6429 (3.4590)  loss_scale: 16384.0000 (3655.6567)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.0007  max mem: 6186
Epoch: [25]  [ 1990/40201]  eta: 5:27:12  lr: 0.000047  min_lr: 0.000001  loss: 2.7780 (3.4579)  loss_scale: 16384.0000 (3719.5861)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.0006  max mem: 6186
[2023-07-25 15:48:14,596] [INFO] [logging.py:69:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1136217508465052e-06, 1.1136217508465052e-06, 1.4848290011286736e-06, 1.4848290011286736e-06, 1.979772001504898e-06, 1.979772001504898e-06, 2.6396960020065306e-06, 2.6396960020065306e-06, 3.519594669342041e-06, 3.519594669342041e-06, 4.692792892456055e-06, 4.692792892456055e-06, 6.257057189941406e-06, 6.257057189941406e-06, 8.342742919921874e-06, 8.342742919921874e-06, 1.11236572265625e-05, 1.11236572265625e-05, 1.483154296875e-05, 1.483154296875e-05, 1.9775390625e-05, 1.9775390625e-05, 2.63671875e-05, 2.63671875e-05, 3.5156250000000004e-05, 3.5156250000000004e-05, 4.6875e-05, 4.6875e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-25 15:48:14,597] [INFO] [timer.py:181:stop] 0/2000, SamplesPerSec=13.733150064236343
Epoch: [25]  [ 2000/40201]  eta: 5:27:47  lr: 0.000047  min_lr: 0.000001  loss: 2.9768 (3.4580)  loss_scale: 16384.0000 (3782.8766)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2010/40201]  eta: 5:28:13  lr: 0.000047  min_lr: 0.000001  loss: 3.6585 (3.4598)  loss_scale: 16384.0000 (3845.5375)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2020/40201]  eta: 5:28:36  lr: 0.000047  min_lr: 0.000001  loss: 3.5895 (3.4603)  loss_scale: 16384.0000 (3907.5784)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2030/40201]  eta: 5:28:57  lr: 0.000047  min_lr: 0.000001  loss: 3.4211 (3.4618)  loss_scale: 16384.0000 (3969.0084)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2040/40201]  eta: 5:29:35  lr: 0.000047  min_lr: 0.000001  loss: 3.7406 (3.4639)  loss_scale: 16384.0000 (4029.8364)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.0004  max mem: 6186
[2023-07-25 15:48:48,374] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:48:48,375] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-07-25 15:48:48,389] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:48:48,389] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [25]  [ 2050/40201]  eta: 5:29:52  lr: 0.000047  min_lr: 0.000001  loss: 3.5080 (3.4626)  loss_scale: 16384.0000 (4106.0478)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2060/40201]  eta: 5:31:02  lr: 0.000047  min_lr: 0.000001  loss: 3.0909 (3.4626)  loss_scale: 32768.0000 (4245.1160)  weight_decay: 0.0500 (0.0500)  time: 0.7832  data: 0.1302  max mem: 6186
Epoch: [25]  [ 2070/40201]  eta: 5:31:05  lr: 0.000047  min_lr: 0.000001  loss: 3.2652 (3.4612)  loss_scale: 32768.0000 (4382.8411)  weight_decay: 0.0500 (0.0500)  time: 0.7443  data: 0.1306  max mem: 6186
Epoch: [25]  [ 2080/40201]  eta: 5:31:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2201 (3.4600)  loss_scale: 32768.0000 (4519.2427)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2090/40201]  eta: 5:31:28  lr: 0.000047  min_lr: 0.000001  loss: 3.2087 (3.4601)  loss_scale: 32768.0000 (4654.3396)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2100/40201]  eta: 5:32:20  lr: 0.000047  min_lr: 0.000001  loss: 3.0741 (3.4573)  loss_scale: 32768.0000 (4788.1504)  weight_decay: 0.0500 (0.0500)  time: 0.7248  data: 0.0020  max mem: 6186
Epoch: [25]  [ 2110/40201]  eta: 5:32:32  lr: 0.000047  min_lr: 0.000001  loss: 3.3836 (3.4600)  loss_scale: 32768.0000 (4920.6935)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.0021  max mem: 6186
Epoch: [25]  [ 2120/40201]  eta: 5:32:50  lr: 0.000047  min_lr: 0.000001  loss: 3.6320 (3.4591)  loss_scale: 32768.0000 (5051.9868)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0021  max mem: 6186
Epoch: [25]  [ 2130/40201]  eta: 5:33:03  lr: 0.000047  min_lr: 0.000001  loss: 3.3296 (3.4587)  loss_scale: 32768.0000 (5182.0479)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2140/40201]  eta: 5:33:18  lr: 0.000047  min_lr: 0.000001  loss: 3.4296 (3.4596)  loss_scale: 32768.0000 (5310.8940)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0004  max mem: 6186
Epoch: [25]  [ 2150/40201]  eta: 5:33:49  lr: 0.000047  min_lr: 0.000001  loss: 3.4296 (3.4617)  loss_scale: 32768.0000 (5438.5421)  weight_decay: 0.0500 (0.0500)  time: 0.6868  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2160/40201]  eta: 5:34:03  lr: 0.000047  min_lr: 0.000001  loss: 3.5823 (3.4620)  loss_scale: 32768.0000 (5565.0088)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2170/40201]  eta: 5:34:17  lr: 0.000047  min_lr: 0.000001  loss: 3.9161 (3.4646)  loss_scale: 32768.0000 (5690.3105)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2180/40201]  eta: 5:34:34  lr: 0.000047  min_lr: 0.000001  loss: 3.3226 (3.4652)  loss_scale: 32768.0000 (5814.4631)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.0014  max mem: 6186
Epoch: [25]  [ 2190/40201]  eta: 5:34:56  lr: 0.000047  min_lr: 0.000001  loss: 3.2125 (3.4660)  loss_scale: 32768.0000 (5937.4824)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2200/40201]  eta: 5:35:12  lr: 0.000047  min_lr: 0.000001  loss: 3.5011 (3.4677)  loss_scale: 32768.0000 (6059.3839)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2210/40201]  eta: 5:34:32  lr: 0.000047  min_lr: 0.000001  loss: 3.5011 (3.4663)  loss_scale: 32768.0000 (6180.1827)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0014  max mem: 6186
Epoch: [25]  [ 2220/40201]  eta: 5:33:51  lr: 0.000047  min_lr: 0.000001  loss: 3.3654 (3.4676)  loss_scale: 32768.0000 (6299.8937)  weight_decay: 0.0500 (0.0500)  time: 0.3240  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2230/40201]  eta: 5:33:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2290 (3.4655)  loss_scale: 32768.0000 (6418.5316)  weight_decay: 0.0500 (0.0500)  time: 0.3380  data: 0.0151  max mem: 6186
Epoch: [25]  [ 2240/40201]  eta: 5:32:55  lr: 0.000047  min_lr: 0.000001  loss: 2.9544 (3.4644)  loss_scale: 32768.0000 (6536.1107)  weight_decay: 0.0500 (0.0500)  time: 0.3940  data: 0.0200  max mem: 6186
Epoch: [25]  [ 2250/40201]  eta: 5:32:29  lr: 0.000047  min_lr: 0.000001  loss: 3.5285 (3.4650)  loss_scale: 32768.0000 (6652.6450)  weight_decay: 0.0500 (0.0500)  time: 0.4175  data: 0.0150  max mem: 6186
Epoch: [25]  [ 2260/40201]  eta: 5:32:05  lr: 0.000047  min_lr: 0.000001  loss: 3.4314 (3.4649)  loss_scale: 32768.0000 (6768.1486)  weight_decay: 0.0500 (0.0500)  time: 0.4074  data: 0.0108  max mem: 6186
Epoch: [25]  [ 2270/40201]  eta: 5:31:35  lr: 0.000047  min_lr: 0.000001  loss: 3.2480 (3.4654)  loss_scale: 32768.0000 (6882.6350)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2280/40201]  eta: 5:31:42  lr: 0.000047  min_lr: 0.000001  loss: 3.6419 (3.4666)  loss_scale: 32768.0000 (6996.1175)  weight_decay: 0.0500 (0.0500)  time: 0.4882  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2290/40201]  eta: 5:31:45  lr: 0.000047  min_lr: 0.000001  loss: 3.4006 (3.4647)  loss_scale: 32768.0000 (7108.6093)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2300/40201]  eta: 5:31:57  lr: 0.000047  min_lr: 0.000001  loss: 3.5337 (3.4661)  loss_scale: 32768.0000 (7220.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0005  max mem: 6186
[2023-07-25 15:51:19,153] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:51:19,153] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-07-25 15:51:19,155] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:51:19,155] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [25]  [ 2310/40201]  eta: 5:32:52  lr: 0.000047  min_lr: 0.000001  loss: 3.5845 (3.4641)  loss_scale: 32768.0000 (7415.7473)  weight_decay: 0.0500 (0.0500)  time: 0.7614  data: 0.0014  max mem: 6186
Epoch: [25]  [ 2320/40201]  eta: 5:33:14  lr: 0.000047  min_lr: 0.000001  loss: 3.3012 (3.4654)  loss_scale: 65536.0000 (7666.1577)  weight_decay: 0.0500 (0.0500)  time: 0.7909  data: 0.0017  max mem: 6186
Epoch: [25]  [ 2330/40201]  eta: 5:33:29  lr: 0.000047  min_lr: 0.000001  loss: 3.7401 (3.4667)  loss_scale: 65536.0000 (7914.4196)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2340/40201]  eta: 5:33:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1213 (3.4648)  loss_scale: 65536.0000 (8160.5604)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.0014  max mem: 6186
Epoch: [25]  [ 2350/40201]  eta: 5:33:52  lr: 0.000047  min_lr: 0.000001  loss: 3.0536 (3.4644)  loss_scale: 65536.0000 (8404.6074)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.0013  max mem: 6186
[2023-07-25 15:51:50,706] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1176
[2023-07-25 15:51:50,706] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-07-25 15:51:50,706] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-07-25 15:51:50,711] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1176
[2023-07-25 15:51:50,711] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
Epoch: [25]  [ 2360/40201]  eta: 5:34:05  lr: 0.000047  min_lr: 0.000001  loss: 3.4590 (3.4643)  loss_scale: 65536.0000 (8535.5561)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2370/40201]  eta: 5:34:28  lr: 0.000047  min_lr: 0.000001  loss: 3.5698 (3.4653)  loss_scale: 32768.0000 (8637.7596)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2380/40201]  eta: 5:34:32  lr: 0.000047  min_lr: 0.000001  loss: 3.7706 (3.4663)  loss_scale: 32768.0000 (8739.1046)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2390/40201]  eta: 5:34:32  lr: 0.000047  min_lr: 0.000001  loss: 3.5921 (3.4677)  loss_scale: 32768.0000 (8839.6018)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2400/40201]  eta: 5:35:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5826 (3.4686)  loss_scale: 32768.0000 (8939.2620)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.1279  max mem: 6186
Epoch: [25]  [ 2410/40201]  eta: 5:35:32  lr: 0.000047  min_lr: 0.000001  loss: 3.6765 (3.4704)  loss_scale: 32768.0000 (9038.0954)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.1277  max mem: 6186
Epoch: [25]  [ 2420/40201]  eta: 5:35:47  lr: 0.000047  min_lr: 0.000001  loss: 3.6665 (3.4697)  loss_scale: 32768.0000 (9136.1124)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2430/40201]  eta: 5:35:44  lr: 0.000047  min_lr: 0.000001  loss: 3.6089 (3.4696)  loss_scale: 32768.0000 (9233.3229)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2440/40201]  eta: 5:36:02  lr: 0.000047  min_lr: 0.000001  loss: 3.6575 (3.4705)  loss_scale: 32768.0000 (9329.7370)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2450/40201]  eta: 5:36:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5053 (3.4720)  loss_scale: 32768.0000 (9425.3643)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2460/40201]  eta: 5:35:57  lr: 0.000047  min_lr: 0.000001  loss: 3.3874 (3.4714)  loss_scale: 32768.0000 (9520.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0004  max mem: 6186
Epoch: [25]  [ 2470/40201]  eta: 5:36:16  lr: 0.000047  min_lr: 0.000001  loss: 3.1435 (3.4703)  loss_scale: 32768.0000 (9614.2970)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.2094  max mem: 6186
Epoch: [25]  [ 2480/40201]  eta: 5:35:55  lr: 0.000047  min_lr: 0.000001  loss: 2.8588 (3.4696)  loss_scale: 32768.0000 (9707.6211)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.2438  max mem: 6186
Epoch: [25]  [ 2490/40201]  eta: 5:35:14  lr: 0.000047  min_lr: 0.000001  loss: 3.1551 (3.4686)  loss_scale: 32768.0000 (9800.1959)  weight_decay: 0.0500 (0.0500)  time: 0.3659  data: 0.0348  max mem: 6186
Epoch: [25]  [ 2500/40201]  eta: 5:34:38  lr: 0.000047  min_lr: 0.000001  loss: 3.3306 (3.4692)  loss_scale: 32768.0000 (9892.0304)  weight_decay: 0.0500 (0.0500)  time: 0.3161  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2510/40201]  eta: 5:34:12  lr: 0.000047  min_lr: 0.000001  loss: 3.8727 (3.4702)  loss_scale: 32768.0000 (9983.1334)  weight_decay: 0.0500 (0.0500)  time: 0.3615  data: 0.0461  max mem: 6186
Epoch: [25]  [ 2520/40201]  eta: 5:34:20  lr: 0.000047  min_lr: 0.000001  loss: 3.6690 (3.4719)  loss_scale: 32768.0000 (10073.5137)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.1059  max mem: 6186
Epoch: [25]  [ 2530/40201]  eta: 5:34:38  lr: 0.000047  min_lr: 0.000001  loss: 3.4382 (3.4724)  loss_scale: 32768.0000 (10163.1798)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.0608  max mem: 6186
Epoch: [25]  [ 2540/40201]  eta: 5:35:00  lr: 0.000047  min_lr: 0.000001  loss: 3.1811 (3.4708)  loss_scale: 32768.0000 (10252.1401)  weight_decay: 0.0500 (0.0500)  time: 0.7012  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2550/40201]  eta: 5:35:25  lr: 0.000047  min_lr: 0.000001  loss: 3.2437 (3.4703)  loss_scale: 32768.0000 (10340.4030)  weight_decay: 0.0500 (0.0500)  time: 0.7289  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2560/40201]  eta: 5:35:43  lr: 0.000047  min_lr: 0.000001  loss: 3.4393 (3.4709)  loss_scale: 32768.0000 (10427.9766)  weight_decay: 0.0500 (0.0500)  time: 0.7153  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2570/40201]  eta: 5:36:04  lr: 0.000047  min_lr: 0.000001  loss: 3.5605 (3.4710)  loss_scale: 32768.0000 (10514.8689)  weight_decay: 0.0500 (0.0500)  time: 0.7034  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2580/40201]  eta: 5:36:23  lr: 0.000047  min_lr: 0.000001  loss: 3.6716 (3.4725)  loss_scale: 32768.0000 (10601.0880)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2590/40201]  eta: 5:36:36  lr: 0.000047  min_lr: 0.000001  loss: 3.6826 (3.4733)  loss_scale: 32768.0000 (10686.6415)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.0010  max mem: 6186
Epoch: [25]  [ 2600/40201]  eta: 5:36:01  lr: 0.000047  min_lr: 0.000001  loss: 3.4148 (3.4735)  loss_scale: 32768.0000 (10771.5371)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0016  max mem: 6186
Epoch: [25]  [ 2610/40201]  eta: 5:35:27  lr: 0.000047  min_lr: 0.000001  loss: 3.5593 (3.4736)  loss_scale: 32768.0000 (10855.7825)  weight_decay: 0.0500 (0.0500)  time: 0.3350  data: 0.0012  max mem: 6186
[2023-07-25 15:54:23,213] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:54:23,213] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 15:54:23,215] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:54:23,215] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 2620/40201]  eta: 5:35:04  lr: 0.000047  min_lr: 0.000001  loss: 3.3668 (3.4740)  loss_scale: 32768.0000 (11064.4060)  weight_decay: 0.0500 (0.0500)  time: 0.3775  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2630/40201]  eta: 5:34:45  lr: 0.000047  min_lr: 0.000001  loss: 3.2203 (3.4733)  loss_scale: 65536.0000 (11271.4436)  weight_decay: 0.0500 (0.0500)  time: 0.4239  data: 0.0020  max mem: 6186
Epoch: [25]  [ 2640/40201]  eta: 5:34:27  lr: 0.000047  min_lr: 0.000001  loss: 3.4438 (3.4739)  loss_scale: 65536.0000 (11476.9133)  weight_decay: 0.0500 (0.0500)  time: 0.4412  data: 0.0017  max mem: 6186
Epoch: [25]  [ 2650/40201]  eta: 5:34:08  lr: 0.000047  min_lr: 0.000001  loss: 3.5936 (3.4740)  loss_scale: 65536.0000 (11680.8329)  weight_decay: 0.0500 (0.0500)  time: 0.4439  data: 0.0292  max mem: 6186
Epoch: [25]  [ 2660/40201]  eta: 5:34:12  lr: 0.000047  min_lr: 0.000001  loss: 3.4214 (3.4755)  loss_scale: 65536.0000 (11883.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0292  max mem: 6186
Epoch: [25]  [ 2670/40201]  eta: 5:34:25  lr: 0.000047  min_lr: 0.000001  loss: 3.7644 (3.4758)  loss_scale: 65536.0000 (12084.0914)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.0015  max mem: 6186
[2023-07-25 15:54:56,223] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1337
[2023-07-25 15:54:56,223] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 15:54:56,229] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1337
[2023-07-25 15:54:56,229] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 15:54:56,229] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 2680/40201]  eta: 5:34:40  lr: 0.000047  min_lr: 0.000001  loss: 3.5814 (3.4769)  loss_scale: 65536.0000 (12210.1305)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2690/40201]  eta: 5:34:59  lr: 0.000047  min_lr: 0.000001  loss: 3.4152 (3.4778)  loss_scale: 32768.0000 (12286.5255)  weight_decay: 0.0500 (0.0500)  time: 0.6966  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2700/40201]  eta: 5:35:15  lr: 0.000047  min_lr: 0.000001  loss: 3.2622 (3.4780)  loss_scale: 32768.0000 (12362.3547)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.0015  max mem: 6186
Epoch: [25]  [ 2710/40201]  eta: 5:35:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2041 (3.4776)  loss_scale: 32768.0000 (12437.6245)  weight_decay: 0.0500 (0.0500)  time: 0.7133  data: 0.0013  max mem: 6186
Epoch: [25]  [ 2720/40201]  eta: 5:36:02  lr: 0.000047  min_lr: 0.000001  loss: 3.5296 (3.4784)  loss_scale: 32768.0000 (12512.3411)  weight_decay: 0.0500 (0.0500)  time: 0.7455  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2730/40201]  eta: 5:36:18  lr: 0.000047  min_lr: 0.000001  loss: 3.6691 (3.4785)  loss_scale: 32768.0000 (12586.5104)  weight_decay: 0.0500 (0.0500)  time: 0.7242  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2740/40201]  eta: 5:36:42  lr: 0.000047  min_lr: 0.000001  loss: 3.4477 (3.4782)  loss_scale: 32768.0000 (12660.1386)  weight_decay: 0.0500 (0.0500)  time: 0.7216  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2750/40201]  eta: 5:37:08  lr: 0.000047  min_lr: 0.000001  loss: 3.3624 (3.4785)  loss_scale: 32768.0000 (12733.2316)  weight_decay: 0.0500 (0.0500)  time: 0.7599  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2760/40201]  eta: 5:37:33  lr: 0.000047  min_lr: 0.000001  loss: 3.4544 (3.4785)  loss_scale: 32768.0000 (12805.7950)  weight_decay: 0.0500 (0.0500)  time: 0.7692  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2770/40201]  eta: 5:37:55  lr: 0.000047  min_lr: 0.000001  loss: 3.5322 (3.4796)  loss_scale: 32768.0000 (12877.8347)  weight_decay: 0.0500 (0.0500)  time: 0.7537  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2780/40201]  eta: 5:38:15  lr: 0.000047  min_lr: 0.000001  loss: 3.4522 (3.4792)  loss_scale: 32768.0000 (12949.3563)  weight_decay: 0.0500 (0.0500)  time: 0.7368  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2790/40201]  eta: 5:38:38  lr: 0.000047  min_lr: 0.000001  loss: 3.4522 (3.4796)  loss_scale: 32768.0000 (13020.3655)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2800/40201]  eta: 5:38:59  lr: 0.000047  min_lr: 0.000001  loss: 3.2219 (3.4788)  loss_scale: 32768.0000 (13090.8675)  weight_decay: 0.0500 (0.0500)  time: 0.7469  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2810/40201]  eta: 5:39:22  lr: 0.000047  min_lr: 0.000001  loss: 3.5007 (3.4803)  loss_scale: 32768.0000 (13160.8680)  weight_decay: 0.0500 (0.0500)  time: 0.7494  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2820/40201]  eta: 5:39:43  lr: 0.000047  min_lr: 0.000001  loss: 3.5007 (3.4790)  loss_scale: 32768.0000 (13230.3722)  weight_decay: 0.0500 (0.0500)  time: 0.7532  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2830/40201]  eta: 5:40:02  lr: 0.000047  min_lr: 0.000001  loss: 3.2717 (3.4789)  loss_scale: 32768.0000 (13299.3854)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2840/40201]  eta: 5:40:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5686 (3.4785)  loss_scale: 32768.0000 (13367.9127)  weight_decay: 0.0500 (0.0500)  time: 0.7192  data: 0.0009  max mem: 6186
Epoch: [25]  [ 2850/40201]  eta: 5:40:45  lr: 0.000047  min_lr: 0.000001  loss: 3.5786 (3.4782)  loss_scale: 32768.0000 (13435.9593)  weight_decay: 0.0500 (0.0500)  time: 0.7518  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2860/40201]  eta: 5:41:34  lr: 0.000047  min_lr: 0.000001  loss: 2.9904 (3.4767)  loss_scale: 32768.0000 (13503.5302)  weight_decay: 0.0500 (0.0500)  time: 0.8774  data: 0.0003  max mem: 6186
Epoch: [25]  [ 2870/40201]  eta: 5:41:50  lr: 0.000047  min_lr: 0.000001  loss: 2.7950 (3.4756)  loss_scale: 32768.0000 (13570.6304)  weight_decay: 0.0500 (0.0500)  time: 0.8381  data: 0.0003  max mem: 6186
Epoch: [25]  [ 2880/40201]  eta: 5:42:08  lr: 0.000047  min_lr: 0.000001  loss: 3.3030 (3.4756)  loss_scale: 32768.0000 (13637.2648)  weight_decay: 0.0500 (0.0500)  time: 0.7218  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2890/40201]  eta: 5:42:31  lr: 0.000047  min_lr: 0.000001  loss: 3.5281 (3.4754)  loss_scale: 32768.0000 (13703.4383)  weight_decay: 0.0500 (0.0500)  time: 0.7491  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2900/40201]  eta: 5:42:59  lr: 0.000047  min_lr: 0.000001  loss: 3.4609 (3.4757)  loss_scale: 32768.0000 (13769.1555)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0008  max mem: 6186
Epoch: [25]  [ 2910/40201]  eta: 5:43:29  lr: 0.000047  min_lr: 0.000001  loss: 3.2867 (3.4750)  loss_scale: 32768.0000 (13834.4212)  weight_decay: 0.0500 (0.0500)  time: 0.8216  data: 0.0011  max mem: 6186
Epoch: [25]  [ 2920/40201]  eta: 5:43:41  lr: 0.000047  min_lr: 0.000001  loss: 3.2656 (3.4746)  loss_scale: 32768.0000 (13899.2400)  weight_decay: 0.0500 (0.0500)  time: 0.7572  data: 0.0012  max mem: 6186
Epoch: [25]  [ 2930/40201]  eta: 5:44:03  lr: 0.000047  min_lr: 0.000001  loss: 3.6428 (3.4753)  loss_scale: 32768.0000 (13963.6165)  weight_decay: 0.0500 (0.0500)  time: 0.7315  data: 0.0010  max mem: 6186
[2023-07-25 15:58:10,351] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:58:10,351] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 15:58:10,381] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 15:58:10,381] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 2940/40201]  eta: 5:44:17  lr: 0.000047  min_lr: 0.000001  loss: 3.4185 (3.4744)  loss_scale: 32768.0000 (14116.6896)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.0006  max mem: 6186
Epoch: [25]  [ 2950/40201]  eta: 5:44:35  lr: 0.000047  min_lr: 0.000001  loss: 3.5602 (3.4754)  loss_scale: 65536.0000 (14290.9332)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.0007  max mem: 6186
video cannot be loaded by decord:  /data/i5O/kinetics400/train/pDPbETciXhw_000167_000177.mp4
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/pDPbETciXhw_000167_000177.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 2960/40201]  eta: 5:44:57  lr: 0.000047  min_lr: 0.000001  loss: 3.9392 (3.4762)  loss_scale: 65536.0000 (14464.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7577  data: 0.0007  max mem: 6186
Epoch: [25]  [ 2970/40201]  eta: 5:45:10  lr: 0.000047  min_lr: 0.000001  loss: 3.9235 (3.4779)  loss_scale: 65536.0000 (14635.9017)  weight_decay: 0.0500 (0.0500)  time: 0.7403  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2980/40201]  eta: 5:45:03  lr: 0.000047  min_lr: 0.000001  loss: 3.1207 (3.4769)  loss_scale: 65536.0000 (14806.6501)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0005  max mem: 6186
Epoch: [25]  [ 2990/40201]  eta: 5:44:31  lr: 0.000047  min_lr: 0.000001  loss: 3.0749 (3.4770)  loss_scale: 65536.0000 (14976.2568)  weight_decay: 0.0500 (0.0500)  time: 0.4437  data: 0.0013  max mem: 6186
[2023-07-25 15:58:54,184] [INFO] [timer.py:181:stop] 0/3000, SamplesPerSec=13.06424665231432
Epoch: [25]  [ 3000/40201]  eta: 5:44:53  lr: 0.000047  min_lr: 0.000001  loss: 3.2517 (3.4764)  loss_scale: 65536.0000 (15144.7331)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.2236  max mem: 6186
Epoch: [25]  [ 3010/40201]  eta: 5:44:29  lr: 0.000047  min_lr: 0.000001  loss: 3.5133 (3.4769)  loss_scale: 65536.0000 (15312.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.2520  max mem: 6186
Epoch: [25]  [ 3020/40201]  eta: 5:44:09  lr: 0.000047  min_lr: 0.000001  loss: 3.5194 (3.4769)  loss_scale: 65536.0000 (15478.3396)  weight_decay: 0.0500 (0.0500)  time: 0.4205  data: 0.0298  max mem: 6186
Epoch: [25]  [ 3030/40201]  eta: 5:43:43  lr: 0.000047  min_lr: 0.000001  loss: 3.5029 (3.4763)  loss_scale: 65536.0000 (15643.4919)  weight_decay: 0.0500 (0.0500)  time: 0.4124  data: 0.0010  max mem: 6186
[2023-07-25 15:59:07,165] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1515
[2023-07-25 15:59:07,165] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 15:59:07,166] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 15:59:07,165] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1515
[2023-07-25 15:59:07,166] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 3040/40201]  eta: 5:44:02  lr: 0.000047  min_lr: 0.000001  loss: 3.7331 (3.4771)  loss_scale: 32768.0000 (15699.8040)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3050/40201]  eta: 5:44:26  lr: 0.000047  min_lr: 0.000001  loss: 3.7483 (3.4782)  loss_scale: 32768.0000 (15755.7470)  weight_decay: 0.0500 (0.0500)  time: 0.7773  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3060/40201]  eta: 5:44:50  lr: 0.000047  min_lr: 0.000001  loss: 3.4170 (3.4776)  loss_scale: 32768.0000 (15811.3244)  weight_decay: 0.0500 (0.0500)  time: 0.7974  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3070/40201]  eta: 5:45:20  lr: 0.000047  min_lr: 0.000001  loss: 3.4170 (3.4786)  loss_scale: 32768.0000 (15866.5399)  weight_decay: 0.0500 (0.0500)  time: 0.8274  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3080/40201]  eta: 5:45:50  lr: 0.000047  min_lr: 0.000001  loss: 3.4198 (3.4776)  loss_scale: 32768.0000 (15921.3969)  weight_decay: 0.0500 (0.0500)  time: 0.8537  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3090/40201]  eta: 5:46:06  lr: 0.000047  min_lr: 0.000001  loss: 2.9145 (3.4770)  loss_scale: 32768.0000 (15975.8991)  weight_decay: 0.0500 (0.0500)  time: 0.7958  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3100/40201]  eta: 5:46:26  lr: 0.000047  min_lr: 0.000001  loss: 3.4803 (3.4773)  loss_scale: 32768.0000 (16030.0497)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3110/40201]  eta: 5:46:37  lr: 0.000047  min_lr: 0.000001  loss: 3.4947 (3.4771)  loss_scale: 32768.0000 (16083.8521)  weight_decay: 0.0500 (0.0500)  time: 0.7377  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3120/40201]  eta: 5:47:00  lr: 0.000047  min_lr: 0.000001  loss: 3.5436 (3.4769)  loss_scale: 32768.0000 (16137.3098)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3130/40201]  eta: 5:47:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5436 (3.4768)  loss_scale: 32768.0000 (16190.4261)  weight_decay: 0.0500 (0.0500)  time: 0.7780  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3140/40201]  eta: 5:47:33  lr: 0.000047  min_lr: 0.000001  loss: 3.2638 (3.4753)  loss_scale: 32768.0000 (16243.2041)  weight_decay: 0.0500 (0.0500)  time: 0.7521  data: 0.0017  max mem: 6186
Epoch: [25]  [ 3150/40201]  eta: 5:47:55  lr: 0.000047  min_lr: 0.000001  loss: 3.2086 (3.4752)  loss_scale: 32768.0000 (16295.6471)  weight_decay: 0.0500 (0.0500)  time: 0.7709  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3160/40201]  eta: 5:48:07  lr: 0.000047  min_lr: 0.000001  loss: 3.2086 (3.4747)  loss_scale: 32768.0000 (16347.7583)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3170/40201]  eta: 5:47:39  lr: 0.000047  min_lr: 0.000001  loss: 2.9209 (3.4734)  loss_scale: 32768.0000 (16399.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3180/40201]  eta: 5:47:21  lr: 0.000047  min_lr: 0.000001  loss: 3.6311 (3.4755)  loss_scale: 32768.0000 (16450.9978)  weight_decay: 0.0500 (0.0500)  time: 0.4174  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3190/40201]  eta: 5:48:13  lr: 0.000047  min_lr: 0.000001  loss: 3.7560 (3.4747)  loss_scale: 32768.0000 (16502.1322)  weight_decay: 0.0500 (0.0500)  time: 0.7566  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3200/40201]  eta: 5:47:44  lr: 0.000047  min_lr: 0.000001  loss: 3.5877 (3.4758)  loss_scale: 32768.0000 (16552.9472)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3210/40201]  eta: 5:47:28  lr: 0.000047  min_lr: 0.000001  loss: 3.4798 (3.4757)  loss_scale: 32768.0000 (16603.4457)  weight_decay: 0.0500 (0.0500)  time: 0.4216  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3220/40201]  eta: 5:47:46  lr: 0.000047  min_lr: 0.000001  loss: 3.3209 (3.4751)  loss_scale: 32768.0000 (16653.6305)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3230/40201]  eta: 5:47:55  lr: 0.000047  min_lr: 0.000001  loss: 3.1567 (3.4743)  loss_scale: 32768.0000 (16703.5048)  weight_decay: 0.0500 (0.0500)  time: 0.7299  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3240/40201]  eta: 5:48:00  lr: 0.000047  min_lr: 0.000001  loss: 3.2513 (3.4743)  loss_scale: 32768.0000 (16753.0713)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3250/40201]  eta: 5:48:09  lr: 0.000047  min_lr: 0.000001  loss: 3.2778 (3.4731)  loss_scale: 32768.0000 (16802.3328)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3260/40201]  eta: 5:48:23  lr: 0.000047  min_lr: 0.000001  loss: 3.3848 (3.4733)  loss_scale: 32768.0000 (16851.2922)  weight_decay: 0.0500 (0.0500)  time: 0.7175  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3270/40201]  eta: 5:48:45  lr: 0.000047  min_lr: 0.000001  loss: 3.4443 (3.4738)  loss_scale: 32768.0000 (16899.9523)  weight_decay: 0.0500 (0.0500)  time: 0.7768  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3280/40201]  eta: 5:49:03  lr: 0.000047  min_lr: 0.000001  loss: 3.4443 (3.4739)  loss_scale: 32768.0000 (16948.3158)  weight_decay: 0.0500 (0.0500)  time: 0.7911  data: 0.0004  max mem: 6186
[2023-07-25 16:02:13,008] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:02:13,008] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:02:13,018] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:02:13,019] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 3290/40201]  eta: 5:49:16  lr: 0.000047  min_lr: 0.000001  loss: 3.5564 (3.4741)  loss_scale: 32768.0000 (17016.2990)  weight_decay: 0.0500 (0.0500)  time: 0.7527  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3300/40201]  eta: 5:49:40  lr: 0.000047  min_lr: 0.000001  loss: 3.4486 (3.4741)  loss_scale: 65536.0000 (17163.2839)  weight_decay: 0.0500 (0.0500)  time: 0.7856  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3310/40201]  eta: 5:49:54  lr: 0.000047  min_lr: 0.000001  loss: 3.6987 (3.4752)  loss_scale: 65536.0000 (17309.3809)  weight_decay: 0.0500 (0.0500)  time: 0.7887  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3320/40201]  eta: 5:50:11  lr: 0.000047  min_lr: 0.000001  loss: 3.6210 (3.4753)  loss_scale: 65536.0000 (17454.5980)  weight_decay: 0.0500 (0.0500)  time: 0.7572  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3330/40201]  eta: 5:50:28  lr: 0.000047  min_lr: 0.000001  loss: 3.3711 (3.4745)  loss_scale: 65536.0000 (17598.9433)  weight_decay: 0.0500 (0.0500)  time: 0.7770  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3340/40201]  eta: 5:50:39  lr: 0.000047  min_lr: 0.000001  loss: 2.7652 (3.4731)  loss_scale: 65536.0000 (17742.4244)  weight_decay: 0.0500 (0.0500)  time: 0.7470  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3350/40201]  eta: 5:50:56  lr: 0.000047  min_lr: 0.000001  loss: 2.7652 (3.4724)  loss_scale: 65536.0000 (17885.0492)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3360/40201]  eta: 5:51:14  lr: 0.000047  min_lr: 0.000001  loss: 3.2659 (3.4719)  loss_scale: 65536.0000 (18026.8253)  weight_decay: 0.0500 (0.0500)  time: 0.7860  data: 0.0555  max mem: 6186
Epoch: [25]  [ 3370/40201]  eta: 5:51:25  lr: 0.000047  min_lr: 0.000001  loss: 3.3759 (3.4720)  loss_scale: 65536.0000 (18167.7603)  weight_decay: 0.0500 (0.0500)  time: 0.7584  data: 0.0561  max mem: 6186
Epoch: [25]  [ 3380/40201]  eta: 5:51:36  lr: 0.000047  min_lr: 0.000001  loss: 3.5552 (3.4724)  loss_scale: 65536.0000 (18307.8616)  weight_decay: 0.0500 (0.0500)  time: 0.7254  data: 0.0015  max mem: 6186
Epoch: [25]  [ 3390/40201]  eta: 5:51:46  lr: 0.000047  min_lr: 0.000001  loss: 3.1884 (3.4725)  loss_scale: 65536.0000 (18447.1365)  weight_decay: 0.0500 (0.0500)  time: 0.7204  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3400/40201]  eta: 5:52:13  lr: 0.000047  min_lr: 0.000001  loss: 3.1555 (3.4715)  loss_scale: 65536.0000 (18585.5925)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0003  max mem: 6186
Epoch: [25]  [ 3410/40201]  eta: 5:52:32  lr: 0.000047  min_lr: 0.000001  loss: 3.2199 (3.4725)  loss_scale: 65536.0000 (18723.2366)  weight_decay: 0.0500 (0.0500)  time: 0.8407  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3420/40201]  eta: 5:52:40  lr: 0.000047  min_lr: 0.000001  loss: 3.2762 (3.4720)  loss_scale: 65536.0000 (18860.0760)  weight_decay: 0.0500 (0.0500)  time: 0.7511  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3430/40201]  eta: 5:52:46  lr: 0.000047  min_lr: 0.000001  loss: 3.1473 (3.4717)  loss_scale: 65536.0000 (18996.1177)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.0005  max mem: 6186
[2023-07-25 16:04:03,063] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1717
[2023-07-25 16:04:03,063] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:04:03,063] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 16:04:03,080] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1717
[2023-07-25 16:04:03,081] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 3440/40201]  eta: 5:52:51  lr: 0.000047  min_lr: 0.000001  loss: 3.2316 (3.4718)  loss_scale: 65536.0000 (19074.2319)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3450/40201]  eta: 5:53:05  lr: 0.000047  min_lr: 0.000001  loss: 3.3184 (3.4717)  loss_scale: 32768.0000 (19113.9125)  weight_decay: 0.0500 (0.0500)  time: 0.7187  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3460/40201]  eta: 5:53:11  lr: 0.000047  min_lr: 0.000001  loss: 3.2619 (3.4718)  loss_scale: 32768.0000 (19153.3638)  weight_decay: 0.0500 (0.0500)  time: 0.7244  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3470/40201]  eta: 5:53:24  lr: 0.000047  min_lr: 0.000001  loss: 3.2586 (3.4710)  loss_scale: 32768.0000 (19192.5877)  weight_decay: 0.0500 (0.0500)  time: 0.7215  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3480/40201]  eta: 5:53:35  lr: 0.000047  min_lr: 0.000001  loss: 3.2478 (3.4712)  loss_scale: 32768.0000 (19231.5863)  weight_decay: 0.0500 (0.0500)  time: 0.7436  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3490/40201]  eta: 5:53:49  lr: 0.000047  min_lr: 0.000001  loss: 3.0493 (3.4698)  loss_scale: 32768.0000 (19270.3615)  weight_decay: 0.0500 (0.0500)  time: 0.7519  data: 0.0013  max mem: 6186
Epoch: [25]  [ 3500/40201]  eta: 5:54:04  lr: 0.000047  min_lr: 0.000001  loss: 3.4601 (3.4701)  loss_scale: 32768.0000 (19308.9152)  weight_decay: 0.0500 (0.0500)  time: 0.7731  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3510/40201]  eta: 5:54:15  lr: 0.000047  min_lr: 0.000001  loss: 3.5757 (3.4711)  loss_scale: 32768.0000 (19347.2492)  weight_decay: 0.0500 (0.0500)  time: 0.7597  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3520/40201]  eta: 5:54:20  lr: 0.000047  min_lr: 0.000001  loss: 3.5284 (3.4699)  loss_scale: 32768.0000 (19385.3655)  weight_decay: 0.0500 (0.0500)  time: 0.7127  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3530/40201]  eta: 5:54:37  lr: 0.000047  min_lr: 0.000001  loss: 3.1997 (3.4703)  loss_scale: 32768.0000 (19423.2659)  weight_decay: 0.0500 (0.0500)  time: 0.7393  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3540/40201]  eta: 5:54:50  lr: 0.000047  min_lr: 0.000001  loss: 3.5955 (3.4702)  loss_scale: 32768.0000 (19460.9523)  weight_decay: 0.0500 (0.0500)  time: 0.7799  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3550/40201]  eta: 5:54:42  lr: 0.000047  min_lr: 0.000001  loss: 3.0179 (3.4694)  loss_scale: 32768.0000 (19498.4264)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3560/40201]  eta: 5:54:21  lr: 0.000047  min_lr: 0.000001  loss: 3.6332 (3.4709)  loss_scale: 32768.0000 (19535.6900)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0010  max mem: 6186
Epoch: [25]  [ 3570/40201]  eta: 5:53:56  lr: 0.000047  min_lr: 0.000001  loss: 3.4805 (3.4701)  loss_scale: 32768.0000 (19572.7449)  weight_decay: 0.0500 (0.0500)  time: 0.4119  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3580/40201]  eta: 5:53:37  lr: 0.000047  min_lr: 0.000001  loss: 3.3980 (3.4703)  loss_scale: 32768.0000 (19609.5929)  weight_decay: 0.0500 (0.0500)  time: 0.4198  data: 0.0729  max mem: 6186
Epoch: [25]  [ 3590/40201]  eta: 5:53:10  lr: 0.000047  min_lr: 0.000001  loss: 3.3980 (3.4695)  loss_scale: 32768.0000 (19646.2356)  weight_decay: 0.0500 (0.0500)  time: 0.4094  data: 0.0730  max mem: 6186
Epoch: [25]  [ 3600/40201]  eta: 5:52:42  lr: 0.000047  min_lr: 0.000001  loss: 3.3609 (3.4704)  loss_scale: 32768.0000 (19682.6748)  weight_decay: 0.0500 (0.0500)  time: 0.3662  data: 0.0112  max mem: 6186
Epoch: [25]  [ 3610/40201]  eta: 5:52:20  lr: 0.000047  min_lr: 0.000001  loss: 3.3609 (3.4695)  loss_scale: 32768.0000 (19718.9122)  weight_decay: 0.0500 (0.0500)  time: 0.3922  data: 0.0115  max mem: 6186
Epoch: [25]  [ 3620/40201]  eta: 5:53:15  lr: 0.000047  min_lr: 0.000001  loss: 3.0793 (3.4694)  loss_scale: 32768.0000 (19754.9495)  weight_decay: 0.0500 (0.0500)  time: 0.8014  data: 0.2276  max mem: 6186
Epoch: [25]  [ 3630/40201]  eta: 5:53:33  lr: 0.000047  min_lr: 0.000001  loss: 3.1132 (3.4695)  loss_scale: 32768.0000 (19790.7882)  weight_decay: 0.0500 (0.0500)  time: 0.9974  data: 0.2274  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/lk5Ap5gZNj0_000009_000019.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 3640/40201]  eta: 5:53:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1815 (3.4696)  loss_scale: 32768.0000 (19826.4301)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3650/40201]  eta: 5:53:56  lr: 0.000047  min_lr: 0.000001  loss: 3.5130 (3.4707)  loss_scale: 32768.0000 (19861.8767)  weight_decay: 0.0500 (0.0500)  time: 0.7490  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3660/40201]  eta: 5:54:04  lr: 0.000047  min_lr: 0.000001  loss: 3.8375 (3.4714)  loss_scale: 32768.0000 (19897.1297)  weight_decay: 0.0500 (0.0500)  time: 0.7324  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3670/40201]  eta: 5:54:17  lr: 0.000047  min_lr: 0.000001  loss: 3.6222 (3.4726)  loss_scale: 32768.0000 (19932.1907)  weight_decay: 0.0500 (0.0500)  time: 0.7465  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3680/40201]  eta: 5:54:24  lr: 0.000047  min_lr: 0.000001  loss: 3.6884 (3.4738)  loss_scale: 32768.0000 (19967.0611)  weight_decay: 0.0500 (0.0500)  time: 0.7381  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3690/40201]  eta: 5:54:28  lr: 0.000047  min_lr: 0.000001  loss: 3.2748 (3.4733)  loss_scale: 32768.0000 (20001.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6978  data: 0.0011  max mem: 6186
[2023-07-25 16:06:57,815] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:06:57,815] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:06:57,828] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:06:57,828] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 3700/40201]  eta: 5:54:39  lr: 0.000047  min_lr: 0.000001  loss: 3.1387 (3.4728)  loss_scale: 32768.0000 (20107.0673)  weight_decay: 0.0500 (0.0500)  time: 0.7195  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3710/40201]  eta: 5:54:44  lr: 0.000047  min_lr: 0.000001  loss: 3.1352 (3.4726)  loss_scale: 65536.0000 (20229.4842)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.0008  max mem: 6186
[2023-07-25 16:07:10,365] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1855
[2023-07-25 16:07:10,365] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:07:10,377] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1855
[2023-07-25 16:07:10,377] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:07:10,377] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 3720/40201]  eta: 5:54:48  lr: 0.000047  min_lr: 0.000001  loss: 3.1355 (3.4726)  loss_scale: 32768.0000 (20263.1809)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3730/40201]  eta: 5:54:55  lr: 0.000047  min_lr: 0.000001  loss: 3.2681 (3.4724)  loss_scale: 32768.0000 (20296.6969)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3740/40201]  eta: 5:55:02  lr: 0.000047  min_lr: 0.000001  loss: 3.3964 (3.4730)  loss_scale: 32768.0000 (20330.0337)  weight_decay: 0.0500 (0.0500)  time: 0.7150  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3750/40201]  eta: 5:55:07  lr: 0.000047  min_lr: 0.000001  loss: 3.0433 (3.4719)  loss_scale: 32768.0000 (20363.1927)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.0011  max mem: 6186
Epoch: [25]  [ 3760/40201]  eta: 5:54:45  lr: 0.000047  min_lr: 0.000001  loss: 2.8855 (3.4713)  loss_scale: 32768.0000 (20396.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3770/40201]  eta: 5:54:22  lr: 0.000047  min_lr: 0.000001  loss: 3.5655 (3.4726)  loss_scale: 32768.0000 (20428.9833)  weight_decay: 0.0500 (0.0500)  time: 0.4081  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3780/40201]  eta: 5:54:49  lr: 0.000047  min_lr: 0.000001  loss: 3.3032 (3.4714)  loss_scale: 32768.0000 (20461.6176)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3790/40201]  eta: 5:54:25  lr: 0.000047  min_lr: 0.000001  loss: 3.2835 (3.4716)  loss_scale: 32768.0000 (20494.0797)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3800/40201]  eta: 5:54:35  lr: 0.000047  min_lr: 0.000001  loss: 3.3387 (3.4712)  loss_scale: 32768.0000 (20526.3710)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0020  max mem: 6186
Epoch: [25]  [ 3810/40201]  eta: 5:54:46  lr: 0.000047  min_lr: 0.000001  loss: 3.4172 (3.4709)  loss_scale: 32768.0000 (20558.4928)  weight_decay: 0.0500 (0.0500)  time: 0.7511  data: 0.0019  max mem: 6186
Epoch: [25]  [ 3820/40201]  eta: 5:55:04  lr: 0.000047  min_lr: 0.000001  loss: 2.9883 (3.4699)  loss_scale: 32768.0000 (20590.4465)  weight_decay: 0.0500 (0.0500)  time: 0.7992  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3830/40201]  eta: 5:55:15  lr: 0.000047  min_lr: 0.000001  loss: 3.5531 (3.4710)  loss_scale: 32768.0000 (20622.2334)  weight_decay: 0.0500 (0.0500)  time: 0.8026  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3840/40201]  eta: 5:55:23  lr: 0.000047  min_lr: 0.000001  loss: 3.6811 (3.4713)  loss_scale: 32768.0000 (20653.8547)  weight_decay: 0.0500 (0.0500)  time: 0.7497  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3850/40201]  eta: 5:55:35  lr: 0.000047  min_lr: 0.000001  loss: 3.5543 (3.4723)  loss_scale: 32768.0000 (20685.3119)  weight_decay: 0.0500 (0.0500)  time: 0.7504  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3860/40201]  eta: 5:55:42  lr: 0.000047  min_lr: 0.000001  loss: 3.5729 (3.4726)  loss_scale: 32768.0000 (20716.6061)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.0004  max mem: 6186
Epoch: [25]  [ 3870/40201]  eta: 5:55:56  lr: 0.000047  min_lr: 0.000001  loss: 3.7139 (3.4732)  loss_scale: 32768.0000 (20747.7386)  weight_decay: 0.0500 (0.0500)  time: 0.7622  data: 0.0005  max mem: 6186
Epoch: [25]  [ 3880/40201]  eta: 5:56:02  lr: 0.000047  min_lr: 0.000001  loss: 3.8403 (3.4753)  loss_scale: 32768.0000 (20778.7106)  weight_decay: 0.0500 (0.0500)  time: 0.7552  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3890/40201]  eta: 5:56:16  lr: 0.000047  min_lr: 0.000001  loss: 3.7310 (3.4746)  loss_scale: 32768.0000 (20809.5235)  weight_decay: 0.0500 (0.0500)  time: 0.7586  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3900/40201]  eta: 5:56:16  lr: 0.000047  min_lr: 0.000001  loss: 3.0639 (3.4733)  loss_scale: 32768.0000 (20840.1784)  weight_decay: 0.0500 (0.0500)  time: 0.7306  data: 0.0014  max mem: 6186
Epoch: [25]  [ 3910/40201]  eta: 5:56:25  lr: 0.000047  min_lr: 0.000001  loss: 3.4057 (3.4741)  loss_scale: 32768.0000 (20870.6766)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.0006  max mem: 6186
Epoch: [25]  [ 3920/40201]  eta: 5:56:32  lr: 0.000047  min_lr: 0.000001  loss: 3.5328 (3.4734)  loss_scale: 32768.0000 (20901.0191)  weight_decay: 0.0500 (0.0500)  time: 0.7371  data: 0.0009  max mem: 6186
Epoch: [25]  [ 3930/40201]  eta: 5:56:38  lr: 0.000047  min_lr: 0.000001  loss: 3.0686 (3.4722)  loss_scale: 32768.0000 (20931.2073)  weight_decay: 0.0500 (0.0500)  time: 0.7245  data: 0.0012  max mem: 6186
Epoch: [25]  [ 3940/40201]  eta: 5:56:53  lr: 0.000047  min_lr: 0.000001  loss: 2.9105 (3.4717)  loss_scale: 32768.0000 (20961.2423)  weight_decay: 0.0500 (0.0500)  time: 0.7656  data: 0.0007  max mem: 6186
Epoch: [25]  [ 3950/40201]  eta: 5:57:04  lr: 0.000047  min_lr: 0.000001  loss: 3.0455 (3.4709)  loss_scale: 32768.0000 (20991.1253)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3960/40201]  eta: 5:57:13  lr: 0.000047  min_lr: 0.000001  loss: 3.2232 (3.4707)  loss_scale: 32768.0000 (21020.8574)  weight_decay: 0.0500 (0.0500)  time: 0.7679  data: 0.0011  max mem: 6186
[2023-07-25 16:10:14,420] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:10:14,420] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:10:14,421] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:10:14,422] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 3970/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.4374 (3.4712)  loss_scale: 32768.0000 (21066.9433)  weight_decay: 0.0500 (0.0500)  time: 0.7338  data: 0.0008  max mem: 6186
Epoch: [25]  [ 3980/40201]  eta: 5:57:33  lr: 0.000047  min_lr: 0.000001  loss: 3.6097 (3.4714)  loss_scale: 65536.0000 (21178.6466)  weight_decay: 0.0500 (0.0500)  time: 0.7627  data: 0.0419  max mem: 6186
Epoch: [25]  [ 3990/40201]  eta: 5:57:38  lr: 0.000047  min_lr: 0.000001  loss: 3.2807 (3.4710)  loss_scale: 65536.0000 (21289.7900)  weight_decay: 0.0500 (0.0500)  time: 0.7702  data: 0.0420  max mem: 6186
[2023-07-25 16:10:32,411] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1996
[2023-07-25 16:10:32,411] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:10:32,411] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 16:10:32,415] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 1996
[2023-07-25 16:10:32,416] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:10:37,983] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=6, lr=[1.1136217508465052e-06, 1.1136217508465052e-06, 1.4848290011286736e-06, 1.4848290011286736e-06, 1.979772001504898e-06, 1.979772001504898e-06, 2.6396960020065306e-06, 2.6396960020065306e-06, 3.519594669342041e-06, 3.519594669342041e-06, 4.692792892456055e-06, 4.692792892456055e-06, 6.257057189941406e-06, 6.257057189941406e-06, 8.342742919921874e-06, 8.342742919921874e-06, 1.11236572265625e-05, 1.11236572265625e-05, 1.483154296875e-05, 1.483154296875e-05, 1.9775390625e-05, 1.9775390625e-05, 2.63671875e-05, 2.63671875e-05, 3.5156250000000004e-05, 3.5156250000000004e-05, 4.6875e-05, 4.6875e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-25 16:10:37,986] [INFO] [timer.py:181:stop] 0/4000, SamplesPerSec=12.428287500323504
Epoch: [25]  [ 4000/40201]  eta: 5:57:52  lr: 0.000047  min_lr: 0.000001  loss: 3.1851 (3.4706)  loss_scale: 65536.0000 (21334.8583)  weight_decay: 0.0500 (0.0500)  time: 0.7649  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4010/40201]  eta: 5:58:05  lr: 0.000047  min_lr: 0.000001  loss: 2.7173 (3.4685)  loss_scale: 32768.0000 (21363.3628)  weight_decay: 0.0500 (0.0500)  time: 0.8035  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4020/40201]  eta: 5:58:14  lr: 0.000047  min_lr: 0.000001  loss: 2.8699 (3.4689)  loss_scale: 32768.0000 (21391.7254)  weight_decay: 0.0500 (0.0500)  time: 0.7795  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4030/40201]  eta: 5:58:18  lr: 0.000047  min_lr: 0.000001  loss: 3.6467 (3.4691)  loss_scale: 32768.0000 (21419.9474)  weight_decay: 0.0500 (0.0500)  time: 0.7329  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4040/40201]  eta: 5:58:24  lr: 0.000047  min_lr: 0.000001  loss: 3.4381 (3.4694)  loss_scale: 32768.0000 (21448.0297)  weight_decay: 0.0500 (0.0500)  time: 0.7155  data: 0.1183  max mem: 6186
Epoch: [25]  [ 4050/40201]  eta: 5:58:03  lr: 0.000047  min_lr: 0.000001  loss: 3.4670 (3.4702)  loss_scale: 32768.0000 (21475.9733)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1183  max mem: 6186
Epoch: [25]  [ 4060/40201]  eta: 5:57:36  lr: 0.000047  min_lr: 0.000001  loss: 3.6639 (3.4713)  loss_scale: 32768.0000 (21503.7794)  weight_decay: 0.0500 (0.0500)  time: 0.3947  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4070/40201]  eta: 5:57:23  lr: 0.000047  min_lr: 0.000001  loss: 3.6365 (3.4712)  loss_scale: 32768.0000 (21531.4488)  weight_decay: 0.0500 (0.0500)  time: 0.4345  data: 0.0158  max mem: 6186
Epoch: [25]  [ 4080/40201]  eta: 5:57:00  lr: 0.000047  min_lr: 0.000001  loss: 3.1549 (3.4709)  loss_scale: 32768.0000 (21558.9826)  weight_decay: 0.0500 (0.0500)  time: 0.4590  data: 0.0161  max mem: 6186
Epoch: [25]  [ 4090/40201]  eta: 5:57:26  lr: 0.000047  min_lr: 0.000001  loss: 3.1024 (3.4700)  loss_scale: 32768.0000 (21586.3818)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.2572  max mem: 6186
Epoch: [25]  [ 4100/40201]  eta: 5:57:30  lr: 0.000047  min_lr: 0.000001  loss: 3.0919 (3.4698)  loss_scale: 32768.0000 (21613.6474)  weight_decay: 0.0500 (0.0500)  time: 0.8302  data: 0.2567  max mem: 6186
Epoch: [25]  [ 4110/40201]  eta: 5:57:39  lr: 0.000047  min_lr: 0.000001  loss: 3.5124 (3.4698)  loss_scale: 32768.0000 (21640.7803)  weight_decay: 0.0500 (0.0500)  time: 0.7378  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4120/40201]  eta: 5:57:55  lr: 0.000047  min_lr: 0.000001  loss: 3.5101 (3.4696)  loss_scale: 32768.0000 (21667.7816)  weight_decay: 0.0500 (0.0500)  time: 0.8041  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4130/40201]  eta: 5:58:03  lr: 0.000047  min_lr: 0.000001  loss: 3.3027 (3.4690)  loss_scale: 32768.0000 (21694.6521)  weight_decay: 0.0500 (0.0500)  time: 0.7993  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4140/40201]  eta: 5:58:20  lr: 0.000047  min_lr: 0.000001  loss: 3.1851 (3.4689)  loss_scale: 32768.0000 (21721.3929)  weight_decay: 0.0500 (0.0500)  time: 0.8062  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4150/40201]  eta: 5:58:26  lr: 0.000047  min_lr: 0.000001  loss: 3.1717 (3.4684)  loss_scale: 32768.0000 (21748.0048)  weight_decay: 0.0500 (0.0500)  time: 0.7982  data: 0.0017  max mem: 6186
Epoch: [25]  [ 4160/40201]  eta: 5:58:34  lr: 0.000047  min_lr: 0.000001  loss: 3.1468 (3.4682)  loss_scale: 32768.0000 (21774.4888)  weight_decay: 0.0500 (0.0500)  time: 0.7438  data: 0.0016  max mem: 6186
Epoch: [25]  [ 4170/40201]  eta: 5:58:43  lr: 0.000047  min_lr: 0.000001  loss: 3.5084 (3.4687)  loss_scale: 32768.0000 (21800.8458)  weight_decay: 0.0500 (0.0500)  time: 0.7660  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4180/40201]  eta: 5:58:47  lr: 0.000047  min_lr: 0.000001  loss: 3.6521 (3.4692)  loss_scale: 32768.0000 (21827.0768)  weight_decay: 0.0500 (0.0500)  time: 0.7420  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4190/40201]  eta: 5:58:48  lr: 0.000047  min_lr: 0.000001  loss: 3.3153 (3.4679)  loss_scale: 32768.0000 (21853.1825)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4200/40201]  eta: 5:58:55  lr: 0.000047  min_lr: 0.000001  loss: 2.8345 (3.4661)  loss_scale: 32768.0000 (21879.1640)  weight_decay: 0.0500 (0.0500)  time: 0.7148  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4210/40201]  eta: 5:58:58  lr: 0.000047  min_lr: 0.000001  loss: 3.0273 (3.4662)  loss_scale: 32768.0000 (21905.0221)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.0018  max mem: 6186
Epoch: [25]  [ 4220/40201]  eta: 5:59:06  lr: 0.000047  min_lr: 0.000001  loss: 3.4005 (3.4655)  loss_scale: 32768.0000 (21930.7576)  weight_decay: 0.0500 (0.0500)  time: 0.7314  data: 0.0015  max mem: 6186
Epoch: [25]  [ 4230/40201]  eta: 5:59:16  lr: 0.000047  min_lr: 0.000001  loss: 3.2429 (3.4645)  loss_scale: 32768.0000 (21956.3715)  weight_decay: 0.0500 (0.0500)  time: 0.7740  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4240/40201]  eta: 5:59:19  lr: 0.000047  min_lr: 0.000001  loss: 3.5560 (3.4653)  loss_scale: 32768.0000 (21981.8647)  weight_decay: 0.0500 (0.0500)  time: 0.7463  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4250/40201]  eta: 5:59:26  lr: 0.000047  min_lr: 0.000001  loss: 3.6616 (3.4651)  loss_scale: 32768.0000 (22007.2378)  weight_decay: 0.0500 (0.0500)  time: 0.7310  data: 0.0009  max mem: 6186
[2023-07-25 16:13:36,298] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:13:36,298] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:13:36,315] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:13:36,316] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4260/40201]  eta: 5:59:38  lr: 0.000047  min_lr: 0.000001  loss: 3.0077 (3.4649)  loss_scale: 32768.0000 (22109.3940)  weight_decay: 0.0500 (0.0500)  time: 0.7861  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4270/40201]  eta: 5:59:44  lr: 0.000047  min_lr: 0.000001  loss: 3.1673 (3.4646)  loss_scale: 65536.0000 (22211.0719)  weight_decay: 0.0500 (0.0500)  time: 0.7749  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4280/40201]  eta: 5:59:49  lr: 0.000047  min_lr: 0.000001  loss: 3.4270 (3.4643)  loss_scale: 65536.0000 (22312.2747)  weight_decay: 0.0500 (0.0500)  time: 0.7373  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4290/40201]  eta: 5:59:57  lr: 0.000047  min_lr: 0.000001  loss: 3.4135 (3.4641)  loss_scale: 65536.0000 (22413.0058)  weight_decay: 0.0500 (0.0500)  time: 0.7519  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4300/40201]  eta: 6:00:04  lr: 0.000047  min_lr: 0.000001  loss: 3.4884 (3.4647)  loss_scale: 65536.0000 (22513.2685)  weight_decay: 0.0500 (0.0500)  time: 0.7652  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4310/40201]  eta: 6:00:10  lr: 0.000047  min_lr: 0.000001  loss: 3.5426 (3.4648)  loss_scale: 65536.0000 (22613.0661)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4320/40201]  eta: 6:00:19  lr: 0.000047  min_lr: 0.000001  loss: 3.5426 (3.4653)  loss_scale: 65536.0000 (22712.4018)  weight_decay: 0.0500 (0.0500)  time: 0.7601  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4330/40201]  eta: 6:00:23  lr: 0.000047  min_lr: 0.000001  loss: 3.3961 (3.4652)  loss_scale: 65536.0000 (22811.2787)  weight_decay: 0.0500 (0.0500)  time: 0.7561  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4340/40201]  eta: 6:00:31  lr: 0.000047  min_lr: 0.000001  loss: 3.2628 (3.4640)  loss_scale: 65536.0000 (22909.7001)  weight_decay: 0.0500 (0.0500)  time: 0.7482  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4350/40201]  eta: 6:00:33  lr: 0.000047  min_lr: 0.000001  loss: 3.2690 (3.4639)  loss_scale: 65536.0000 (23007.6690)  weight_decay: 0.0500 (0.0500)  time: 0.7392  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4360/40201]  eta: 6:00:36  lr: 0.000047  min_lr: 0.000001  loss: 3.4015 (3.4642)  loss_scale: 65536.0000 (23105.1887)  weight_decay: 0.0500 (0.0500)  time: 0.7081  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4370/40201]  eta: 6:00:38  lr: 0.000047  min_lr: 0.000001  loss: 3.4996 (3.4641)  loss_scale: 65536.0000 (23202.2622)  weight_decay: 0.0500 (0.0500)  time: 0.7024  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4380/40201]  eta: 6:00:40  lr: 0.000047  min_lr: 0.000001  loss: 3.4377 (3.4645)  loss_scale: 65536.0000 (23298.8925)  weight_decay: 0.0500 (0.0500)  time: 0.7036  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4390/40201]  eta: 6:00:47  lr: 0.000047  min_lr: 0.000001  loss: 3.1550 (3.4638)  loss_scale: 65536.0000 (23395.0827)  weight_decay: 0.0500 (0.0500)  time: 0.7386  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4400/40201]  eta: 6:00:49  lr: 0.000047  min_lr: 0.000001  loss: 3.2186 (3.4638)  loss_scale: 65536.0000 (23490.8357)  weight_decay: 0.0500 (0.0500)  time: 0.7340  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4410/40201]  eta: 6:00:59  lr: 0.000047  min_lr: 0.000001  loss: 3.5359 (3.4644)  loss_scale: 65536.0000 (23586.1546)  weight_decay: 0.0500 (0.0500)  time: 0.7508  data: 0.0015  max mem: 6186
Epoch: [25]  [ 4420/40201]  eta: 6:01:07  lr: 0.000047  min_lr: 0.000001  loss: 3.5312 (3.4639)  loss_scale: 65536.0000 (23681.0423)  weight_decay: 0.0500 (0.0500)  time: 0.7871  data: 0.0011  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/_M6Ko0yRfD4_000097_000107.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 4430/40201]  eta: 6:01:15  lr: 0.000047  min_lr: 0.000001  loss: 3.2892 (3.4638)  loss_scale: 65536.0000 (23775.5017)  weight_decay: 0.0500 (0.0500)  time: 0.7771  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4440/40201]  eta: 6:01:21  lr: 0.000047  min_lr: 0.000001  loss: 3.3224 (3.4636)  loss_scale: 65536.0000 (23869.5357)  weight_decay: 0.0500 (0.0500)  time: 0.7706  data: 0.0009  max mem: 6186
[2023-07-25 16:16:01,282] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2222
[2023-07-25 16:16:01,282] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:16:01,282] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 16:16:01,284] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2222
[2023-07-25 16:16:01,284] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 4450/40201]  eta: 6:01:27  lr: 0.000047  min_lr: 0.000001  loss: 3.5925 (3.4651)  loss_scale: 65536.0000 (23918.9755)  weight_decay: 0.0500 (0.0500)  time: 0.7568  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4460/40201]  eta: 6:01:31  lr: 0.000047  min_lr: 0.000001  loss: 3.5925 (3.4648)  loss_scale: 32768.0000 (23938.8119)  weight_decay: 0.0500 (0.0500)  time: 0.7436  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4470/40201]  eta: 6:01:33  lr: 0.000047  min_lr: 0.000001  loss: 3.2848 (3.4645)  loss_scale: 32768.0000 (23958.5596)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4480/40201]  eta: 6:01:47  lr: 0.000047  min_lr: 0.000001  loss: 3.3932 (3.4646)  loss_scale: 32768.0000 (23978.2191)  weight_decay: 0.0500 (0.0500)  time: 0.7834  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4490/40201]  eta: 6:01:51  lr: 0.000047  min_lr: 0.000001  loss: 3.0301 (3.4634)  loss_scale: 32768.0000 (23997.7911)  weight_decay: 0.0500 (0.0500)  time: 0.7967  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4500/40201]  eta: 6:02:01  lr: 0.000047  min_lr: 0.000001  loss: 2.9912 (3.4634)  loss_scale: 32768.0000 (24017.2762)  weight_decay: 0.0500 (0.0500)  time: 0.7758  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4510/40201]  eta: 6:02:03  lr: 0.000047  min_lr: 0.000001  loss: 3.4064 (3.4642)  loss_scale: 32768.0000 (24036.6748)  weight_decay: 0.0500 (0.0500)  time: 0.7575  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4520/40201]  eta: 6:02:05  lr: 0.000047  min_lr: 0.000001  loss: 3.3944 (3.4634)  loss_scale: 32768.0000 (24055.9876)  weight_decay: 0.0500 (0.0500)  time: 0.7062  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4530/40201]  eta: 6:02:13  lr: 0.000047  min_lr: 0.000001  loss: 3.1713 (3.4637)  loss_scale: 32768.0000 (24075.2152)  weight_decay: 0.0500 (0.0500)  time: 0.7552  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4540/40201]  eta: 6:02:17  lr: 0.000047  min_lr: 0.000001  loss: 3.5555 (3.4638)  loss_scale: 32768.0000 (24094.3581)  weight_decay: 0.0500 (0.0500)  time: 0.7637  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4550/40201]  eta: 6:02:20  lr: 0.000047  min_lr: 0.000001  loss: 3.5555 (3.4636)  loss_scale: 32768.0000 (24113.4168)  weight_decay: 0.0500 (0.0500)  time: 0.7271  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4560/40201]  eta: 6:02:20  lr: 0.000047  min_lr: 0.000001  loss: 3.5586 (3.4636)  loss_scale: 32768.0000 (24132.3920)  weight_decay: 0.0500 (0.0500)  time: 0.7088  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4570/40201]  eta: 6:02:31  lr: 0.000047  min_lr: 0.000001  loss: 3.6890 (3.4639)  loss_scale: 32768.0000 (24151.2842)  weight_decay: 0.0500 (0.0500)  time: 0.7571  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4580/40201]  eta: 6:02:33  lr: 0.000047  min_lr: 0.000001  loss: 3.7252 (3.4636)  loss_scale: 32768.0000 (24170.0939)  weight_decay: 0.0500 (0.0500)  time: 0.7680  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4590/40201]  eta: 6:02:29  lr: 0.000047  min_lr: 0.000001  loss: 3.2740 (3.4636)  loss_scale: 32768.0000 (24188.8216)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4600/40201]  eta: 6:02:05  lr: 0.000047  min_lr: 0.000001  loss: 3.1140 (3.4633)  loss_scale: 32768.0000 (24207.4679)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4610/40201]  eta: 6:01:42  lr: 0.000047  min_lr: 0.000001  loss: 3.1140 (3.4633)  loss_scale: 32768.0000 (24226.0334)  weight_decay: 0.0500 (0.0500)  time: 0.3838  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4620/40201]  eta: 6:01:19  lr: 0.000047  min_lr: 0.000001  loss: 3.4619 (3.4630)  loss_scale: 32768.0000 (24244.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3932  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4630/40201]  eta: 6:00:59  lr: 0.000047  min_lr: 0.000001  loss: 3.1261 (3.4618)  loss_scale: 32768.0000 (24262.9238)  weight_decay: 0.0500 (0.0500)  time: 0.4076  data: 0.0014  max mem: 6186
Epoch: [25]  [ 4640/40201]  eta: 6:00:34  lr: 0.000047  min_lr: 0.000001  loss: 3.2286 (3.4624)  loss_scale: 32768.0000 (24281.2497)  weight_decay: 0.0500 (0.0500)  time: 0.3945  data: 0.0034  max mem: 6186
Epoch: [25]  [ 4650/40201]  eta: 6:00:16  lr: 0.000047  min_lr: 0.000001  loss: 3.6627 (3.4619)  loss_scale: 32768.0000 (24299.4969)  weight_decay: 0.0500 (0.0500)  time: 0.4086  data: 0.0027  max mem: 6186
Epoch: [25]  [ 4660/40201]  eta: 6:00:13  lr: 0.000047  min_lr: 0.000001  loss: 3.3849 (3.4624)  loss_scale: 32768.0000 (24317.6657)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4670/40201]  eta: 6:00:17  lr: 0.000047  min_lr: 0.000001  loss: 3.6964 (3.4635)  loss_scale: 32768.0000 (24335.7568)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4680/40201]  eta: 6:00:22  lr: 0.000047  min_lr: 0.000001  loss: 3.3673 (3.4627)  loss_scale: 32768.0000 (24353.7706)  weight_decay: 0.0500 (0.0500)  time: 0.7434  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4690/40201]  eta: 6:00:28  lr: 0.000047  min_lr: 0.000001  loss: 3.4371 (3.4630)  loss_scale: 32768.0000 (24371.7075)  weight_decay: 0.0500 (0.0500)  time: 0.7664  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4700/40201]  eta: 6:00:36  lr: 0.000047  min_lr: 0.000001  loss: 3.4475 (3.4632)  loss_scale: 32768.0000 (24389.5682)  weight_decay: 0.0500 (0.0500)  time: 0.7859  data: 0.0010  max mem: 6186
[2023-07-25 16:18:52,928] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:18:52,928] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:18:52,930] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:18:52,930] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4710/40201]  eta: 6:00:42  lr: 0.000047  min_lr: 0.000001  loss: 3.3959 (3.4628)  loss_scale: 32768.0000 (24462.9981)  weight_decay: 0.0500 (0.0500)  time: 0.7783  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4720/40201]  eta: 6:00:53  lr: 0.000047  min_lr: 0.000001  loss: 3.2376 (3.4639)  loss_scale: 65536.0000 (24549.9987)  weight_decay: 0.0500 (0.0500)  time: 0.7996  data: 0.0014  max mem: 6186
[2023-07-25 16:19:08,869] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2363
[2023-07-25 16:19:08,869] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:19:08,869] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2363
[2023-07-25 16:19:08,869] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:19:08,869] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 4730/40201]  eta: 6:00:30  lr: 0.000047  min_lr: 0.000001  loss: 3.7549 (3.4645)  loss_scale: 65536.0000 (24608.9267)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0019  max mem: 6186
Epoch: [25]  [ 4740/40201]  eta: 6:00:01  lr: 0.000047  min_lr: 0.000001  loss: 3.6594 (3.4650)  loss_scale: 32768.0000 (24626.1363)  weight_decay: 0.0500 (0.0500)  time: 0.3465  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4750/40201]  eta: 6:00:26  lr: 0.000047  min_lr: 0.000001  loss: 3.6318 (3.4654)  loss_scale: 32768.0000 (24643.2734)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4760/40201]  eta: 6:00:11  lr: 0.000047  min_lr: 0.000001  loss: 3.4423 (3.4650)  loss_scale: 32768.0000 (24660.3386)  weight_decay: 0.0500 (0.0500)  time: 0.7558  data: 0.0007  max mem: 6186
Epoch: [25]  [ 4770/40201]  eta: 6:00:00  lr: 0.000047  min_lr: 0.000001  loss: 3.3703 (3.4647)  loss_scale: 32768.0000 (24677.3322)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0011  max mem: 6186
Epoch: [25]  [ 4780/40201]  eta: 6:00:06  lr: 0.000047  min_lr: 0.000001  loss: 3.3753 (3.4646)  loss_scale: 32768.0000 (24694.2548)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4790/40201]  eta: 6:00:16  lr: 0.000047  min_lr: 0.000001  loss: 3.2809 (3.4644)  loss_scale: 32768.0000 (24711.1067)  weight_decay: 0.0500 (0.0500)  time: 0.8018  data: 0.0017  max mem: 6186
Epoch: [25]  [ 4800/40201]  eta: 6:00:21  lr: 0.000047  min_lr: 0.000001  loss: 3.1364 (3.4638)  loss_scale: 32768.0000 (24727.8884)  weight_decay: 0.0500 (0.0500)  time: 0.7985  data: 0.0018  max mem: 6186
Epoch: [25]  [ 4810/40201]  eta: 6:00:22  lr: 0.000047  min_lr: 0.000001  loss: 3.2978 (3.4638)  loss_scale: 32768.0000 (24744.6003)  weight_decay: 0.0500 (0.0500)  time: 0.7343  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4820/40201]  eta: 6:00:29  lr: 0.000047  min_lr: 0.000001  loss: 3.4329 (3.4634)  loss_scale: 32768.0000 (24761.2429)  weight_decay: 0.0500 (0.0500)  time: 0.7447  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4830/40201]  eta: 6:00:37  lr: 0.000047  min_lr: 0.000001  loss: 3.3229 (3.4632)  loss_scale: 32768.0000 (24777.8166)  weight_decay: 0.0500 (0.0500)  time: 0.7960  data: 0.0008  max mem: 6186
Epoch: [25]  [ 4840/40201]  eta: 6:00:50  lr: 0.000047  min_lr: 0.000001  loss: 3.6601 (3.4633)  loss_scale: 32768.0000 (24794.3218)  weight_decay: 0.0500 (0.0500)  time: 0.8389  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4850/40201]  eta: 6:00:49  lr: 0.000047  min_lr: 0.000001  loss: 3.4635 (3.4629)  loss_scale: 32768.0000 (24810.7590)  weight_decay: 0.0500 (0.0500)  time: 0.7748  data: 0.0019  max mem: 6186
Epoch: [25]  [ 4860/40201]  eta: 6:00:55  lr: 0.000047  min_lr: 0.000001  loss: 3.0346 (3.4620)  loss_scale: 32768.0000 (24827.1286)  weight_decay: 0.0500 (0.0500)  time: 0.7289  data: 0.0016  max mem: 6186
Epoch: [25]  [ 4870/40201]  eta: 6:01:00  lr: 0.000047  min_lr: 0.000001  loss: 3.1968 (3.4619)  loss_scale: 32768.0000 (24843.4309)  weight_decay: 0.0500 (0.0500)  time: 0.7757  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4880/40201]  eta: 6:01:03  lr: 0.000047  min_lr: 0.000001  loss: 3.5416 (3.4620)  loss_scale: 32768.0000 (24859.6665)  weight_decay: 0.0500 (0.0500)  time: 0.7569  data: 0.0037  max mem: 6186
Epoch: [25]  [ 4890/40201]  eta: 6:01:10  lr: 0.000047  min_lr: 0.000001  loss: 3.5614 (3.4624)  loss_scale: 32768.0000 (24875.8356)  weight_decay: 0.0500 (0.0500)  time: 0.7627  data: 0.0034  max mem: 6186
Epoch: [25]  [ 4900/40201]  eta: 6:01:22  lr: 0.000047  min_lr: 0.000001  loss: 3.3686 (3.4621)  loss_scale: 32768.0000 (24891.9388)  weight_decay: 0.0500 (0.0500)  time: 0.8301  data: 0.0004  max mem: 6186
Epoch: [25]  [ 4910/40201]  eta: 6:01:27  lr: 0.000047  min_lr: 0.000001  loss: 3.1216 (3.4617)  loss_scale: 32768.0000 (24907.9764)  weight_decay: 0.0500 (0.0500)  time: 0.8226  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4920/40201]  eta: 6:01:38  lr: 0.000047  min_lr: 0.000001  loss: 3.1216 (3.4623)  loss_scale: 32768.0000 (24923.9488)  weight_decay: 0.0500 (0.0500)  time: 0.8124  data: 0.0012  max mem: 6186
Epoch: [25]  [ 4930/40201]  eta: 6:01:42  lr: 0.000047  min_lr: 0.000001  loss: 3.1124 (3.4620)  loss_scale: 32768.0000 (24939.8564)  weight_decay: 0.0500 (0.0500)  time: 0.8009  data: 0.0013  max mem: 6186
Epoch: [25]  [ 4940/40201]  eta: 6:01:45  lr: 0.000047  min_lr: 0.000001  loss: 3.2652 (3.4619)  loss_scale: 32768.0000 (24955.6997)  weight_decay: 0.0500 (0.0500)  time: 0.7464  data: 0.0009  max mem: 6186
Epoch: [25]  [ 4950/40201]  eta: 6:01:54  lr: 0.000047  min_lr: 0.000001  loss: 3.2724 (3.4624)  loss_scale: 32768.0000 (24971.4789)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.0010  max mem: 6186
Epoch: [25]  [ 4960/40201]  eta: 6:01:57  lr: 0.000047  min_lr: 0.000001  loss: 3.2473 (3.4620)  loss_scale: 32768.0000 (24987.1945)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.0005  max mem: 6186
Epoch: [25]  [ 4970/40201]  eta: 6:01:59  lr: 0.000047  min_lr: 0.000001  loss: 3.1348 (3.4613)  loss_scale: 32768.0000 (25002.8469)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.0006  max mem: 6186
Epoch: [25]  [ 4980/40201]  eta: 6:02:04  lr: 0.000047  min_lr: 0.000001  loss: 3.2525 (3.4611)  loss_scale: 32768.0000 (25018.4365)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.0010  max mem: 6186
[2023-07-25 16:22:22,672] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:22:22,672] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:22:22,687] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:22:22,688] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 4990/40201]  eta: 6:02:14  lr: 0.000047  min_lr: 0.000001  loss: 3.4281 (3.4611)  loss_scale: 32768.0000 (25073.3560)  weight_decay: 0.0500 (0.0500)  time: 0.8100  data: 0.0010  max mem: 6186
[2023-07-25 16:22:33,317] [INFO] [timer.py:181:stop] 0/5000, SamplesPerSec=12.00819550798308
Epoch: [25]  [ 5000/40201]  eta: 6:02:19  lr: 0.000047  min_lr: 0.000001  loss: 3.2445 (3.4613)  loss_scale: 65536.0000 (25154.2651)  weight_decay: 0.0500 (0.0500)  time: 0.8132  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5010/40201]  eta: 6:02:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2445 (3.4607)  loss_scale: 65536.0000 (25234.8513)  weight_decay: 0.0500 (0.0500)  time: 0.7296  data: 0.0007  max mem: 6186
[2023-07-25 16:22:45,801] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2508
[2023-07-25 16:22:45,801] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:22:45,806] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2508
[2023-07-25 16:22:45,806] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:22:45,806] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 5020/40201]  eta: 6:02:15  lr: 0.000047  min_lr: 0.000001  loss: 3.3740 (3.4599)  loss_scale: 65536.0000 (25289.0118)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5030/40201]  eta: 6:02:14  lr: 0.000047  min_lr: 0.000001  loss: 3.2098 (3.4594)  loss_scale: 32768.0000 (25303.8776)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5040/40201]  eta: 6:02:15  lr: 0.000047  min_lr: 0.000001  loss: 3.5057 (3.4599)  loss_scale: 32768.0000 (25318.6844)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5050/40201]  eta: 6:02:12  lr: 0.000047  min_lr: 0.000001  loss: 3.4301 (3.4598)  loss_scale: 32768.0000 (25333.4326)  weight_decay: 0.0500 (0.0500)  time: 0.6926  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5060/40201]  eta: 6:02:16  lr: 0.000047  min_lr: 0.000001  loss: 3.2019 (3.4595)  loss_scale: 32768.0000 (25348.1225)  weight_decay: 0.0500 (0.0500)  time: 0.7147  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5070/40201]  eta: 6:02:15  lr: 0.000047  min_lr: 0.000001  loss: 3.2429 (3.4598)  loss_scale: 32768.0000 (25362.7545)  weight_decay: 0.0500 (0.0500)  time: 0.7243  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5080/40201]  eta: 6:02:14  lr: 0.000047  min_lr: 0.000001  loss: 3.7387 (3.4601)  loss_scale: 32768.0000 (25377.3289)  weight_decay: 0.0500 (0.0500)  time: 0.6986  data: 0.0015  max mem: 6186
/root/models/mvd/kinetics.py:137: UserWarning: video /data/i5O/kinetics400/train/uLaU_15HYdo_000002_000012.mp4 not correctly loaded during training
  warnings.warn(
Epoch: [25]  [ 5090/40201]  eta: 6:02:11  lr: 0.000047  min_lr: 0.000001  loss: 3.7387 (3.4607)  loss_scale: 32768.0000 (25391.8460)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.0018  max mem: 6186
Epoch: [25]  [ 5100/40201]  eta: 6:02:08  lr: 0.000047  min_lr: 0.000001  loss: 3.7941 (3.4610)  loss_scale: 32768.0000 (25406.3062)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5110/40201]  eta: 6:02:07  lr: 0.000047  min_lr: 0.000001  loss: 3.5900 (3.4610)  loss_scale: 32768.0000 (25420.7098)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5120/40201]  eta: 6:02:02  lr: 0.000047  min_lr: 0.000001  loss: 3.5604 (3.4607)  loss_scale: 32768.0000 (25435.0572)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5130/40201]  eta: 6:01:57  lr: 0.000047  min_lr: 0.000001  loss: 2.7291 (3.4595)  loss_scale: 32768.0000 (25449.3487)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.0017  max mem: 6186
Epoch: [25]  [ 5140/40201]  eta: 6:01:52  lr: 0.000047  min_lr: 0.000001  loss: 2.8148 (3.4595)  loss_scale: 32768.0000 (25463.5845)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5150/40201]  eta: 6:01:51  lr: 0.000047  min_lr: 0.000001  loss: 3.2868 (3.4593)  loss_scale: 32768.0000 (25477.7651)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5160/40201]  eta: 6:01:48  lr: 0.000047  min_lr: 0.000001  loss: 3.4484 (3.4601)  loss_scale: 32768.0000 (25491.8907)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5170/40201]  eta: 6:01:24  lr: 0.000047  min_lr: 0.000001  loss: 3.3463 (3.4598)  loss_scale: 32768.0000 (25505.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5180/40201]  eta: 6:01:02  lr: 0.000047  min_lr: 0.000001  loss: 3.2775 (3.4603)  loss_scale: 32768.0000 (25519.9784)  weight_decay: 0.0500 (0.0500)  time: 0.3690  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5190/40201]  eta: 6:00:36  lr: 0.000047  min_lr: 0.000001  loss: 3.4943 (3.4600)  loss_scale: 32768.0000 (25533.9411)  weight_decay: 0.0500 (0.0500)  time: 0.3589  data: 0.0024  max mem: 6186
Epoch: [25]  [ 5200/40201]  eta: 6:00:59  lr: 0.000047  min_lr: 0.000001  loss: 3.0352 (3.4597)  loss_scale: 32768.0000 (25547.8500)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.3861  max mem: 6186
Epoch: [25]  [ 5210/40201]  eta: 6:00:48  lr: 0.000047  min_lr: 0.000001  loss: 3.2983 (3.4595)  loss_scale: 32768.0000 (25561.7056)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.5031  max mem: 6186
Epoch: [25]  [ 5220/40201]  eta: 6:00:47  lr: 0.000047  min_lr: 0.000001  loss: 3.4296 (3.4593)  loss_scale: 32768.0000 (25575.5081)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1195  max mem: 6186
Epoch: [25]  [ 5230/40201]  eta: 6:00:47  lr: 0.000047  min_lr: 0.000001  loss: 3.7618 (3.4608)  loss_scale: 32768.0000 (25589.2579)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5240/40201]  eta: 6:00:40  lr: 0.000047  min_lr: 0.000001  loss: 4.3437 (3.4624)  loss_scale: 32768.0000 (25602.9552)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5250/40201]  eta: 6:00:34  lr: 0.000047  min_lr: 0.000001  loss: 3.5518 (3.4622)  loss_scale: 32768.0000 (25616.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5260/40201]  eta: 6:00:29  lr: 0.000047  min_lr: 0.000001  loss: 3.3803 (3.4623)  loss_scale: 32768.0000 (25630.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5270/40201]  eta: 6:00:25  lr: 0.000047  min_lr: 0.000001  loss: 3.6963 (3.4625)  loss_scale: 32768.0000 (25643.7352)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0014  max mem: 6186
[2023-07-25 16:25:32,073] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:25:32,073] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:25:32,097] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:25:32,097] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 5280/40201]  eta: 6:00:21  lr: 0.000047  min_lr: 0.000001  loss: 3.9874 (3.4634)  loss_scale: 32768.0000 (25694.4548)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.0016  max mem: 6186
Epoch: [25]  [ 5290/40201]  eta: 6:00:19  lr: 0.000047  min_lr: 0.000001  loss: 3.6189 (3.4635)  loss_scale: 65536.0000 (25769.7554)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5300/40201]  eta: 6:00:17  lr: 0.000047  min_lr: 0.000001  loss: 3.5005 (3.4638)  loss_scale: 65536.0000 (25844.7719)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5310/40201]  eta: 6:00:10  lr: 0.000047  min_lr: 0.000001  loss: 3.3860 (3.4640)  loss_scale: 65536.0000 (25919.5059)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0006  max mem: 6186
[2023-07-25 16:25:58,644] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2657
[2023-07-25 16:25:58,644] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:25:58,647] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2657
[2023-07-25 16:25:58,647] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:25:58,647] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 5320/40201]  eta: 6:00:11  lr: 0.000047  min_lr: 0.000001  loss: 2.7674 (3.4624)  loss_scale: 65536.0000 (25957.0096)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.0003  max mem: 6186
Epoch: [25]  [ 5330/40201]  eta: 6:00:13  lr: 0.000047  min_lr: 0.000001  loss: 2.7674 (3.4618)  loss_scale: 32768.0000 (25969.7858)  weight_decay: 0.0500 (0.0500)  time: 0.7398  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5340/40201]  eta: 6:00:14  lr: 0.000047  min_lr: 0.000001  loss: 3.1984 (3.4621)  loss_scale: 32768.0000 (25982.5141)  weight_decay: 0.0500 (0.0500)  time: 0.7355  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5350/40201]  eta: 6:00:17  lr: 0.000047  min_lr: 0.000001  loss: 3.7868 (3.4630)  loss_scale: 32768.0000 (25995.1949)  weight_decay: 0.0500 (0.0500)  time: 0.7501  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5360/40201]  eta: 6:00:20  lr: 0.000047  min_lr: 0.000001  loss: 3.3177 (3.4629)  loss_scale: 32768.0000 (26007.8284)  weight_decay: 0.0500 (0.0500)  time: 0.7638  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5370/40201]  eta: 6:00:20  lr: 0.000047  min_lr: 0.000001  loss: 3.1402 (3.4625)  loss_scale: 32768.0000 (26020.4148)  weight_decay: 0.0500 (0.0500)  time: 0.7356  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5380/40201]  eta: 6:00:23  lr: 0.000047  min_lr: 0.000001  loss: 3.2753 (3.4627)  loss_scale: 32768.0000 (26032.9545)  weight_decay: 0.0500 (0.0500)  time: 0.7434  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5390/40201]  eta: 6:00:31  lr: 0.000047  min_lr: 0.000001  loss: 2.9765 (3.4615)  loss_scale: 32768.0000 (26045.4476)  weight_decay: 0.0500 (0.0500)  time: 0.8006  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5400/40201]  eta: 6:00:30  lr: 0.000047  min_lr: 0.000001  loss: 3.2356 (3.4619)  loss_scale: 32768.0000 (26057.8945)  weight_decay: 0.0500 (0.0500)  time: 0.7692  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5410/40201]  eta: 6:00:33  lr: 0.000047  min_lr: 0.000001  loss: 3.6283 (3.4620)  loss_scale: 32768.0000 (26070.2953)  weight_decay: 0.0500 (0.0500)  time: 0.7383  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5420/40201]  eta: 6:00:30  lr: 0.000047  min_lr: 0.000001  loss: 3.6283 (3.4631)  loss_scale: 32768.0000 (26082.6504)  weight_decay: 0.0500 (0.0500)  time: 0.7157  data: 0.0003  max mem: 6186
Epoch: [25]  [ 5430/40201]  eta: 6:00:33  lr: 0.000047  min_lr: 0.000001  loss: 3.4294 (3.4627)  loss_scale: 32768.0000 (26094.9600)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.0003  max mem: 6186
Epoch: [25]  [ 5440/40201]  eta: 6:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.9602 (3.4616)  loss_scale: 32768.0000 (26107.2244)  weight_decay: 0.0500 (0.0500)  time: 0.7856  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5450/40201]  eta: 6:00:39  lr: 0.000047  min_lr: 0.000001  loss: 3.2592 (3.4615)  loss_scale: 32768.0000 (26119.4438)  weight_decay: 0.0500 (0.0500)  time: 0.7656  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5460/40201]  eta: 6:00:37  lr: 0.000047  min_lr: 0.000001  loss: 3.4733 (3.4615)  loss_scale: 32768.0000 (26131.6184)  weight_decay: 0.0500 (0.0500)  time: 0.7086  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5470/40201]  eta: 6:00:20  lr: 0.000047  min_lr: 0.000001  loss: 3.3575 (3.4615)  loss_scale: 32768.0000 (26143.7485)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5480/40201]  eta: 6:00:00  lr: 0.000047  min_lr: 0.000001  loss: 3.3082 (3.4615)  loss_scale: 32768.0000 (26155.8343)  weight_decay: 0.0500 (0.0500)  time: 0.4307  data: 0.0018  max mem: 6186
Epoch: [25]  [ 5490/40201]  eta: 5:59:40  lr: 0.000047  min_lr: 0.000001  loss: 3.5677 (3.4619)  loss_scale: 32768.0000 (26167.8762)  weight_decay: 0.0500 (0.0500)  time: 0.4079  data: 0.0022  max mem: 6186
Epoch: [25]  [ 5500/40201]  eta: 5:59:24  lr: 0.000047  min_lr: 0.000001  loss: 3.5677 (3.4617)  loss_scale: 32768.0000 (26179.8742)  weight_decay: 0.0500 (0.0500)  time: 0.4320  data: 0.0630  max mem: 6186
Epoch: [25]  [ 5510/40201]  eta: 5:59:02  lr: 0.000047  min_lr: 0.000001  loss: 3.6493 (3.4624)  loss_scale: 32768.0000 (26191.8287)  weight_decay: 0.0500 (0.0500)  time: 0.4183  data: 0.0848  max mem: 6186
Epoch: [25]  [ 5520/40201]  eta: 5:58:44  lr: 0.000047  min_lr: 0.000001  loss: 3.6493 (3.4626)  loss_scale: 32768.0000 (26203.7399)  weight_decay: 0.0500 (0.0500)  time: 0.4081  data: 0.0431  max mem: 6186
Epoch: [25]  [ 5530/40201]  eta: 5:58:31  lr: 0.000047  min_lr: 0.000001  loss: 3.4506 (3.4624)  loss_scale: 32768.0000 (26215.6080)  weight_decay: 0.0500 (0.0500)  time: 0.4727  data: 0.0260  max mem: 6186
Epoch: [25]  [ 5540/40201]  eta: 5:58:35  lr: 0.000047  min_lr: 0.000001  loss: 3.5578 (3.4626)  loss_scale: 32768.0000 (26227.4333)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.0057  max mem: 6186
Epoch: [25]  [ 5550/40201]  eta: 5:58:39  lr: 0.000047  min_lr: 0.000001  loss: 3.5578 (3.4629)  loss_scale: 32768.0000 (26239.2160)  weight_decay: 0.0500 (0.0500)  time: 0.7803  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5560/40201]  eta: 5:58:43  lr: 0.000047  min_lr: 0.000001  loss: 3.4386 (3.4627)  loss_scale: 32768.0000 (26250.9563)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5570/40201]  eta: 5:58:40  lr: 0.000047  min_lr: 0.000001  loss: 3.5084 (3.4631)  loss_scale: 32768.0000 (26262.6545)  weight_decay: 0.0500 (0.0500)  time: 0.7344  data: 0.0004  max mem: 6186
[2023-07-25 16:28:50,388] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:28:50,388] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:28:50,412] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:28:50,412] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:28:51,807] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2787
[2023-07-25 16:28:51,807] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:28:51,808] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 16:28:51,814] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 2787
[2023-07-25 16:28:51,814] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 5580/40201]  eta: 5:58:41  lr: 0.000047  min_lr: 0.000001  loss: 3.5084 (3.4626)  loss_scale: 32768.0000 (26286.0534)  weight_decay: 0.0500 (0.0500)  time: 0.6977  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5590/40201]  eta: 5:58:44  lr: 0.000047  min_lr: 0.000001  loss: 2.8877 (3.4624)  loss_scale: 32768.0000 (26297.6469)  weight_decay: 0.0500 (0.0500)  time: 0.7482  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5600/40201]  eta: 5:58:47  lr: 0.000047  min_lr: 0.000001  loss: 3.5227 (3.4631)  loss_scale: 32768.0000 (26309.1991)  weight_decay: 0.0500 (0.0500)  time: 0.7714  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5610/40201]  eta: 5:58:43  lr: 0.000047  min_lr: 0.000001  loss: 3.5249 (3.4629)  loss_scale: 32768.0000 (26320.7100)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5620/40201]  eta: 5:58:43  lr: 0.000047  min_lr: 0.000001  loss: 3.1496 (3.4633)  loss_scale: 32768.0000 (26332.1800)  weight_decay: 0.0500 (0.0500)  time: 0.6948  data: 0.0017  max mem: 6186
Epoch: [25]  [ 5630/40201]  eta: 5:58:44  lr: 0.000047  min_lr: 0.000001  loss: 3.6259 (3.4632)  loss_scale: 32768.0000 (26343.6093)  weight_decay: 0.0500 (0.0500)  time: 0.7301  data: 0.0016  max mem: 6186
Epoch: [25]  [ 5640/40201]  eta: 5:58:47  lr: 0.000047  min_lr: 0.000001  loss: 3.6982 (3.4632)  loss_scale: 32768.0000 (26354.9980)  weight_decay: 0.0500 (0.0500)  time: 0.7555  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5650/40201]  eta: 5:58:50  lr: 0.000047  min_lr: 0.000001  loss: 3.6982 (3.4633)  loss_scale: 32768.0000 (26366.3465)  weight_decay: 0.0500 (0.0500)  time: 0.7699  data: 0.0003  max mem: 6186
Epoch: [25]  [ 5660/40201]  eta: 5:58:47  lr: 0.000047  min_lr: 0.000001  loss: 3.4432 (3.4640)  loss_scale: 32768.0000 (26377.6548)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5670/40201]  eta: 5:58:49  lr: 0.000047  min_lr: 0.000001  loss: 3.7135 (3.4645)  loss_scale: 32768.0000 (26388.9233)  weight_decay: 0.0500 (0.0500)  time: 0.7206  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5680/40201]  eta: 5:58:53  lr: 0.000047  min_lr: 0.000001  loss: 3.5495 (3.4647)  loss_scale: 32768.0000 (26400.1521)  weight_decay: 0.0500 (0.0500)  time: 0.7747  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5690/40201]  eta: 5:58:57  lr: 0.000047  min_lr: 0.000001  loss: 3.4986 (3.4647)  loss_scale: 32768.0000 (26411.3414)  weight_decay: 0.0500 (0.0500)  time: 0.7964  data: 0.0005  max mem: 6186
Epoch: [25]  [ 5700/40201]  eta: 5:58:57  lr: 0.000047  min_lr: 0.000001  loss: 3.2442 (3.4648)  loss_scale: 32768.0000 (26422.4915)  weight_decay: 0.0500 (0.0500)  time: 0.7609  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5710/40201]  eta: 5:58:56  lr: 0.000047  min_lr: 0.000001  loss: 3.1681 (3.4650)  loss_scale: 32768.0000 (26433.6025)  weight_decay: 0.0500 (0.0500)  time: 0.7117  data: 0.0008  max mem: 6186
Epoch: [25]  [ 5720/40201]  eta: 5:58:58  lr: 0.000047  min_lr: 0.000001  loss: 3.1662 (3.4647)  loss_scale: 32768.0000 (26444.6747)  weight_decay: 0.0500 (0.0500)  time: 0.7409  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5730/40201]  eta: 5:59:01  lr: 0.000047  min_lr: 0.000001  loss: 3.1420 (3.4643)  loss_scale: 32768.0000 (26455.7083)  weight_decay: 0.0500 (0.0500)  time: 0.7736  data: 0.0012  max mem: 6186
Epoch: [25]  [ 5740/40201]  eta: 5:58:55  lr: 0.000047  min_lr: 0.000001  loss: 3.3290 (3.4649)  loss_scale: 32768.0000 (26466.7034)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5750/40201]  eta: 5:58:34  lr: 0.000047  min_lr: 0.000001  loss: 3.3290 (3.4648)  loss_scale: 32768.0000 (26477.6602)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5760/40201]  eta: 5:58:13  lr: 0.000047  min_lr: 0.000001  loss: 3.4685 (3.4661)  loss_scale: 32768.0000 (26488.5791)  weight_decay: 0.0500 (0.0500)  time: 0.3801  data: 0.0016  max mem: 6186
Epoch: [25]  [ 5770/40201]  eta: 5:58:13  lr: 0.000047  min_lr: 0.000001  loss: 3.4672 (3.4661)  loss_scale: 32768.0000 (26499.4601)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.1751  max mem: 6186
Epoch: [25]  [ 5780/40201]  eta: 5:57:52  lr: 0.000047  min_lr: 0.000001  loss: 3.2298 (3.4657)  loss_scale: 32768.0000 (26510.3034)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1749  max mem: 6186
Epoch: [25]  [ 5790/40201]  eta: 5:57:33  lr: 0.000047  min_lr: 0.000001  loss: 3.2322 (3.4655)  loss_scale: 32768.0000 (26521.1093)  weight_decay: 0.0500 (0.0500)  time: 0.3935  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5800/40201]  eta: 5:57:14  lr: 0.000047  min_lr: 0.000001  loss: 3.3707 (3.4654)  loss_scale: 32768.0000 (26531.8780)  weight_decay: 0.0500 (0.0500)  time: 0.4047  data: 0.0013  max mem: 6186
Epoch: [25]  [ 5810/40201]  eta: 5:57:14  lr: 0.000047  min_lr: 0.000001  loss: 3.2933 (3.4653)  loss_scale: 32768.0000 (26542.6095)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5820/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.0527 (3.4647)  loss_scale: 32768.0000 (26553.3042)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.0025  max mem: 6186
Epoch: [25]  [ 5830/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.6545 (3.4654)  loss_scale: 32768.0000 (26563.9623)  weight_decay: 0.0500 (0.0500)  time: 0.7590  data: 0.0022  max mem: 6186
[2023-07-25 16:31:44,915] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:31:44,916] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:31:44,919] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:31:44,920] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 5840/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.7089 (3.4655)  loss_scale: 32768.0000 (26619.4638)  weight_decay: 0.0500 (0.0500)  time: 0.7304  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5850/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.3586 (3.4655)  loss_scale: 65536.0000 (26685.9764)  weight_decay: 0.0500 (0.0500)  time: 0.7309  data: 0.0015  max mem: 6186
Epoch: [25]  [ 5860/40201]  eta: 5:57:18  lr: 0.000047  min_lr: 0.000001  loss: 3.5674 (3.4657)  loss_scale: 65536.0000 (26752.2621)  weight_decay: 0.0500 (0.0500)  time: 0.7370  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5870/40201]  eta: 5:57:22  lr: 0.000047  min_lr: 0.000001  loss: 3.6642 (3.4667)  loss_scale: 65536.0000 (26818.3219)  weight_decay: 0.0500 (0.0500)  time: 0.7681  data: 0.0009  max mem: 6186
Epoch: [25]  [ 5880/40201]  eta: 5:57:24  lr: 0.000047  min_lr: 0.000001  loss: 3.3128 (3.4660)  loss_scale: 65536.0000 (26884.1571)  weight_decay: 0.0500 (0.0500)  time: 0.7750  data: 0.0014  max mem: 6186
Epoch: [25]  [ 5890/40201]  eta: 5:57:21  lr: 0.000047  min_lr: 0.000001  loss: 3.6975 (3.4668)  loss_scale: 65536.0000 (26949.7688)  weight_decay: 0.0500 (0.0500)  time: 0.7245  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5900/40201]  eta: 5:57:22  lr: 0.000047  min_lr: 0.000001  loss: 3.6975 (3.4659)  loss_scale: 65536.0000 (27015.1581)  weight_decay: 0.0500 (0.0500)  time: 0.7181  data: 0.0004  max mem: 6186
Epoch: [25]  [ 5910/40201]  eta: 5:57:22  lr: 0.000047  min_lr: 0.000001  loss: 3.4812 (3.4662)  loss_scale: 65536.0000 (27080.3262)  weight_decay: 0.0500 (0.0500)  time: 0.7423  data: 0.0007  max mem: 6186
Epoch: [25]  [ 5920/40201]  eta: 5:57:20  lr: 0.000047  min_lr: 0.000001  loss: 3.4812 (3.4661)  loss_scale: 65536.0000 (27145.2741)  weight_decay: 0.0500 (0.0500)  time: 0.7206  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5930/40201]  eta: 5:57:23  lr: 0.000047  min_lr: 0.000001  loss: 3.1767 (3.4661)  loss_scale: 65536.0000 (27210.0030)  weight_decay: 0.0500 (0.0500)  time: 0.7377  data: 0.0010  max mem: 6186
Epoch: [25]  [ 5940/40201]  eta: 5:57:25  lr: 0.000047  min_lr: 0.000001  loss: 3.3547 (3.4661)  loss_scale: 65536.0000 (27274.5141)  weight_decay: 0.0500 (0.0500)  time: 0.7691  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5950/40201]  eta: 5:57:15  lr: 0.000047  min_lr: 0.000001  loss: 3.4520 (3.4662)  loss_scale: 65536.0000 (27338.8083)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.0011  max mem: 6186
Epoch: [25]  [ 5960/40201]  eta: 5:56:59  lr: 0.000047  min_lr: 0.000001  loss: 3.5174 (3.4664)  loss_scale: 65536.0000 (27402.8868)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0028  max mem: 6186
Epoch: [25]  [ 5970/40201]  eta: 5:56:38  lr: 0.000047  min_lr: 0.000001  loss: 3.6417 (3.4666)  loss_scale: 65536.0000 (27466.7506)  weight_decay: 0.0500 (0.0500)  time: 0.4125  data: 0.0024  max mem: 6186
Epoch: [25]  [ 5980/40201]  eta: 5:56:51  lr: 0.000047  min_lr: 0.000001  loss: 3.7020 (3.4670)  loss_scale: 65536.0000 (27530.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.0006  max mem: 6186
Epoch: [25]  [ 5990/40201]  eta: 5:56:28  lr: 0.000047  min_lr: 0.000001  loss: 3.5530 (3.4671)  loss_scale: 65536.0000 (27593.8388)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.0009  max mem: 6186
[2023-07-25 16:33:34,315] [INFO] [logging.py:69:log_dist] [Rank 0] step=3000, skipped=11, lr=[1.1136217508465052e-06, 1.1136217508465052e-06, 1.4848290011286736e-06, 1.4848290011286736e-06, 1.979772001504898e-06, 1.979772001504898e-06, 2.6396960020065306e-06, 2.6396960020065306e-06, 3.519594669342041e-06, 3.519594669342041e-06, 4.692792892456055e-06, 4.692792892456055e-06, 6.257057189941406e-06, 6.257057189941406e-06, 8.342742919921874e-06, 8.342742919921874e-06, 1.11236572265625e-05, 1.11236572265625e-05, 1.483154296875e-05, 1.483154296875e-05, 1.9775390625e-05, 1.9775390625e-05, 2.63671875e-05, 2.63671875e-05, 3.5156250000000004e-05, 3.5156250000000004e-05, 4.6875e-05, 4.6875e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-07-25 16:33:34,315] [INFO] [timer.py:181:stop] 0/6000, SamplesPerSec=11.93907138542243
Epoch: [25]  [ 6000/40201]  eta: 5:56:05  lr: 0.000047  min_lr: 0.000001  loss: 3.3237 (3.4674)  loss_scale: 65536.0000 (27657.0652)  weight_decay: 0.0500 (0.0500)  time: 0.3276  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6010/40201]  eta: 5:56:08  lr: 0.000047  min_lr: 0.000001  loss: 3.3237 (3.4671)  loss_scale: 65536.0000 (27720.0812)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6020/40201]  eta: 5:56:10  lr: 0.000047  min_lr: 0.000001  loss: 3.0725 (3.4671)  loss_scale: 65536.0000 (27782.8879)  weight_decay: 0.0500 (0.0500)  time: 0.7842  data: 0.0003  max mem: 6186
Epoch: [25]  [ 6030/40201]  eta: 5:56:07  lr: 0.000047  min_lr: 0.000001  loss: 3.3792 (3.4671)  loss_scale: 65536.0000 (27845.4863)  weight_decay: 0.0500 (0.0500)  time: 0.7264  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6040/40201]  eta: 5:56:06  lr: 0.000047  min_lr: 0.000001  loss: 3.1715 (3.4669)  loss_scale: 65536.0000 (27907.8775)  weight_decay: 0.0500 (0.0500)  time: 0.6956  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6050/40201]  eta: 5:56:04  lr: 0.000047  min_lr: 0.000001  loss: 3.1363 (3.4664)  loss_scale: 65536.0000 (27970.0625)  weight_decay: 0.0500 (0.0500)  time: 0.7084  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6060/40201]  eta: 5:56:04  lr: 0.000047  min_lr: 0.000001  loss: 3.1162 (3.4660)  loss_scale: 65536.0000 (28032.0422)  weight_decay: 0.0500 (0.0500)  time: 0.7239  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6070/40201]  eta: 5:56:07  lr: 0.000047  min_lr: 0.000001  loss: 3.3835 (3.4662)  loss_scale: 65536.0000 (28093.8178)  weight_decay: 0.0500 (0.0500)  time: 0.7642  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6080/40201]  eta: 5:56:06  lr: 0.000047  min_lr: 0.000001  loss: 3.5633 (3.4666)  loss_scale: 65536.0000 (28155.3902)  weight_decay: 0.0500 (0.0500)  time: 0.7515  data: 0.0012  max mem: 6186
[2023-07-25 16:34:40,417] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3044
[2023-07-25 16:34:40,417] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:34:40,417] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-07-25 16:34:40,425] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3044
[2023-07-25 16:34:40,425] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 6090/40201]  eta: 5:56:05  lr: 0.000047  min_lr: 0.000001  loss: 3.5098 (3.4667)  loss_scale: 65536.0000 (28206.0010)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6100/40201]  eta: 5:56:07  lr: 0.000047  min_lr: 0.000001  loss: 3.2329 (3.4661)  loss_scale: 32768.0000 (28213.4784)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.0016  max mem: 6186
Epoch: [25]  [ 6110/40201]  eta: 5:56:06  lr: 0.000047  min_lr: 0.000001  loss: 3.1485 (3.4656)  loss_scale: 32768.0000 (28220.9314)  weight_decay: 0.0500 (0.0500)  time: 0.7491  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6120/40201]  eta: 5:56:07  lr: 0.000047  min_lr: 0.000001  loss: 3.6201 (3.4658)  loss_scale: 32768.0000 (28228.3601)  weight_decay: 0.0500 (0.0500)  time: 0.7463  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6130/40201]  eta: 5:56:07  lr: 0.000047  min_lr: 0.000001  loss: 3.6082 (3.4656)  loss_scale: 32768.0000 (28235.7645)  weight_decay: 0.0500 (0.0500)  time: 0.7527  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6140/40201]  eta: 5:56:06  lr: 0.000047  min_lr: 0.000001  loss: 3.1051 (3.4655)  loss_scale: 32768.0000 (28243.1448)  weight_decay: 0.0500 (0.0500)  time: 0.7260  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6150/40201]  eta: 5:55:48  lr: 0.000047  min_lr: 0.000001  loss: 3.4155 (3.4656)  loss_scale: 32768.0000 (28250.5011)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6160/40201]  eta: 5:55:28  lr: 0.000047  min_lr: 0.000001  loss: 3.4305 (3.4656)  loss_scale: 32768.0000 (28257.8335)  weight_decay: 0.0500 (0.0500)  time: 0.4030  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6170/40201]  eta: 5:55:07  lr: 0.000047  min_lr: 0.000001  loss: 3.4499 (3.4660)  loss_scale: 32768.0000 (28265.1421)  weight_decay: 0.0500 (0.0500)  time: 0.3712  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6180/40201]  eta: 5:54:50  lr: 0.000047  min_lr: 0.000001  loss: 3.7024 (3.4666)  loss_scale: 32768.0000 (28272.4271)  weight_decay: 0.0500 (0.0500)  time: 0.3931  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6190/40201]  eta: 5:54:39  lr: 0.000047  min_lr: 0.000001  loss: 3.7023 (3.4671)  loss_scale: 32768.0000 (28279.6886)  weight_decay: 0.0500 (0.0500)  time: 0.4843  data: 0.1074  max mem: 6186
Epoch: [25]  [ 6200/40201]  eta: 5:54:19  lr: 0.000047  min_lr: 0.000001  loss: 3.5949 (3.4671)  loss_scale: 32768.0000 (28286.9266)  weight_decay: 0.0500 (0.0500)  time: 0.4536  data: 0.1080  max mem: 6186
Epoch: [25]  [ 6210/40201]  eta: 5:54:05  lr: 0.000047  min_lr: 0.000001  loss: 3.1261 (3.4669)  loss_scale: 32768.0000 (28294.1414)  weight_decay: 0.0500 (0.0500)  time: 0.4271  data: 0.0025  max mem: 6186
Epoch: [25]  [ 6220/40201]  eta: 5:54:07  lr: 0.000047  min_lr: 0.000001  loss: 3.0840 (3.4670)  loss_scale: 32768.0000 (28301.3329)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0019  max mem: 6186
Epoch: [25]  [ 6230/40201]  eta: 5:54:07  lr: 0.000047  min_lr: 0.000001  loss: 3.2453 (3.4667)  loss_scale: 32768.0000 (28308.5014)  weight_decay: 0.0500 (0.0500)  time: 0.7579  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6240/40201]  eta: 5:54:09  lr: 0.000047  min_lr: 0.000001  loss: 3.7727 (3.4673)  loss_scale: 32768.0000 (28315.6469)  weight_decay: 0.0500 (0.0500)  time: 0.7582  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6250/40201]  eta: 5:54:11  lr: 0.000047  min_lr: 0.000001  loss: 3.8570 (3.4679)  loss_scale: 32768.0000 (28322.7695)  weight_decay: 0.0500 (0.0500)  time: 0.7734  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6260/40201]  eta: 5:54:08  lr: 0.000047  min_lr: 0.000001  loss: 3.9345 (3.4683)  loss_scale: 32768.0000 (28329.8693)  weight_decay: 0.0500 (0.0500)  time: 0.7321  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6270/40201]  eta: 5:54:07  lr: 0.000047  min_lr: 0.000001  loss: 3.5259 (3.4685)  loss_scale: 32768.0000 (28336.9466)  weight_decay: 0.0500 (0.0500)  time: 0.7060  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6280/40201]  eta: 5:54:06  lr: 0.000047  min_lr: 0.000001  loss: 3.5191 (3.4688)  loss_scale: 32768.0000 (28344.0013)  weight_decay: 0.0500 (0.0500)  time: 0.7252  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6290/40201]  eta: 5:54:04  lr: 0.000047  min_lr: 0.000001  loss: 3.5103 (3.4687)  loss_scale: 32768.0000 (28351.0335)  weight_decay: 0.0500 (0.0500)  time: 0.7129  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6300/40201]  eta: 5:54:03  lr: 0.000047  min_lr: 0.000001  loss: 3.4277 (3.4689)  loss_scale: 32768.0000 (28358.0435)  weight_decay: 0.0500 (0.0500)  time: 0.7165  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6310/40201]  eta: 5:54:04  lr: 0.000047  min_lr: 0.000001  loss: 3.6466 (3.4694)  loss_scale: 32768.0000 (28365.0312)  weight_decay: 0.0500 (0.0500)  time: 0.7514  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6320/40201]  eta: 5:54:01  lr: 0.000047  min_lr: 0.000001  loss: 3.7135 (3.4696)  loss_scale: 32768.0000 (28371.9968)  weight_decay: 0.0500 (0.0500)  time: 0.7213  data: 0.0017  max mem: 6186
Epoch: [25]  [ 6330/40201]  eta: 5:54:01  lr: 0.000047  min_lr: 0.000001  loss: 3.1368 (3.4695)  loss_scale: 32768.0000 (28378.9405)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6340/40201]  eta: 5:54:01  lr: 0.000047  min_lr: 0.000001  loss: 3.3023 (3.4695)  loss_scale: 32768.0000 (28385.8622)  weight_decay: 0.0500 (0.0500)  time: 0.7423  data: 0.0005  max mem: 6186
[2023-07-25 16:37:28,598] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:37:28,598] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:37:28,616] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:37:28,617] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6350/40201]  eta: 5:53:58  lr: 0.000047  min_lr: 0.000001  loss: 3.4468 (3.4696)  loss_scale: 32768.0000 (28413.4001)  weight_decay: 0.0500 (0.0500)  time: 0.7160  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6360/40201]  eta: 5:53:56  lr: 0.000047  min_lr: 0.000001  loss: 3.3440 (3.4691)  loss_scale: 65536.0000 (28471.7598)  weight_decay: 0.0500 (0.0500)  time: 0.7032  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6370/40201]  eta: 5:53:54  lr: 0.000047  min_lr: 0.000001  loss: 3.4982 (3.4696)  loss_scale: 65536.0000 (28529.9363)  weight_decay: 0.0500 (0.0500)  time: 0.7097  data: 0.0008  max mem: 6186
[2023-07-25 16:37:46,804] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3186
[2023-07-25 16:37:46,805] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:37:46,813] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3186
[2023-07-25 16:37:46,813] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:37:46,814] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 6380/40201]  eta: 5:53:52  lr: 0.000047  min_lr: 0.000001  loss: 3.7990 (3.4694)  loss_scale: 65536.0000 (28546.8485)  weight_decay: 0.0500 (0.0500)  time: 0.7082  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6390/40201]  eta: 5:53:50  lr: 0.000047  min_lr: 0.000001  loss: 2.8731 (3.4685)  loss_scale: 32768.0000 (28553.4533)  weight_decay: 0.0500 (0.0500)  time: 0.7023  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6400/40201]  eta: 5:53:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1351 (3.4690)  loss_scale: 32768.0000 (28560.0375)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6410/40201]  eta: 5:53:39  lr: 0.000047  min_lr: 0.000001  loss: 3.3919 (3.4684)  loss_scale: 32768.0000 (28566.6012)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6420/40201]  eta: 5:53:33  lr: 0.000047  min_lr: 0.000001  loss: 3.1873 (3.4686)  loss_scale: 32768.0000 (28573.1444)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0017  max mem: 6186
Epoch: [25]  [ 6430/40201]  eta: 5:53:27  lr: 0.000047  min_lr: 0.000001  loss: 3.3008 (3.4687)  loss_scale: 32768.0000 (28579.6672)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0016  max mem: 6186
Epoch: [25]  [ 6440/40201]  eta: 5:53:21  lr: 0.000047  min_lr: 0.000001  loss: 3.3008 (3.4684)  loss_scale: 32768.0000 (28586.1698)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6450/40201]  eta: 5:53:11  lr: 0.000047  min_lr: 0.000001  loss: 3.4361 (3.4687)  loss_scale: 32768.0000 (28592.6523)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6460/40201]  eta: 5:53:05  lr: 0.000047  min_lr: 0.000001  loss: 3.5805 (3.4687)  loss_scale: 32768.0000 (28599.1147)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6470/40201]  eta: 5:52:59  lr: 0.000047  min_lr: 0.000001  loss: 3.7566 (3.4689)  loss_scale: 32768.0000 (28605.5571)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6480/40201]  eta: 5:52:54  lr: 0.000047  min_lr: 0.000001  loss: 3.7097 (3.4692)  loss_scale: 32768.0000 (28611.9796)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.0286  max mem: 6186
Epoch: [25]  [ 6490/40201]  eta: 5:52:47  lr: 0.000047  min_lr: 0.000001  loss: 3.4314 (3.4692)  loss_scale: 32768.0000 (28618.3824)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.0282  max mem: 6186
Epoch: [25]  [ 6500/40201]  eta: 5:52:39  lr: 0.000047  min_lr: 0.000001  loss: 3.2370 (3.4683)  loss_scale: 32768.0000 (28624.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6510/40201]  eta: 5:52:30  lr: 0.000047  min_lr: 0.000001  loss: 3.2331 (3.4685)  loss_scale: 32768.0000 (28631.1289)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6520/40201]  eta: 5:52:21  lr: 0.000047  min_lr: 0.000001  loss: 3.6154 (3.4686)  loss_scale: 32768.0000 (28637.4728)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6530/40201]  eta: 5:52:10  lr: 0.000047  min_lr: 0.000001  loss: 3.6944 (3.4687)  loss_scale: 32768.0000 (28643.7973)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6540/40201]  eta: 5:51:59  lr: 0.000047  min_lr: 0.000001  loss: 3.6944 (3.4694)  loss_scale: 32768.0000 (28650.1024)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6550/40201]  eta: 5:51:50  lr: 0.000047  min_lr: 0.000001  loss: 3.4466 (3.4693)  loss_scale: 32768.0000 (28656.3883)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6560/40201]  eta: 5:51:42  lr: 0.000047  min_lr: 0.000001  loss: 3.4466 (3.4698)  loss_scale: 32768.0000 (28662.6551)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6570/40201]  eta: 5:51:32  lr: 0.000047  min_lr: 0.000001  loss: 3.4103 (3.4694)  loss_scale: 32768.0000 (28668.9028)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0004  max mem: 6186
Epoch: [25]  [ 6580/40201]  eta: 5:51:26  lr: 0.000047  min_lr: 0.000001  loss: 3.0364 (3.4687)  loss_scale: 32768.0000 (28675.1314)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6590/40201]  eta: 5:51:16  lr: 0.000047  min_lr: 0.000001  loss: 3.2438 (3.4692)  loss_scale: 32768.0000 (28681.3412)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6600/40201]  eta: 5:51:10  lr: 0.000047  min_lr: 0.000001  loss: 3.5967 (3.4692)  loss_scale: 32768.0000 (28687.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6610/40201]  eta: 5:51:07  lr: 0.000047  min_lr: 0.000001  loss: 3.2771 (3.4694)  loss_scale: 32768.0000 (28693.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6620/40201]  eta: 5:51:10  lr: 0.000047  min_lr: 0.000001  loss: 3.7087 (3.4700)  loss_scale: 32768.0000 (28699.8580)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6630/40201]  eta: 5:51:13  lr: 0.000047  min_lr: 0.000001  loss: 3.5213 (3.4694)  loss_scale: 32768.0000 (28705.9931)  weight_decay: 0.0500 (0.0500)  time: 0.8091  data: 0.0004  max mem: 6186
[2023-07-25 16:40:29,106] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:40:29,107] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:40:29,310] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:40:29,310] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6640/40201]  eta: 5:51:12  lr: 0.000047  min_lr: 0.000001  loss: 3.2362 (3.4698)  loss_scale: 32768.0000 (28761.4516)  weight_decay: 0.0500 (0.0500)  time: 0.7696  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6650/40201]  eta: 5:50:52  lr: 0.000047  min_lr: 0.000001  loss: 3.5071 (3.4698)  loss_scale: 65536.0000 (28816.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0014  max mem: 6186
Epoch: [25]  [ 6660/40201]  eta: 5:50:39  lr: 0.000047  min_lr: 0.000001  loss: 3.7085 (3.4702)  loss_scale: 65536.0000 (28871.8691)  weight_decay: 0.0500 (0.0500)  time: 0.4199  data: 0.0018  max mem: 6186
[2023-07-25 16:40:50,225] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3333
[2023-07-25 16:40:50,225] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:40:50,289] [INFO] [fused_optimizer.py:339:_update_scale] 
Grad overflow on iteration 3333
[2023-07-25 16:40:50,290] [INFO] [fused_optimizer.py:340:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-07-25 16:40:50,290] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 6670/40201]  eta: 5:50:37  lr: 0.000047  min_lr: 0.000001  loss: 3.6354 (3.4699)  loss_scale: 65536.0000 (28907.1815)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0018  max mem: 6186
Epoch: [25]  [ 6680/40201]  eta: 5:50:16  lr: 0.000047  min_lr: 0.000001  loss: 3.3563 (3.4697)  loss_scale: 32768.0000 (28912.9603)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0013  max mem: 6186
Epoch: [25]  [ 6690/40201]  eta: 5:50:00  lr: 0.000047  min_lr: 0.000001  loss: 2.9550 (3.4689)  loss_scale: 32768.0000 (28918.7219)  weight_decay: 0.0500 (0.0500)  time: 0.3810  data: 0.0019  max mem: 6186
Epoch: [25]  [ 6700/40201]  eta: 5:49:56  lr: 0.000047  min_lr: 0.000001  loss: 2.9550 (3.4685)  loss_scale: 32768.0000 (28924.4662)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0027  max mem: 6186
Epoch: [25]  [ 6710/40201]  eta: 5:49:56  lr: 0.000047  min_lr: 0.000001  loss: 3.2663 (3.4689)  loss_scale: 32768.0000 (28930.1934)  weight_decay: 0.0500 (0.0500)  time: 0.7181  data: 0.0018  max mem: 6186
Epoch: [25]  [ 6720/40201]  eta: 5:49:56  lr: 0.000047  min_lr: 0.000001  loss: 3.5890 (3.4693)  loss_scale: 32768.0000 (28935.9036)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.0005  max mem: 6186
Epoch: [25]  [ 6730/40201]  eta: 5:49:53  lr: 0.000047  min_lr: 0.000001  loss: 3.3133 (3.4688)  loss_scale: 32768.0000 (28941.5968)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6740/40201]  eta: 5:49:34  lr: 0.000047  min_lr: 0.000001  loss: 3.2085 (3.4687)  loss_scale: 32768.0000 (28947.2731)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0012  max mem: 6186
Epoch: [25]  [ 6750/40201]  eta: 5:49:22  lr: 0.000047  min_lr: 0.000001  loss: 3.3448 (3.4690)  loss_scale: 32768.0000 (28952.9326)  weight_decay: 0.0500 (0.0500)  time: 0.4377  data: 0.0019  max mem: 6186
Epoch: [25]  [ 6760/40201]  eta: 5:49:03  lr: 0.000047  min_lr: 0.000001  loss: 3.2906 (3.4690)  loss_scale: 32768.0000 (28958.5754)  weight_decay: 0.0500 (0.0500)  time: 0.4412  data: 0.0023  max mem: 6186
Epoch: [25]  [ 6770/40201]  eta: 5:48:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1259 (3.4686)  loss_scale: 32768.0000 (28964.2014)  weight_decay: 0.0500 (0.0500)  time: 0.3770  data: 0.0023  max mem: 6186
Epoch: [25]  [ 6780/40201]  eta: 5:48:25  lr: 0.000047  min_lr: 0.000001  loss: 3.4958 (3.4688)  loss_scale: 32768.0000 (28969.8109)  weight_decay: 0.0500 (0.0500)  time: 0.3654  data: 0.0016  max mem: 6186
Epoch: [25]  [ 6790/40201]  eta: 5:48:50  lr: 0.000047  min_lr: 0.000001  loss: 3.4772 (3.4689)  loss_scale: 32768.0000 (28975.4039)  weight_decay: 0.0500 (0.0500)  time: 0.8059  data: 0.3037  max mem: 6186
Epoch: [25]  [ 6800/40201]  eta: 5:48:51  lr: 0.000047  min_lr: 0.000001  loss: 3.3672 (3.4692)  loss_scale: 32768.0000 (28980.9804)  weight_decay: 0.0500 (0.0500)  time: 1.0187  data: 0.3038  max mem: 6186
Epoch: [25]  [ 6810/40201]  eta: 5:48:50  lr: 0.000047  min_lr: 0.000001  loss: 3.9446 (3.4700)  loss_scale: 32768.0000 (28986.5406)  weight_decay: 0.0500 (0.0500)  time: 0.7618  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6820/40201]  eta: 5:48:51  lr: 0.000047  min_lr: 0.000001  loss: 3.7383 (3.4696)  loss_scale: 32768.0000 (28992.0844)  weight_decay: 0.0500 (0.0500)  time: 0.7522  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6830/40201]  eta: 5:48:49  lr: 0.000047  min_lr: 0.000001  loss: 3.2074 (3.4692)  loss_scale: 32768.0000 (28997.6121)  weight_decay: 0.0500 (0.0500)  time: 0.7429  data: 0.0008  max mem: 6186
Epoch: [25]  [ 6840/40201]  eta: 5:48:48  lr: 0.000047  min_lr: 0.000001  loss: 3.2998 (3.4697)  loss_scale: 32768.0000 (29003.1235)  weight_decay: 0.0500 (0.0500)  time: 0.7299  data: 0.0010  max mem: 6186
Epoch: [25]  [ 6850/40201]  eta: 5:48:48  lr: 0.000047  min_lr: 0.000001  loss: 3.5545 (3.4698)  loss_scale: 32768.0000 (29008.6189)  weight_decay: 0.0500 (0.0500)  time: 0.7467  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6860/40201]  eta: 5:48:48  lr: 0.000047  min_lr: 0.000001  loss: 3.3062 (3.4697)  loss_scale: 32768.0000 (29014.0982)  weight_decay: 0.0500 (0.0500)  time: 0.7533  data: 0.0009  max mem: 6186
Epoch: [25]  [ 6870/40201]  eta: 5:48:47  lr: 0.000047  min_lr: 0.000001  loss: 3.2675 (3.4694)  loss_scale: 32768.0000 (29019.5616)  weight_decay: 0.0500 (0.0500)  time: 0.7476  data: 0.0019  max mem: 6186
Epoch: [25]  [ 6880/40201]  eta: 5:48:46  lr: 0.000047  min_lr: 0.000001  loss: 3.2442 (3.4689)  loss_scale: 32768.0000 (29025.0092)  weight_decay: 0.0500 (0.0500)  time: 0.7368  data: 0.0025  max mem: 6186
Epoch: [25]  [ 6890/40201]  eta: 5:48:47  lr: 0.000047  min_lr: 0.000001  loss: 3.2006 (3.4684)  loss_scale: 32768.0000 (29030.4409)  weight_decay: 0.0500 (0.0500)  time: 0.7521  data: 0.0016  max mem: 6186
Epoch: [25]  [ 6900/40201]  eta: 5:48:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1222 (3.4683)  loss_scale: 32768.0000 (29035.8568)  weight_decay: 0.0500 (0.0500)  time: 0.7443  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6910/40201]  eta: 5:48:45  lr: 0.000047  min_lr: 0.000001  loss: 3.1876 (3.4679)  loss_scale: 32768.0000 (29041.2571)  weight_decay: 0.0500 (0.0500)  time: 0.7376  data: 0.0006  max mem: 6186
Epoch: [25]  [ 6920/40201]  eta: 5:48:44  lr: 0.000047  min_lr: 0.000001  loss: 3.6203 (3.4685)  loss_scale: 32768.0000 (29046.6418)  weight_decay: 0.0500 (0.0500)  time: 0.7581  data: 0.0017  max mem: 6186
[2023-07-25 16:43:41,538] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:43:41,538] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-07-25 16:43:41,738] [INFO] [fused_optimizer.py:349:_update_scale] No Grad overflow for 128 iterations
[2023-07-25 16:43:41,739] [INFO] [fused_optimizer.py:351:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 6930/40201]  eta: 5:48:45  lr: 0.000047  min_lr: 0.000001  loss: 3.7248 (3.4686)  loss_scale: 32768.0000 (29080.3774)  weight_decay: 0.0500 (0.0500)  time: 0.7638  data: 0.0024  max mem: 6186
Epoch: [25]  [ 6940/40201]  eta: 5:48:30  lr: 0.000047  min_lr: 0.000001  loss: 3.3585 (3.4683)  loss_scale: 65536.0000 (29132.8996)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0011  max mem: 6186
Epoch: [25]  [ 6950/40201]  eta: 5:48:10  lr: 0.000047  min_lr: 0.000001  loss: 3.0971 (3.4674)  loss_scale: 65536.0000 (29185.2706)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0007  max mem: 6186
Epoch: [25]  [ 6960/40201]  eta: 5:47:50  lr: 0.000047  min_lr: 0.000001  loss: 3.0623 (3.4671)  loss_scale: 65536.0000 (29237.4912)  weight_decay: 0.0500 (0.0500)  time: 0.3384  data: 0.0015  max mem: 6186
Epoch: [25]  [ 6970/40201]  eta: 5:47:35  lr: 0.000047  min_lr: 0.000001  loss: 3.5859 (3.4673)  loss_scale: 65536.0000 (29289.5619)  weight_decay: 0.0500 (0.0500)  time: 0.3942  data: 0.0018  max mem: 6186
Epoch: [25]  [ 6980/40201]  eta: 5:47:17  lr: 0.000047  min_lr: 0.000001  loss: 3.2901 (3.4668)  loss_scale: 65536.0000 (29341.4835)  weight_decay: 0.0500 (0.0500)  time: 0.4128  data: 0.0014  max mem: 6186
/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3967467 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3967468 closing signal SIGHUP
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 843, in _invoke_run
    time.sleep(monitor_interval)
  File "/root/myenv/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3967463 got signal: 1
